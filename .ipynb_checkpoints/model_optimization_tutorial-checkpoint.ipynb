{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–å®Œæ•´æ•™ç¨‹\n",
    "\n",
    "## ğŸ“Š æ•™ç¨‹æ¦‚è§ˆ\n",
    "\n",
    "**ç›®æ ‡**: ç³»ç»Ÿå­¦ä¹ æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„å®Œæ•´æµç¨‹ï¼Œå°† RMSE ä» 6697 ä¼˜åŒ–åˆ° 5600-5800ï¼ˆæå‡13-16%ï¼‰\n",
    "\n",
    "### ğŸ“š å­¦ä¹ è·¯çº¿å›¾\n",
    "\n",
    "```\n",
    "ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡ä¸æ•°æ®æ¸…æ´—\n",
    "  â”œâ”€â”€ å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†\n",
    "  â””â”€â”€ æ•°æ®è´¨é‡ä¼˜åŒ–\n",
    "  é¢„æœŸæå‡: -100 RMSE\n",
    "\n",
    "ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§ç‰¹å¾å·¥ç¨‹ â­â­â­\n",
    "  â”œâ”€â”€ é¢†åŸŸçŸ¥è¯†ç‰¹å¾\n",
    "  â”œâ”€â”€ Target Encoding\n",
    "  â”œâ”€â”€ åˆ†ç»„ç»Ÿè®¡ç‰¹å¾\n",
    "  â””â”€â”€ ç‰¹å¾é€‰æ‹©\n",
    "  é¢„æœŸæå‡: -400 RMSE\n",
    "\n",
    "ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¶…å‚æ•°ä¼˜åŒ–\n",
    "  â”œâ”€â”€ Optuna è´å¶æ–¯ä¼˜åŒ–\n",
    "  â””â”€â”€ å‚æ•°é‡è¦æ€§åˆ†æ\n",
    "  é¢„æœŸæå‡: -150 RMSE\n",
    "\n",
    "ç¬¬å››éƒ¨åˆ†ï¼šéªŒè¯ç­–ç•¥ä¼˜åŒ–\n",
    "  â””â”€â”€ åˆ†å±‚äº¤å‰éªŒè¯\n",
    "  é¢„æœŸæå‡: ç¨³å®šæ€§æå‡\n",
    "\n",
    "ç¬¬äº”éƒ¨åˆ†ï¼šæ¨¡å‹èåˆ â­â­â­\n",
    "  â”œâ”€â”€ XGBoost + CatBoost\n",
    "  â”œâ”€â”€ åŠ æƒå¹³å‡\n",
    "  â””â”€â”€ Stacking\n",
    "  é¢„æœŸæå‡: -300 RMSE\n",
    "\n",
    "ç¬¬å…­éƒ¨åˆ†ï¼šåå¤„ç†ä¼˜åŒ–\n",
    "  â””â”€â”€ æ®‹å·®åˆ†æä¸æ ¡å‡†\n",
    "  é¢„æœŸæå‡: -100 RMSE\n",
    "\n",
    "ç¬¬ä¸ƒéƒ¨åˆ†ï¼šæ¨¡å‹è¯Šæ–­\n",
    "  â””â”€â”€ SHAP å¯è§£é‡Šæ€§åˆ†æ\n",
    "```\n",
    "\n",
    "### ğŸ¯ æ€§èƒ½æå‡é¢„æœŸ\n",
    "\n",
    "| é˜¶æ®µ | RMSE | æå‡ | ç´¯è®¡æå‡ |\n",
    "|------|------|------|----------|\n",
    "| åŸºçº¿ï¼ˆå½“å‰ï¼‰ | 6697 | - | - |\n",
    "| æ•°æ®æ¸…æ´— | 6600 | -97 | -97 |\n",
    "| ç‰¹å¾å·¥ç¨‹ | 6200 | -400 | -497 |\n",
    "| è¶…å‚æ•°ä¼˜åŒ– | 6050 | -150 | -647 |\n",
    "| éªŒè¯ç­–ç•¥ | 6000 | -50 | -697 |\n",
    "| æ¨¡å‹èåˆ | 5700 | -300 | -997 |\n",
    "| åå¤„ç† | 5600 | -100 | -1097 |\n",
    "| **æœ€ç»ˆç›®æ ‡** | **5600** | - | **-16.4%** |\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ å­¦ä¹ å»ºè®®\n",
    "\n",
    "1. **æŒ‰é¡ºåºæ‰§è¡Œæ¯ä¸ªcell** - æ¯ä¸ªcelléƒ½æœ‰æ˜ç¡®çš„å­¦ä¹ ç›®æ ‡\n",
    "2. **ä»”ç»†é˜…è¯»ç†è®ºéƒ¨åˆ†** - ç†è§£\"ä¸ºä»€ä¹ˆ\"æ¯”\"æ€ä¹ˆåš\"æ›´é‡è¦\n",
    "3. **è§‚å¯Ÿæ¯ä¸ªä¼˜åŒ–çš„æ•ˆæœ** - å¯¹æ¯”RMSEçš„å˜åŒ–\n",
    "4. **å°è¯•ä¿®æ”¹å‚æ•°** - åŠ¨æ‰‹å®éªŒæ‰èƒ½çœŸæ­£æŒæ¡\n",
    "5. **è®°å½•å…³é”®çŸ¥è¯†ç‚¹** - å¯ä»¥åœ¨ç¬”è®°æœ¬ä¸­æ·»åŠ è‡ªå·±çš„æ€è€ƒ\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ ç¯å¢ƒè¦æ±‚\n",
    "\n",
    "éœ€è¦å®‰è£…çš„åº“ï¼š\n",
    "- `optuna` - è¶…å‚æ•°ä¼˜åŒ–\n",
    "- `xgboost` - XGBoostæ¨¡å‹\n",
    "- `catboost` - CatBoostæ¨¡å‹\n",
    "- `shap` - æ¨¡å‹å¯è§£é‡Šæ€§ï¼ˆå¯é€‰ï¼‰\n",
    "\n",
    "å·²æœ‰çš„åº“ï¼š\n",
    "- pandas, numpy, sklearn, lightgbm, matplotlib, seaborn\n",
    "\n",
    "ç°åœ¨å¼€å§‹å­¦ä¹ ä¹‹æ—…ï¼ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ç¬¬ä¸€éƒ¨åˆ†ï¼šç¯å¢ƒå‡†å¤‡ä¸æ•°æ®æ¸…æ´—\n",
    "\n",
    "## ğŸ¯ å­¦ä¹ ç›®æ ‡\n",
    "1. ç†è§£å¼‚å¸¸å€¼å¯¹æ¨¡å‹çš„å½±å“\n",
    "2. æŒæ¡å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•\n",
    "3. å­¦ä¹ ä¸åŒçš„å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥\n",
    "4. é€šè¿‡å®éªŒéªŒè¯å¤„ç†æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 1: å¯¼å…¥åº“ + ç¯å¢ƒæ£€æŸ¥\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # Mac\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿ç»“æœå¯å¤ç°\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"âœ… åŸºç¡€åº“å¯¼å…¥æˆåŠŸï¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ£€æŸ¥å¹¶å®‰è£…å¿…è¦çš„åº“\n",
    "print(\"\\nğŸ” æ£€æŸ¥é¢å¤–ä¾èµ–...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ£€æŸ¥ optuna\n",
    "try:\n",
    "    import optuna\n",
    "    print(\"âœ… optuna å·²å®‰è£… (ç‰ˆæœ¬: {})\".format(optuna.__version__))\n",
    "except ImportError:\n",
    "    print(\"âŒ optuna æœªå®‰è£…\")\n",
    "    print(\"   å®‰è£…å‘½ä»¤: pip install optuna\")\n",
    "\n",
    "# æ£€æŸ¥ xgboost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"âœ… xgboost å·²å®‰è£… (ç‰ˆæœ¬: {})\".format(xgb.__version__))\n",
    "except ImportError:\n",
    "    print(\"âŒ xgboost æœªå®‰è£…\")\n",
    "    print(\"   å®‰è£…å‘½ä»¤: pip install xgboost\")\n",
    "\n",
    "# æ£€æŸ¥ catboost\n",
    "try:\n",
    "    import catboost as cb\n",
    "    print(\"âœ… catboost å·²å®‰è£… (ç‰ˆæœ¬: {})\".format(cb.__version__))\n",
    "except ImportError:\n",
    "    print(\"âŒ catboost æœªå®‰è£…\")\n",
    "    print(\"   å®‰è£…å‘½ä»¤: pip install catboost\")\n",
    "\n",
    "# æ£€æŸ¥ shap\n",
    "try:\n",
    "    import shap\n",
    "    print(\"âœ… shap å·²å®‰è£… (ç‰ˆæœ¬: {})\".format(shap.__version__))\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  shap æœªå®‰è£…ï¼ˆå¯é€‰ï¼Œç”¨äºæ¨¡å‹è§£é‡Šï¼‰\")\n",
    "    print(\"   å®‰è£…å‘½ä»¤: pip install shap\")\n",
    "\n",
    "print(\"\\nğŸ’¡ æç¤º: å¦‚æœæœ‰æœªå®‰è£…çš„åº“ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œå¯¹åº”çš„å®‰è£…å‘½ä»¤\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-1",
   "metadata": {},
   "source": [
    "## ğŸ“š ç†è®ºçŸ¥è¯†ï¼šå¼‚å¸¸å€¼çš„å½±å“\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯å¼‚å¸¸å€¼ï¼Ÿ\n",
    "\n",
    "å¼‚å¸¸å€¼ï¼ˆOutliersï¼‰æ˜¯æŒ‡æ•°æ®é›†ä¸­ä¸å…¶ä»–è§‚æµ‹å€¼æ˜¾è‘—ä¸åŒçš„æ•°æ®ç‚¹ã€‚å®ƒä»¬å¯èƒ½æ˜¯ï¼š\n",
    "1. **æ•°æ®é”™è¯¯** - å½•å…¥é”™è¯¯ã€ä¼ æ„Ÿå™¨æ•…éšœç­‰\n",
    "2. **çœŸå®çš„æç«¯æƒ…å†µ** - ç½•è§ä½†çœŸå®å­˜åœ¨çš„æƒ…å†µ\n",
    "\n",
    "### å¼‚å¸¸å€¼å¯¹æ¨¡å‹çš„å½±å“\n",
    "\n",
    "#### 1ï¸âƒ£ å¯¹çº¿æ€§æ¨¡å‹çš„å½±å“ï¼ˆä¸¥é‡ï¼‰\n",
    "```\n",
    "çº¿æ€§å›å½’ä½¿ç”¨æœ€å°äºŒä¹˜æ³•ï¼š\n",
    "Loss = Î£(y_true - y_pred)Â²\n",
    "\n",
    "å¼‚å¸¸å€¼ä¼šäº§ç”Ÿå·¨å¤§çš„è¯¯å·®å¹³æ–¹ï¼Œå¯¼è‡´ï¼š\n",
    "- æ¨¡å‹å‚æ•°è¢«\"æ‹‰å\"\n",
    "- é¢„æµ‹æ€§èƒ½ä¸‹é™\n",
    "```\n",
    "\n",
    "#### 2ï¸âƒ£ å¯¹æ ‘æ¨¡å‹çš„å½±å“ï¼ˆè¾ƒå°ä½†ä»å­˜åœ¨ï¼‰\n",
    "```\n",
    "æ ‘æ¨¡å‹è™½ç„¶å¯¹å¼‚å¸¸å€¼ç›¸å¯¹é²æ£’ï¼Œä½†ï¼š\n",
    "- å¼‚å¸¸å€¼å¯èƒ½æˆä¸ºå•ç‹¬çš„å¶å­èŠ‚ç‚¹\n",
    "- å½±å“ç‰¹å¾é‡è¦æ€§è®¡ç®—\n",
    "- é™ä½æ¨¡å‹æ³›åŒ–èƒ½åŠ›\n",
    "```\n",
    "\n",
    "### åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­\n",
    "\n",
    "ä»ä¹‹å‰çš„æ¢ç´¢ä¸­å‘ç°ï¼š\n",
    "- **BMI æœ€å¤§å€¼ = 29330.99** â† è¿™æ˜¾ç„¶æ˜¯é”™è¯¯æ•°æ®ï¼\n",
    "- æ­£å¸¸äººç±»BMIèŒƒå›´ï¼š15-50ï¼ˆæç«¯è‚¥èƒ–ä¹Ÿå¾ˆå°‘è¶…è¿‡60ï¼‰\n",
    "- BMI = ä½“é‡(kg) / èº«é«˜Â²(mÂ²)\n",
    "\n",
    "**ä¾‹å­**ï¼š\n",
    "- BMI = 29330 æ„å‘³ç€ï¼šå‡è®¾èº«é«˜1.7mï¼Œä½“é‡éœ€è¦ 29330 Ã— 1.7Â² â‰ˆ 84,735 kgï¼\n",
    "- è¿™æ˜¯ä¸å¯èƒ½çš„ï¼Œå¿…å®šæ˜¯æ•°æ®é”™è¯¯\n",
    "\n",
    "### å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•\n",
    "\n",
    "#### æ–¹æ³•1: ç»Ÿè®¡æ–¹æ³•\n",
    "```python\n",
    "# IQRæ–¹æ³•ï¼ˆå››åˆ†ä½è·ï¼‰\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "```\n",
    "\n",
    "#### æ–¹æ³•2: é¢†åŸŸçŸ¥è¯†\n",
    "```python\n",
    "# åŸºäºä¸šåŠ¡å¸¸è¯†\n",
    "# ä¾‹å¦‚ï¼šBMIé€šå¸¸åœ¨ 15-60 ä¹‹é—´\n",
    "outliers = df[df['bmi'] > 60]\n",
    "```\n",
    "\n",
    "#### æ–¹æ³•3: å¯è§†åŒ–\n",
    "```python\n",
    "# ç®±çº¿å›¾ã€æ•£ç‚¹å›¾ç­‰ç›´è§‚å±•ç¤º\n",
    "```\n",
    "\n",
    "### å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥\n",
    "\n",
    "| ç­–ç•¥ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |\n",
    "|------|------|------|----------|\n",
    "| **åˆ é™¤** | ç®€å•ç›´æ¥ | æŸå¤±æ•°æ® | æ•°æ®é‡å¤§ï¼Œå¼‚å¸¸å€¼å°‘ |\n",
    "| **æˆªæ–­ï¼ˆClipï¼‰** | ä¿ç•™æ•°æ®é‡ | å¯èƒ½å¼•å…¥åå·® | å¼‚å¸¸å€¼æ˜¯æç«¯çœŸå®å€¼ |\n",
    "| **æ›¿æ¢ï¼ˆä¸­ä½æ•°/å‡å€¼ï¼‰** | ä¿ç•™æ ·æœ¬ | æ”¹å˜åˆ†å¸ƒ | ç¼ºå¤±æ•°æ®å¡«å…… |\n",
    "| **ä¿ç•™** | å®Œæ•´æ€§ | å½±å“æ¨¡å‹ | å¼‚å¸¸å€¼æœ‰æ„ä¹‰ |\n",
    "| **å•ç‹¬å»ºæ¨¡** | ç²¾ç»†åŒ– | å¤æ‚ | å¼‚å¸¸å€¼æœ‰ç‰¹æ®Šæ¨¡å¼ |\n",
    "\n",
    "æ¥ä¸‹æ¥æˆ‘ä»¬ä¼šé€šè¿‡å®éªŒå¯¹æ¯”ä¸åŒç­–ç•¥çš„æ•ˆæœï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 2: åŠ è½½æ•°æ® + å¼‚å¸¸å€¼æ£€æµ‹\n",
    "# ========================================\n",
    "\n",
    "print(\"ğŸ“‚ åŠ è½½æ•°æ®...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åŠ è½½æ•°æ®\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å½¢çŠ¶: {train.shape}\")\n",
    "print(f\"æµ‹è¯•é›†å½¢çŠ¶: {test.shape}\")\n",
    "print(\"\\nå‰5è¡Œæ•°æ®:\")\n",
    "print(train.head())\n",
    "\n",
    "# ========================================\n",
    "# å¼‚å¸¸å€¼æ£€æµ‹\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\\nğŸ” å¼‚å¸¸å€¼æ£€æµ‹åˆ†æ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. ç»Ÿè®¡æè¿°\n",
    "print(\"\\nğŸ“Š æ•°å€¼ç‰¹å¾ç»Ÿè®¡ï¼ˆé‡ç‚¹å…³æ³¨BMIï¼‰:\")\n",
    "print(train[['age', 'bmi', 'children', 'charges']].describe())\n",
    "\n",
    "# 2. BMI å¼‚å¸¸å€¼æ£€æµ‹\n",
    "print(\"\\n\\nğŸ¯ BMI å­—æ®µè¯¦ç»†åˆ†æ:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# é¢†åŸŸçŸ¥è¯†ï¼šæ­£å¸¸BMIèŒƒå›´\n",
    "NORMAL_BMI_MIN = 15\n",
    "NORMAL_BMI_MAX = 60  # æç«¯è‚¥èƒ–çš„ä¸Šé™\n",
    "\n",
    "# ç»Ÿè®¡æ–¹æ³•ï¼šIQR\n",
    "Q1 = train['bmi'].quantile(0.25)\n",
    "Q3 = train['bmi'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "iqr_lower = Q1 - 1.5 * IQR\n",
    "iqr_upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"BMI åŸºæœ¬ç»Ÿè®¡:\")\n",
    "print(f\"  - æœ€å°å€¼: {train['bmi'].min():.2f}\")\n",
    "print(f\"  - 25åˆ†ä½: {Q1:.2f}\")\n",
    "print(f\"  - ä¸­ä½æ•°: {train['bmi'].median():.2f}\")\n",
    "print(f\"  - 75åˆ†ä½: {Q3:.2f}\")\n",
    "print(f\"  - æœ€å¤§å€¼: {train['bmi'].max():.2f} â† æ˜æ˜¾å¼‚å¸¸ï¼\")\n",
    "print(f\"  - å‡å€¼: {train['bmi'].mean():.2f}\")\n",
    "print(f\"  - æ ‡å‡†å·®: {train['bmi'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nIQRæ–¹æ³•æ£€æµ‹:\")\n",
    "print(f\"  - IQR = {IQR:.2f}\")\n",
    "print(f\"  - ä¸‹ç•Œ = Q1 - 1.5*IQR = {iqr_lower:.2f}\")\n",
    "print(f\"  - ä¸Šç•Œ = Q3 + 1.5*IQR = {iqr_upper:.2f}\")\n",
    "\n",
    "# ç»Ÿè®¡å¼‚å¸¸å€¼æ•°é‡\n",
    "outliers_iqr = train[(train['bmi'] < iqr_lower) | (train['bmi'] > iqr_upper)]\n",
    "outliers_domain = train[(train['bmi'] < NORMAL_BMI_MIN) | (train['bmi'] > NORMAL_BMI_MAX)]\n",
    "\n",
    "print(f\"\\nå¼‚å¸¸å€¼ç»Ÿè®¡:\")\n",
    "print(f\"  - IQRæ–¹æ³•æ£€æµ‹åˆ°: {len(outliers_iqr)} ä¸ªå¼‚å¸¸å€¼ ({len(outliers_iqr)/len(train)*100:.2f}%)\")\n",
    "print(f\"  - é¢†åŸŸçŸ¥è¯†æ£€æµ‹åˆ°: {len(outliers_domain)} ä¸ªå¼‚å¸¸å€¼ ({len(outliers_domain)/len(train)*100:.2f}%)\")\n",
    "\n",
    "# æ˜¾ç¤ºæç«¯å¼‚å¸¸å€¼\n",
    "extreme_outliers = train[train['bmi'] > 100]\n",
    "print(f\"\\nâš ï¸  æç«¯å¼‚å¸¸å€¼ (BMI > 100): {len(extreme_outliers)} ä¸ª\")\n",
    "if len(extreme_outliers) > 0:\n",
    "    print(\"\\nå‰10ä¸ªæç«¯å¼‚å¸¸æ ·æœ¬:\")\n",
    "    print(extreme_outliers[['id', 'age', 'bmi', 'charges']].head(10))\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "print(\"\\n\\nğŸ“Š å¯è§†åŒ–åˆ†æ...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. BMI ç›´æ–¹å›¾\n",
    "axes[0, 0].hist(train['bmi'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(NORMAL_BMI_MAX, color='red', linestyle='--', label=f'æ­£å¸¸ä¸Šé™ ({NORMAL_BMI_MAX})')\n",
    "axes[0, 0].set_xlabel('BMI')\n",
    "axes[0, 0].set_ylabel('é¢‘æ•°')\n",
    "axes[0, 0].set_title('BMI åˆ†å¸ƒï¼ˆåŒ…å«å¼‚å¸¸å€¼ï¼‰')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. BMI ç®±çº¿å›¾\n",
    "axes[0, 1].boxplot(train['bmi'])\n",
    "axes[0, 1].set_ylabel('BMI')\n",
    "axes[0, 1].set_title('BMI ç®±çº¿å›¾')\n",
    "axes[0, 1].axhline(NORMAL_BMI_MAX, color='red', linestyle='--', label='æ­£å¸¸ä¸Šé™')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. è¿‡æ»¤å¼‚å¸¸å€¼åçš„ç›´æ–¹å›¾\n",
    "normal_bmi = train[(train['bmi'] >= NORMAL_BMI_MIN) & (train['bmi'] <= NORMAL_BMI_MAX)]['bmi']\n",
    "axes[1, 0].hist(normal_bmi, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('BMI')\n",
    "axes[1, 0].set_ylabel('é¢‘æ•°')\n",
    "axes[1, 0].set_title('BMI åˆ†å¸ƒï¼ˆè¿‡æ»¤å¼‚å¸¸å€¼åï¼‰')\n",
    "\n",
    "# 4. BMI vs Charges æ•£ç‚¹å›¾\n",
    "axes[1, 1].scatter(train['bmi'], train['charges'], alpha=0.5, s=10)\n",
    "axes[1, 1].axvline(NORMAL_BMI_MAX, color='red', linestyle='--', label='å¼‚å¸¸å€¼åˆ†ç•Œçº¿')\n",
    "axes[1, 1].set_xlabel('BMI')\n",
    "axes[1, 1].set_ylabel('Charges')\n",
    "axes[1, 1].set_title('BMI vs Charges')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ è§‚å¯Ÿè¦ç‚¹:\")\n",
    "print(\"  1. å·¦ä¸Šå›¾ï¼šBMIåˆ†å¸ƒä¸¥é‡å³åï¼Œæœ‰æç«¯å¼‚å¸¸å€¼\")\n",
    "print(\"  2. å³ä¸Šå›¾ï¼šç®±çº¿å›¾æ˜¾ç¤ºå¤§é‡å¼‚å¸¸ç‚¹\")\n",
    "print(\"  3. å·¦ä¸‹å›¾ï¼šè¿‡æ»¤åçš„BMIå‘ˆç°æ­£æ€åˆ†å¸ƒ\")\n",
    "print(\"  4. å³ä¸‹å›¾ï¼šå¼‚å¸¸BMIå€¼çš„æ ·æœ¬è´¹ç”¨åˆ†å¸ƒå¼‚å¸¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4xdroultufd",
   "source": "---\n\n# ç¬¬äºŒéƒ¨åˆ†ï¼šé«˜çº§ç‰¹å¾å·¥ç¨‹ â­â­â­\n\n## ğŸ¯ å­¦ä¹ ç›®æ ‡\n1. ç†è§£ç‰¹å¾å·¥ç¨‹åœ¨æœºå™¨å­¦ä¹ ä¸­çš„æ ¸å¿ƒåœ°ä½\n2. æŒæ¡é¢†åŸŸçŸ¥è¯†ç‰¹å¾çš„åˆ›å»ºæ–¹æ³•\n3. å­¦ä¹ Target EncodingæŠ€æœ¯åŠé˜²æ­¢æ•°æ®æ³„æ¼çš„æ–¹æ³•\n4. æŒæ¡åˆ†ç»„ç»Ÿè®¡ç‰¹å¾çš„åˆ›å»º\n5. å­¦ä¼šè¯„ä¼°ç‰¹å¾çš„æœ‰æ•ˆæ€§\n\n## ğŸ’¡ ä¸ºä»€ä¹ˆç‰¹å¾å·¥ç¨‹æœ€é‡è¦?\n\nåœ¨æœºå™¨å­¦ä¹ ç•Œæœ‰ä¸€å¥åè¨€ï¼š\n> **\"æ•°æ®å’Œç‰¹å¾å†³å®šäº†æœºå™¨å­¦ä¹ çš„ä¸Šé™ï¼Œè€Œæ¨¡å‹å’Œç®—æ³•åªæ˜¯é€¼è¿‘è¿™ä¸ªä¸Šé™è€Œå·²\"**\n\n### ç‰¹å¾å·¥ç¨‹çš„ä»·å€¼\n\n```\nå¥½çš„ç‰¹å¾å·¥ç¨‹ > å¤æ‚çš„æ¨¡å‹è°ƒå‚\n\nä¾‹å­ï¼š\n- ä»…ç”¨çº¿æ€§å›å½’ + ä¼˜ç§€ç‰¹å¾ â†’ å¯èƒ½è¶…è¿‡ éšæœºæ£®æ— + åŸå§‹ç‰¹å¾\n- Kaggleæ¯”èµ›ä¸­ï¼Œå‰å‡ åçš„å·®å¼‚å¾€å¾€åœ¨äºç‰¹å¾å·¥ç¨‹ï¼Œè€Œéæ¨¡å‹é€‰æ‹©\n```\n\n### ç‰¹å¾å·¥ç¨‹é‡‘å­—å¡”\n\n```\nLevel 4: è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ï¼ˆAutoFEï¼‰\n          â†‘ ä½¿ç”¨å·¥å…·è‡ªåŠ¨ç”Ÿæˆç‰¹å¾\n          \nLevel 3: é«˜é˜¶ç‰¹å¾\n          â†‘ Target Encoding, ç»Ÿè®¡èšåˆï¼Œæ·±åº¦äº¤äº’\n          \nLevel 2: åŸºç¡€ç»„åˆç‰¹å¾\n          â†‘ ç®€å•äº¤äº’ï¼Œå¤šé¡¹å¼ï¼Œåˆ†ç®±\n          \nLevel 1: åŸå§‹ç‰¹å¾å¤„ç†\n          â†‘ ç¼ºå¤±å€¼å¡«å……ï¼Œç±»åˆ«ç¼–ç ï¼Œæ ‡å‡†åŒ–\n          \nLevel 0: åŸå§‹æ•°æ®\n```\n\næˆ‘ä»¬ä¼šä»Level 1é€æ­¥å‘ä¸Šæ„å»ºï¼",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "k9zbaepozja",
   "source": "## ğŸ“š ç†è®ºçŸ¥è¯†ï¼šä¿é™©é¢†åŸŸçš„ä¸šåŠ¡é€»è¾‘\n\n### ä¿é™©è´¹ç”¨çš„å†³å®šå› ç´ \n\nåœ¨åŒ»ç–—ä¿é™©å®šä»·ä¸­ï¼Œä¿é™©å…¬å¸ä¸»è¦è€ƒè™‘ä»¥ä¸‹é£é™©å› ç´ ï¼š\n\n#### 1ï¸âƒ£ å¸çƒŸçŠ¶æ€ï¼ˆSmokerï¼‰- æœ€é‡è¦ï¼\n```\nå¸çƒŸè€…çš„åŒ»ç–—è´¹ç”¨é€šå¸¸æ˜¯éå¸çƒŸè€…çš„ 2-3 å€\n\nåŸå› ï¼š\n- æ›´é«˜çš„å¿ƒè¡€ç®¡ç–¾ç—…é£é™©\n- ç™Œç—‡é£é™©å¢åŠ \n- å‘¼å¸ç³»ç»Ÿç–¾ç—…\n- é¢„æœŸå¯¿å‘½ç¼©çŸ­\n```\n\n#### 2ï¸âƒ£ å¹´é¾„ï¼ˆAgeï¼‰\n```\nè´¹ç”¨éšå¹´é¾„å¢é•¿ï¼š\n- 18-30å²ï¼šåŸºç¡€è´¹ç”¨è¾ƒä½\n- 30-50å²ï¼šé€æ¸å¢åŠ \n- 50+å²ï¼šæ˜¾è‘—å¢åŠ ï¼ˆæ…¢æ€§ç—…å¤šå‘æœŸï¼‰\n\nå…³é”®èŠ‚ç‚¹ï¼š\n- 40å²ï¼šå¥åº·é£é™©å¼€å§‹æ˜¾è‘—ä¸Šå‡\n- 60å²ï¼šè¿›å…¥è€å¹´æœŸï¼Œè´¹ç”¨å¤§å¹…å¢åŠ \n```\n\n#### 3ï¸âƒ£ BMIï¼ˆBody Mass Indexï¼‰\n```\nBMIåˆ†ç±»ï¼ˆWHOæ ‡å‡†ï¼‰ï¼š\n- < 18.5ï¼šä½“é‡ä¸è¶³\n- 18.5-25ï¼šæ­£å¸¸\n- 25-30ï¼šè¶…é‡\n- 30-35ï¼šè‚¥èƒ–ï¼ˆIçº§ï¼‰\n- 35-40ï¼šè‚¥èƒ–ï¼ˆIIçº§ï¼‰\n- > 40ï¼šç—…æ€è‚¥èƒ–ï¼ˆIIIçº§ï¼‰\n\nåŒ»ç–—æˆæœ¬å…³ç³»ï¼š\n- æ­£å¸¸BMIï¼šåŸºå‡†è´¹ç”¨\n- è¶…é‡/è‚¥èƒ–ï¼šè´¹ç”¨å¢åŠ  20-50%\n- ç—…æ€è‚¥èƒ–ï¼šè´¹ç”¨å¯èƒ½ç¿»å€\n```\n\n#### 4ï¸âƒ£ äº¤äº’æ•ˆåº”\n```\né‡è¦çš„äº¤äº’ï¼š\n1. å¸çƒŸ Ã— BMI\n   - å¸çƒŸ + è‚¥èƒ– = é£é™©å åŠ ï¼ˆéçº¿æ€§ï¼‰\n   - å½±å“ï¼šå¿ƒè¡€ç®¡ç–¾ç—…é£é™©æŒ‡æ•°çº§å¢é•¿\n\n2. å¸çƒŸ Ã— å¹´é¾„\n   - å¹´é¾„è¶Šå¤§ï¼Œå¸çƒŸçš„ç´¯ç§¯ä¼¤å®³è¶Šæ˜æ˜¾\n   \n3. å¹´é¾„ Ã— BMI\n   - è€å¹´è‚¥èƒ–è€…çš„åŒ»ç–—è´¹ç”¨æ˜¾è‘—é«˜äºå¹´è½»è‚¥èƒ–è€…\n   \n4. æœ‰å­©å­ Ã— å®¶åº­è®¡åˆ’\n   - å®¶åº­æˆå‘˜æ•°é‡å½±å“åŒ»ç–—éœ€æ±‚\n```\n\n### ç‰¹å¾åˆ›å»ºç­–ç•¥\n\nåŸºäºä¸Šè¿°é¢†åŸŸçŸ¥è¯†ï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºï¼š\n\n**é£é™©è¯„åˆ†ç‰¹å¾**ï¼š\n- å¸çƒŸé£é™©åˆ†æ•°\n- å¹´é¾„é£é™©åˆ†æ•°\n- BMIé£é™©åˆ†æ•°\n- ç»¼åˆé£é™©è¯„åˆ†\n\n**äº¤äº’ç‰¹å¾**ï¼š\n- smoker Ã— age\n- smoker Ã— bmi\n- age Ã— bmi\n- smoker Ã— age Ã— bmiï¼ˆä¸‰é˜¶äº¤äº’ï¼‰\n\n**åˆ†ç®±ç‰¹å¾**ï¼š\n- age_groupï¼ˆå¹´é¾„æ®µï¼‰\n- bmi_categoryï¼ˆBMIåˆ†ç±»ï¼‰\n- risk_levelï¼ˆé£é™©ç­‰çº§ï¼‰\n\næ¥ä¸‹æ¥æˆ‘ä»¬ä¼šé€ä¸€å®ç°è¿™äº›ç‰¹å¾ï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bndn6bhxne7",
   "source": "# ========================================\n# Cell 5: é¢†åŸŸçŸ¥è¯†ç‰¹å¾åˆ›å»º\n# ========================================\n\nprint(\"ğŸ—ï¸ åˆ›å»ºé¢†åŸŸçŸ¥è¯†ç‰¹å¾\")\nprint(\"=\"*60)\n\n# åŠ è½½æ¸…æ´—åçš„æ•°æ®\nprint(\"\\nğŸ“‚ åŠ è½½æ¸…æ´—åçš„æ•°æ®...\")\ntrain_clean = pd.read_csv('train_cleaned.csv')\ntest_clean = pd.read_csv('test_cleaned.csv')\nprint(f\"è®­ç»ƒé›†: {train_clean.shape}\")\nprint(f\"æµ‹è¯•é›†: {test_clean.shape}\")\n\ndef create_domain_features(df):\n    \"\"\"\n    åŸºäºä¿é™©é¢†åŸŸçŸ¥è¯†åˆ›å»ºç‰¹å¾\n    \n    å‚æ•°:\n        df: åŸå§‹æ•°æ®æ¡†\n    \n    è¿”å›:\n        df: æ·»åŠ äº†æ–°ç‰¹å¾çš„æ•°æ®æ¡†\n    \"\"\"\n    df = df.copy()\n    \n    # ========================================\n    # 1. å¹´é¾„ç›¸å…³ç‰¹å¾\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºå¹´é¾„ç›¸å…³ç‰¹å¾...\")\n    \n    # å¹´é¾„åˆ†ç»„ï¼ˆåŸºäºåŒ»ç–—é£é™©é˜¶æ®µï¼‰\n    df['age_group'] = pd.cut(\n        df['age'],\n        bins=[0, 18, 30, 40, 50, 65, 100],\n        labels=['teenager', 'young_adult', 'adult', 'middle_age', 'senior', 'elderly']\n    )\n    \n    # å¹´é¾„é£é™©åˆ†æ•°ï¼ˆæŒ‡æ•°å¢é•¿æ¨¡å¼ï¼‰\n    # 40å²ä»¥ä¸‹é£é™©è¾ƒä½ï¼Œä¹‹åå¿«é€Ÿå¢é•¿\n    df['age_risk_score'] = df['age'].apply(lambda x: \n        1.0 if x < 30 else\n        1.5 if x < 40 else\n        2.0 if x < 50 else\n        3.0 if x < 60 else\n        4.5\n    )\n    \n    # æ˜¯å¦è€å¹´äººï¼ˆ60å²ä»¥ä¸Šï¼‰\n    df['is_senior'] = (df['age'] >= 60).astype(int)\n    \n    # æ˜¯å¦é«˜é£é™©å¹´é¾„ï¼ˆ50å²ä»¥ä¸Šï¼‰\n    df['is_high_risk_age'] = (df['age'] >= 50).astype(int)\n    \n    print(f\"  - age_group: 6ä¸ªå¹´é¾„æ®µ\")\n    print(f\"  - age_risk_score: é£é™©è¯„åˆ† (1.0-4.5)\")\n    print(f\"  - is_senior: æ˜¯å¦è€å¹´äºº\")\n    print(f\"  - is_high_risk_age: æ˜¯å¦é«˜é£é™©å¹´é¾„\")\n    \n    # ========================================\n    # 2. BMIç›¸å…³ç‰¹å¾\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºBMIç›¸å…³ç‰¹å¾...\")\n    \n    # BMIåˆ†ç±»ï¼ˆWHOæ ‡å‡†ï¼‰\n    df['bmi_category'] = pd.cut(\n        df['bmi'],\n        bins=[0, 18.5, 25, 30, 35, 40, 100],\n        labels=['underweight', 'normal', 'overweight', 'obese_1', 'obese_2', 'obese_3']\n    )\n    \n    # BMIé£é™©åˆ†æ•°\n    df['bmi_risk_score'] = df['bmi'].apply(lambda x:\n        1.2 if x < 18.5 else  # ä½“é‡ä¸è¶³ä¹Ÿæœ‰é£é™©\n        1.0 if x < 25 else    # æ­£å¸¸\n        1.3 if x < 30 else    # è¶…é‡\n        1.8 if x < 35 else    # è‚¥èƒ–Içº§\n        2.5 if x < 40 else    # è‚¥èƒ–IIçº§\n        3.5                    # ç—…æ€è‚¥èƒ–\n    )\n    \n    # æ˜¯å¦è‚¥èƒ–\n    df['is_obese'] = (df['bmi'] >= 30).astype(int)\n    \n    # æ˜¯å¦ç—…æ€è‚¥èƒ–\n    df['is_severely_obese'] = (df['bmi'] >= 35).astype(int)\n    \n    # BMIåç¦»æ­£å¸¸å€¼çš„ç¨‹åº¦\n    # æ­£å¸¸BMIä¸­å¿ƒå€¼ä¸º21.75ï¼ˆ18.5-25çš„ä¸­ç‚¹ï¼‰\n    df['bmi_deviation'] = np.abs(df['bmi'] - 21.75)\n    \n    print(f\"  - bmi_category: 6ä¸ªBMIåˆ†ç±»\")\n    print(f\"  - bmi_risk_score: é£é™©è¯„åˆ† (1.0-3.5)\")\n    print(f\"  - is_obese: æ˜¯å¦è‚¥èƒ–\")\n    print(f\"  - is_severely_obese: æ˜¯å¦ç—…æ€è‚¥èƒ–\")\n    print(f\"  - bmi_deviation: BMIåç¦»åº¦\")\n    \n    # ========================================\n    # 3. å¸çƒŸç›¸å…³ç‰¹å¾\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºå¸çƒŸç›¸å…³ç‰¹å¾...\")\n    \n    # å…ˆç¼–ç smokerï¼ˆå¦‚æœè¿˜æ˜¯å­—ç¬¦ä¸²ï¼‰\n    if df['smoker'].dtype == 'object':\n        df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n    \n    # å¸çƒŸé£é™©åˆ†æ•°ï¼ˆå¸çƒŸè€…é£é™©æ˜¯éå¸çƒŸè€…çš„2.5å€ï¼‰\n    df['smoker_risk_score'] = df['smoker'].apply(lambda x: 2.5 if x == 1 else 1.0)\n    \n    print(f\"  - smoker: ç¼–ç ä¸º 0/1\")\n    print(f\"  - smoker_risk_score: é£é™©è¯„åˆ† (1.0 æˆ– 2.5)\")\n    \n    # ========================================\n    # 4. å®¶åº­ç›¸å…³ç‰¹å¾\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºå®¶åº­ç›¸å…³ç‰¹å¾...\")\n    \n    # æ˜¯å¦æœ‰å­©å­\n    df['has_children'] = (df['children'] > 0).astype(int)\n    \n    # å®¶åº­è§„æ¨¡ï¼ˆå‡è®¾æœ‰é…å¶ï¼‰\n    df['family_size'] = df['children'] + 2  # æœ¬äºº + é…å¶ + å­©å­\n    \n    # å¤šå­å¥³å®¶åº­\n    df['large_family'] = (df['children'] >= 3).astype(int)\n    \n    print(f\"  - has_children: æ˜¯å¦æœ‰å­©å­\")\n    print(f\"  - family_size: å®¶åº­è§„æ¨¡\")\n    print(f\"  - large_family: æ˜¯å¦å¤šå­å¥³\")\n    \n    # ========================================\n    # 5. ç»¼åˆé£é™©è¯„åˆ†\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºç»¼åˆé£é™©è¯„åˆ†...\")\n    \n    # æ–¹æ³•1: ç®€å•åŠ æƒå¹³å‡\n    df['risk_score_simple'] = (\n        df['age_risk_score'] * 0.3 +\n        df['bmi_risk_score'] * 0.3 +\n        df['smoker_risk_score'] * 0.4  # å¸çƒŸå½±å“æœ€å¤§\n    )\n    \n    # æ–¹æ³•2: ä¹˜æ³•äº¤äº’ï¼ˆé£é™©å åŠ ï¼‰\n    df['risk_score_multiplicative'] = (\n        df['age_risk_score'] *\n        df['bmi_risk_score'] *\n        df['smoker_risk_score']\n    )\n    \n    # é«˜é£é™©äººç¾¤æ ‡è¯†ï¼ˆå¤šä¸ªé«˜é£é™©å› ç´ å åŠ ï¼‰\n    df['high_risk_count'] = (\n        df['is_high_risk_age'] +\n        df['is_obese'] +\n        df['smoker']\n    )\n    \n    # æ˜¯å¦æé«˜é£é™©ï¼ˆ3ä¸ªå› ç´ éƒ½æœ‰ï¼‰\n    df['is_very_high_risk'] = (df['high_risk_count'] >= 3).astype(int)\n    \n    print(f\"  - risk_score_simple: åŠ æƒé£é™©åˆ†\")\n    print(f\"  - risk_score_multiplicative: ä¹˜æ³•é£é™©åˆ†\")\n    print(f\"  - high_risk_count: é«˜é£é™©å› ç´ æ•°é‡\")\n    print(f\"  - is_very_high_risk: æ˜¯å¦æé«˜é£é™©\")\n    \n    # ========================================\n    # 6. äº¤äº’ç‰¹å¾\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºäº¤äº’ç‰¹å¾...\")\n    \n    # æ ¸å¿ƒäº¤äº’ï¼ˆè¿™äº›äº¤äº’åœ¨ä¿é™©å®šä»·ä¸­ç‰¹åˆ«é‡è¦ï¼‰\n    df['smoker_age'] = df['smoker'] * df['age']\n    df['smoker_bmi'] = df['smoker'] * df['bmi']\n    df['age_bmi'] = df['age'] * df['bmi']\n    \n    # ä¸‰é˜¶äº¤äº’ï¼ˆå¸çƒŸ Ã— å¹´é¾„ Ã— BMIï¼‰\n    df['smoker_age_bmi'] = df['smoker'] * df['age'] * df['bmi']\n    \n    # å¸çƒŸ Ã— è‚¥èƒ–ï¼ˆç‰¹åˆ«å±é™©çš„ç»„åˆï¼‰\n    df['smoker_and_obese'] = df['smoker'] * df['is_obese']\n    \n    # è€å¹´ Ã— è‚¥èƒ–\n    df['senior_and_obese'] = df['is_senior'] * df['is_obese']\n    \n    # å¸çƒŸ Ã— è€å¹´\n    df['smoker_and_senior'] = df['smoker'] * df['is_senior']\n    \n    print(f\"  - smoker_age, smoker_bmi, age_bmi: äºŒé˜¶äº¤äº’\")\n    print(f\"  - smoker_age_bmi: ä¸‰é˜¶äº¤äº’\")\n    print(f\"  - smoker_and_obese: å¸çƒŸÃ—è‚¥èƒ–\")\n    print(f\"  - senior_and_obese: è€å¹´Ã—è‚¥èƒ–\")\n    print(f\"  - smoker_and_senior: å¸çƒŸÃ—è€å¹´\")\n    \n    # ========================================\n    # 7. å¤šé¡¹å¼ç‰¹å¾\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾...\")\n    \n    # å¹´é¾„å’ŒBMIçš„å¹³æ–¹ï¼ˆæ•æ‰éçº¿æ€§å…³ç³»ï¼‰\n    df['age_squared'] = df['age'] ** 2\n    df['bmi_squared'] = df['bmi'] ** 2\n    \n    # ç«‹æ–¹é¡¹ï¼ˆæ›´é«˜é˜¶çš„éçº¿æ€§ï¼‰\n    df['age_cubed'] = df['age'] ** 3\n    \n    print(f\"  - age_squared, bmi_squared: å¹³æ–¹é¡¹\")\n    print(f\"  - age_cubed: ç«‹æ–¹é¡¹\")\n    \n    return df\n\n# åº”ç”¨ç‰¹å¾å·¥ç¨‹\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"å¼€å§‹ç‰¹å¾å·¥ç¨‹...\")\nprint(\"=\"*60)\n\ntrain_fe = create_domain_features(train_clean)\ntest_fe = create_domain_features(test_clean)\n\nprint(\"\\n\\nâœ… é¢†åŸŸçŸ¥è¯†ç‰¹å¾åˆ›å»ºå®Œæˆï¼\")\nprint(\"=\"*60)\nprint(f\"\\nåŸå§‹ç‰¹å¾æ•°: {train_clean.shape[1]}\")\nprint(f\"æ–°å¢ç‰¹å¾å: {train_fe.shape[1]}\")\nprint(f\"æ–°å¢ç‰¹å¾æ•°: {train_fe.shape[1] - train_clean.shape[1]}\")\n\n# æŸ¥çœ‹æ–°ç‰¹å¾\nprint(\"\\nğŸ“‹ æ–°å¢çš„ç‰¹å¾åˆ—è¡¨:\")\nnew_cols = [col for col in train_fe.columns if col not in train_clean.columns]\nfor i, col in enumerate(new_cols, 1):\n    print(f\"  {i:2d}. {col}\")\n\n# æŸ¥çœ‹éƒ¨åˆ†ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯\nprint(\"\\nğŸ“Š éƒ¨åˆ†æ–°ç‰¹å¾çš„ç»Ÿè®¡ä¿¡æ¯:\")\nfeature_cols = ['age_risk_score', 'bmi_risk_score', 'smoker_risk_score', \n                'risk_score_simple', 'risk_score_multiplicative']\nprint(train_fe[feature_cols].describe())\n\n# æŸ¥çœ‹æé«˜é£é™©äººç¾¤å æ¯”\nprint(f\"\\nâš ï¸  æé«˜é£é™©äººç¾¤å æ¯”: {train_fe['is_very_high_risk'].mean()*100:.2f}%\")\nprint(f\"   (åŒæ—¶æ»¡è¶³ï¼šå¹´é¾„â‰¥50 + BMIâ‰¥30 + å¸çƒŸ)\")\n\n# ä¿å­˜ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®ï¼ˆæš‚æ—¶ä¸ä¿å­˜ç±»åˆ«ç‰¹å¾çš„one-hotç¼–ç ç‰ˆæœ¬ï¼‰\nprint(\"\\nğŸ’¾ ä¿å­˜ç‰¹å¾å·¥ç¨‹åçš„æ•°æ®...\")\ntrain_fe.to_csv('train_domain_features.csv', index=False)\ntest_fe.to_csv('test_domain_features.csv', index=False)\nprint(\"âœ… å·²ä¿å­˜:\")\nprint(\"  - train_domain_features.csv\")\nprint(\"  - test_domain_features.csv\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "stfderigb5o",
   "source": "## ğŸ“š ç†è®ºçŸ¥è¯†ï¼šTarget Encoding\n\n### ä»€ä¹ˆæ˜¯ Target Encodingï¼Ÿ\n\nTarget Encodingï¼ˆç›®æ ‡ç¼–ç ï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç±»åˆ«å˜é‡ç¼–ç æ–¹æ³•ï¼Œç‰¹åˆ«é€‚ç”¨äºï¼š\n- é«˜åŸºæ•°ç±»åˆ«ç‰¹å¾ï¼ˆç±»åˆ«æ•°é‡å¾ˆå¤šï¼‰\n- ç±»åˆ«ç‰¹å¾ä¸ç›®æ ‡å˜é‡æœ‰å¼ºç›¸å…³æ€§çš„æƒ…å†µ\n\n### åŸç†\n\n**åŸºæœ¬æ€æƒ³**ï¼šç”¨è¯¥ç±»åˆ«å¯¹åº”çš„ç›®æ ‡å˜é‡å¹³å‡å€¼æ¥æ›¿ä»£ç±»åˆ«å€¼\n\n```python\n# ä¾‹å¦‚ï¼šregion åˆ—\nnortheast â†’ è¯¥regionä¸‹æ‰€æœ‰æ ·æœ¬çš„å¹³å‡charges\nnorthwest â†’ è¯¥regionä¸‹æ‰€æœ‰æ ·æœ¬çš„å¹³å‡charges\nsoutheast â†’ è¯¥regionä¸‹æ‰€æœ‰æ ·æœ¬çš„å¹³å‡charges\nsouthwest â†’ è¯¥regionä¸‹æ‰€æœ‰æ ·æœ¬çš„å¹³å‡charges\n```\n\n### ä¸ºä»€ä¹ˆæ¯” One-Hot Encoding æ›´å¥½ï¼Ÿ\n\n| æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ |\n|------|------|------|\n| **One-Hot** | ç®€å•ï¼Œæ— æ•°æ®æ³„æ¼ | é«˜åŸºæ•°æ—¶ç»´åº¦çˆ†ç‚¸ï¼Œæ— æ³•æ•æ‰ç±»åˆ«çš„\"æ•°å€¼æ„ä¹‰\" |\n| **Label Encoding** | ç»´åº¦ä¸å¢åŠ  | å¼•å…¥äº†ä¸å­˜åœ¨çš„é¡ºåºå…³ç³» |\n| **Target Encoding** | ç»´åº¦ä¸å¢åŠ ï¼Œæ•æ‰ç±»åˆ«ä¸ç›®æ ‡çš„å…³ç³» | éœ€è¦é˜²æ­¢è¿‡æ‹Ÿåˆå’Œæ•°æ®æ³„æ¼ |\n\n### ç¤ºä¾‹å¯¹æ¯”\n\nå‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹æ•°æ®ï¼š\n\n```\nregion     | charges\n-----------|---------\nnortheast  | 10000\nnortheast  | 12000\nnorthwest  | 8000\nnorthwest  | 9000\n```\n\n**One-Hot Encoding**:\n```\nnortheast_0  northeast_1  northwest_0  northwest_1\n1            0            0            0            â†’ 10000\n1            0            0            0            â†’ 12000\n0            1            0            0            â†’ 8000\n0            1            0            0            â†’ 9000\n```\n\n**Target Encoding**:\n```\nregion_encoded\n11000  â†’ (10000+12000)/2\n11000\n8500   â†’ (8000+9000)/2\n8500\n```\n\n**ä¼˜åŠ¿æ˜æ˜¾**ï¼šTarget Encodingç›´æ¥å‘Šè¯‰æ¨¡å‹\"northeastçš„å¹³å‡è´¹ç”¨æ˜¯11000\"ï¼\n\n### âš ï¸ å…³é”®é—®é¢˜ï¼šæ•°æ®æ³„æ¼\n\n**ä»€ä¹ˆæ˜¯æ•°æ®æ³„æ¼ï¼Ÿ**\n\nå¦‚æœæˆ‘ä»¬ç›´æ¥ç”¨å…¨å±€å¹³å‡å€¼ç¼–ç ï¼Œä¼šå‘ç”Ÿï¼š\n```python\n# é”™è¯¯åšæ³•âŒ\ntrain['region_encoded'] = train.groupby('region')['charges'].transform('mean')\n```\n\n**é—®é¢˜**ï¼šæ¯ä¸ªæ ·æœ¬çš„ç¼–ç å€¼åŒ…å«äº†å®ƒè‡ªå·±çš„ç›®æ ‡å€¼ä¿¡æ¯ï¼\n- è¿™åœ¨è®­ç»ƒæ—¶ä¼šè®©æ¨¡å‹\"ä½œå¼Š\"\n- åœ¨æµ‹è¯•æ—¶æ— æ³•å¤ç°ï¼ˆæµ‹è¯•é›†æ²¡æœ‰ç›®æ ‡å€¼ï¼‰\n\n### âœ… æ­£ç¡®åšæ³•ï¼šK-Fold Target Encoding\n\n**æ ¸å¿ƒæ€æƒ³**ï¼šä½¿ç”¨äº¤å‰éªŒè¯æ–¹å¼ï¼Œç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„ç¼–ç å€¼ä¸åŒ…å«è‡ªå·±çš„ç›®æ ‡å€¼\n\n```python\n# ä¼ªä»£ç \nfor fold in KFold:\n    train_idx, val_idx = fold\n    \n    # ç”¨è®­ç»ƒé›†è®¡ç®—å‡å€¼\n    encoding_map = train[train_idx].groupby('region')['charges'].mean()\n    \n    # åº”ç”¨åˆ°éªŒè¯é›†ï¼ˆéªŒè¯é›†æ ·æœ¬çš„ç¼–ç ä¸åŒ…å«è‡ªå·±ï¼‰\n    train.loc[val_idx, 'region_encoded'] = train.loc[val_idx, 'region'].map(encoding_map)\n```\n\n### å¢å¼ºæŠ€å·§\n\n#### 1. å¹³æ»‘å¤„ç†ï¼ˆSmoothingï¼‰\n\né¿å…å°æ ·æœ¬ç±»åˆ«çš„ä¸ç¨³å®šä¼°è®¡ï¼š\n\n```python\nglobal_mean = train['charges'].mean()\ncategory_stats = train.groupby('region').agg({\n    'charges': ['mean', 'count']\n})\n\n# è´å¶æ–¯å¹³æ»‘\nsmoothing_factor = 10  # è¶…å‚æ•°\nsmoothed_mean = (\n    (category_stats['mean'] * category_stats['count'] + global_mean * smoothing_factor) /\n    (category_stats['count'] + smoothing_factor)\n)\n```\n\n**ä½œç”¨**ï¼šå°æ ·æœ¬ç±»åˆ«ä¼šå‘å…¨å±€å‡å€¼\"æ”¶ç¼©\"\n\n#### 2. æ·»åŠ å™ªå£°\n\né˜²æ­¢è¿‡æ‹Ÿåˆï¼š\n\n```python\nencoded_values = encoded_values + np.random.normal(0, std, size=len(encoded_values))\n```\n\næ¥ä¸‹æ¥æˆ‘ä»¬ä¼šå®ç°å®Œæ•´çš„ K-Fold Target Encodingï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "b2tjqm9wkvh",
   "source": "# ========================================\n# Cell 6: Target Encoding å®ç°\n# ========================================\n\nprint(\"ğŸ¯ å®ç° K-Fold Target Encoding\")\nprint(\"=\"*60)\n\nfrom sklearn.model_selection import KFold\n\nclass TargetEncoder:\n    \"\"\"\n    K-Fold Target Encoding ç¼–ç å™¨\n    \n    é˜²æ­¢æ•°æ®æ³„æ¼çš„å…³é”®ï¼š\n    - è®­ç»ƒé›†ï¼šä½¿ç”¨K-Foldæ–¹å¼ï¼Œæ¯ä¸ªæ ·æœ¬çš„ç¼–ç ä¸åŒ…å«è‡ªå·±\n    - æµ‹è¯•é›†ï¼šä½¿ç”¨å…¨éƒ¨è®­ç»ƒæ•°æ®çš„ç»Ÿè®¡å€¼\n    \"\"\"\n    \n    def __init__(self, columns, n_splits=5, smoothing=10, random_state=42):\n        \"\"\"\n        å‚æ•°:\n            columns: éœ€è¦ç¼–ç çš„åˆ—ååˆ—è¡¨\n            n_splits: KæŠ˜æ•°é‡\n            smoothing: å¹³æ»‘å‚æ•°ï¼ˆè¶Šå¤§è¶Šå‘å…¨å±€å‡å€¼æ”¶ç¼©ï¼‰\n            random_state: éšæœºç§å­\n        \"\"\"\n        self.columns = columns\n        self.n_splits = n_splits\n        self.smoothing = smoothing\n        self.random_state = random_state\n        self.global_mean = None\n        self.encoding_maps = {}  # å­˜å‚¨æ¯ä¸ªç±»åˆ«ç‰¹å¾çš„ç¼–ç æ˜ å°„\n        \n    def fit_transform(self, X, y):\n        \"\"\"\n        å¯¹è®­ç»ƒé›†è¿›è¡ŒK-Foldç¼–ç \n        \n        å‚æ•°:\n            X: ç‰¹å¾æ•°æ®æ¡†\n            y: ç›®æ ‡å˜é‡\n        \n        è¿”å›:\n            X_encoded: ç¼–ç åçš„æ•°æ®æ¡†\n        \"\"\"\n        X_encoded = X.copy()\n        self.global_mean = y.mean()\n        \n        print(f\"\\nå…¨å±€å¹³å‡å€¼: {self.global_mean:.2f}\")\n        print(f\"ä½¿ç”¨ {self.n_splits}-Fold ç¼–ç ï¼Œå¹³æ»‘å‚æ•°={self.smoothing}\")\n        \n        for col in self.columns:\n            print(f\"\\nç¼–ç ç‰¹å¾: {col}\")\n            \n            # ä¸ºæ¯ä¸ªåˆ—åˆ›å»ºæ–°çš„ç¼–ç åˆ—å\n            encoded_col = f'{col}_encoded'\n            X_encoded[encoded_col] = 0.0\n            \n            # K-Foldç¼–ç \n            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n            \n            for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n                # åœ¨è®­ç»ƒé›†ä¸Šè®¡ç®—ç»Ÿè®¡å€¼\n                train_stats = pd.DataFrame({\n                    'mean': y.iloc[train_idx].groupby(X[col].iloc[train_idx]).mean(),\n                    'count': y.iloc[train_idx].groupby(X[col].iloc[train_idx]).count()\n                })\n                \n                # å¹³æ»‘å¤„ç†ï¼ˆè´å¶æ–¯å¹³æ»‘ï¼‰\n                smoothed_mean = (\n                    (train_stats['mean'] * train_stats['count'] + \n                     self.global_mean * self.smoothing) /\n                    (train_stats['count'] + self.smoothing)\n                )\n                \n                # åº”ç”¨åˆ°éªŒè¯é›†\n                X_encoded.loc[X.index[val_idx], encoded_col] = (\n                    X[col].iloc[val_idx].map(smoothed_mean).fillna(self.global_mean)\n                )\n            \n            # ä¿å­˜å®Œæ•´è®­ç»ƒé›†çš„ç¼–ç æ˜ å°„ï¼ˆç”¨äºæµ‹è¯•é›†ï¼‰\n            full_stats = pd.DataFrame({\n                'mean': y.groupby(X[col]).mean(),\n                'count': y.groupby(X[col]).count()\n            })\n            \n            self.encoding_maps[col] = (\n                (full_stats['mean'] * full_stats['count'] + \n                 self.global_mean * self.smoothing) /\n                (full_stats['count'] + self.smoothing)\n            )\n            \n            # æ˜¾ç¤ºç¼–ç ç»“æœ\n            print(f\"  ç±»åˆ«æ•°é‡: {X[col].nunique()}\")\n            print(f\"  ç¼–ç å€¼èŒƒå›´: [{X_encoded[encoded_col].min():.2f}, {X_encoded[encoded_col].max():.2f}]\")\n            print(f\"  æ ·æœ¬ç¼–ç æ˜ å°„ï¼ˆå‰5ä¸ªç±»åˆ«ï¼‰:\")\n            for category in X[col].unique()[:5]:\n                mean_val = self.encoding_maps[col].get(category, self.global_mean)\n                print(f\"    {category}: {mean_val:.2f}\")\n        \n        return X_encoded\n    \n    def transform(self, X):\n        \"\"\"\n        å¯¹æµ‹è¯•é›†è¿›è¡Œç¼–ç \n        \n        å‚æ•°:\n            X: æµ‹è¯•é›†ç‰¹å¾æ•°æ®æ¡†\n        \n        è¿”å›:\n            X_encoded: ç¼–ç åçš„æ•°æ®æ¡†\n        \"\"\"\n        X_encoded = X.copy()\n        \n        for col in self.columns:\n            encoded_col = f'{col}_encoded'\n            # ä½¿ç”¨è®­ç»ƒé›†çš„ç¼–ç æ˜ å°„\n            X_encoded[encoded_col] = (\n                X[col].map(self.encoding_maps[col]).fillna(self.global_mean)\n            )\n        \n        return X_encoded\n\n\n# ========================================\n# åº”ç”¨ Target Encoding\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"åº”ç”¨ Target Encoding åˆ°ç±»åˆ«ç‰¹å¾\")\nprint(\"=\"*60)\n\n# åŠ è½½æ•°æ®\ntrain_fe = pd.read_csv('train_domain_features.csv')\ntest_fe = pd.read_csv('test_domain_features.csv')\n\n# éœ€è¦è¿›è¡Œtarget encodingçš„ç±»åˆ«ç‰¹å¾\ncategorical_cols = ['sex', 'region']\n\nprint(f\"\\nå¾…ç¼–ç çš„ç±»åˆ«ç‰¹å¾: {categorical_cols}\")\n\n# åˆå§‹åŒ–ç¼–ç å™¨\nencoder = TargetEncoder(\n    columns=categorical_cols,\n    n_splits=5,\n    smoothing=10,  # å¹³æ»‘å‚æ•°ï¼ˆå¯ä»¥è°ƒæ•´ï¼‰\n    random_state=SEED\n)\n\n# ç¼–ç è®­ç»ƒé›†ï¼ˆK-Foldæ–¹å¼ï¼‰\nX_train = train_fe.drop(['charges', 'id'], axis=1, errors='ignore')\ny_train = train_fe['charges']\n\nX_train_encoded = encoder.fit_transform(X_train, y_train)\n\n# ç¼–ç æµ‹è¯•é›†\nX_test = test_fe.drop(['id'], axis=1, errors='ignore')\nX_test_encoded = encoder.transform(X_test)\n\nprint(\"\\nâœ… Target Encoding å®Œæˆ!\")\nprint(f\"è®­ç»ƒé›†ç¼–ç åå½¢çŠ¶: {X_train_encoded.shape}\")\nprint(f\"æµ‹è¯•é›†ç¼–ç åå½¢çŠ¶: {X_test_encoded.shape}\")\n\n# å¯¹æ¯”åŸå§‹ç±»åˆ«ç‰¹å¾å’Œç¼–ç åçš„ç‰¹å¾\nprint(\"\\nğŸ“Š ç¼–ç æ•ˆæœå¯¹æ¯”ï¼ˆä»¥ region ä¸ºä¾‹ï¼‰:\")\nprint(\"\\nåŸå§‹ region åˆ†å¸ƒ:\")\nprint(train_fe['region'].value_counts())\n\nprint(\"\\nç¼–ç å region_encoded çš„ç»Ÿè®¡:\")\ncomparison = train_fe[['region', 'charges']].copy()\ncomparison['region_encoded'] = X_train_encoded['region_encoded']\nregion_comparison = comparison.groupby('region').agg({\n    'charges': ['mean', 'count'],\n    'region_encoded': 'mean'\n}).round(2)\nprint(region_comparison)\n\nprint(\"\\nğŸ’¡ è§‚å¯Ÿè¦ç‚¹:\")\nprint(\"  1. region_encoded çš„å€¼æ¥è¿‘è¯¥regionçš„å¹³å‡charges\")\nprint(\"  2. ä½†ç”±äºK-Foldå’Œå¹³æ»‘ï¼Œæ¯ä¸ªæ ·æœ¬çš„ç¼–ç å€¼ç•¥æœ‰ä¸åŒ\")\nprint(\"  3. è¿™æ ·æ—¢ä¿ç•™äº†ç±»åˆ«ä¿¡æ¯ï¼Œåˆé¿å…äº†æ•°æ®æ³„æ¼\")\n\n# ä¿å­˜ç¼–ç åçš„æ•°æ®\ntrain_fe_encoded = train_fe[['id', 'charges']].copy()\ntrain_fe_encoded = pd.concat([train_fe_encoded, X_train_encoded], axis=1)\n\ntest_fe_encoded = test_fe[['id']].copy()\ntest_fe_encoded = pd.concat([test_fe_encoded, X_test_encoded], axis=1)\n\ntrain_fe_encoded.to_csv('train_target_encoded.csv', index=False)\ntest_fe_encoded.to_csv('test_target_encoded.csv', index=False)\n\nprint(\"\\nğŸ’¾ å·²ä¿å­˜:\")\nprint(\"  - train_target_encoded.csv\")\nprint(\"  - test_target_encoded.csv\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "08qphbix49hu",
   "source": "## ğŸ“š ç†è®ºçŸ¥è¯†ï¼šåˆ†ç»„ç»Ÿè®¡ç‰¹å¾\n\n### ä»€ä¹ˆæ˜¯åˆ†ç»„ç»Ÿè®¡ç‰¹å¾ï¼Ÿ\n\nåˆ†ç»„ç»Ÿè®¡ç‰¹å¾ï¼ˆAggregation Featuresï¼‰æ˜¯æŒ‡æŒ‰æŸä¸ªæˆ–æŸå‡ ä¸ªç±»åˆ«åˆ†ç»„åï¼Œå¯¹æ•°å€¼ç‰¹å¾è¿›è¡Œç»Ÿè®¡è®¡ç®—å¾—åˆ°çš„ç‰¹å¾ã€‚\n\n### æ ¸å¿ƒæ€æƒ³\n\n```\nä¸åŒçš„ç¾¤ä½“æœ‰ä¸åŒçš„ç‰¹å¾åˆ†å¸ƒ\n\nä¾‹å¦‚ï¼š\n- å¸çƒŸè€…çš„å¹³å‡BMIæ˜¯å¤šå°‘ï¼Ÿ\n- ä¸åŒåœ°åŒºçš„å¹³å‡å¹´é¾„æ˜¯å¤šå°‘ï¼Ÿ\n- æ¯ä¸ªå¹´é¾„æ®µä¸­å¸çƒŸè€…çš„æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ\n```\n\n### å¸¸ç”¨çš„ç»Ÿè®¡é‡\n\n| ç»Ÿè®¡é‡ | å«ä¹‰ | é€‚ç”¨åœºæ™¯ |\n|--------|------|----------|\n| **mean** | å¹³å‡å€¼ | æ•æ‰ä¸­å¿ƒè¶‹åŠ¿ |\n| **median** | ä¸­ä½æ•° | å¯¹å¼‚å¸¸å€¼é²æ£’ |\n| **std** | æ ‡å‡†å·® | è¡¡é‡ç¦»æ•£ç¨‹åº¦ |\n| **min/max** | æœ€å°/æœ€å¤§å€¼ | æ•æ‰æå€¼ä¿¡æ¯ |\n| **count** | æ•°é‡ | ç¾¤ä½“å¤§å°ä¿¡æ¯ |\n| **sum** | æ€»å’Œ | ç´¯ç§¯æ•ˆåº” |\n\n### ç¤ºä¾‹\n\n```python\n# æŒ‰ smoker åˆ†ç»„ï¼Œè®¡ç®— BMI çš„ç»Ÿè®¡é‡\ngroup_stats = df.groupby('smoker')['bmi'].agg(['mean', 'std', 'median'])\n\n# å°†ç»Ÿè®¡é‡æ˜ å°„å›åŸæ•°æ®\ndf['smoker_bmi_mean'] = df['smoker'].map(group_stats['mean'])\ndf['smoker_bmi_std'] = df['smoker'].map(group_stats['std'])\n```\n\n### ä¸šåŠ¡å«ä¹‰ç¤ºä¾‹\n\nåœ¨æˆ‘ä»¬çš„ä¿é™©æ•°æ®ä¸­ï¼š\n\n**æŒ‰å¸çƒŸçŠ¶æ€åˆ†ç»„**ï¼š\n- `smoker_age_mean`: å¸çƒŸè€…/éå¸çƒŸè€…çš„å¹³å‡å¹´é¾„\n  - å¦‚æœå¸çƒŸè€…æ™®éå¹´è½»ï¼Œå¯èƒ½é£é™©ä¸åŒ\n  \n- `smoker_bmi_mean`: å¸çƒŸè€…/éå¸çƒŸè€…çš„å¹³å‡BMI\n  - å¸çƒŸä¸è‚¥èƒ–çš„ç›¸å…³æ€§\n\n**æŒ‰åœ°åŒºåˆ†ç»„**ï¼š\n- `region_age_mean`: è¯¥åœ°åŒºçš„å¹³å‡å¹´é¾„\n  - åæ˜ åœ°åŒºäººå£ç»“æ„\n  \n- `region_bmi_std`: è¯¥åœ°åŒºBMIçš„æ ‡å‡†å·®\n  - åæ˜ åœ°åŒºå¥åº·çŠ¶å†µçš„å·®å¼‚æ€§\n\n**å¤šç»´åˆ†ç»„**ï¼š\n- `smoker_region_charges_mean`: ä¸åŒåœ°åŒºå¸çƒŸè€…çš„å¹³å‡è´¹ç”¨\n  - æ•æ‰åœ°åŒºÃ—å¸çƒŸçš„äº¤äº’æ•ˆåº”\n\n### ä¸Target Encodingçš„åŒºåˆ«\n\n| ç‰¹å¾ç±»å‹ | åˆ†ç»„é”® | ç»Ÿè®¡å¯¹è±¡ | ç”¨é€” |\n|----------|--------|----------|------|\n| **Target Encoding** | ç±»åˆ«ç‰¹å¾ | ç›®æ ‡å˜é‡ | ç›´æ¥ç¼–ç ç±»åˆ«ä¿¡æ¯ |\n| **åˆ†ç»„ç»Ÿè®¡** | ç±»åˆ«ç‰¹å¾ | å…¶ä»–æ•°å€¼ç‰¹å¾ | æ•æ‰ç¾¤ä½“ç‰¹å¾åˆ†å¸ƒ |\n\nä¸¤è€…å¯ä»¥äº’è¡¥ï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "udbmaso9ho9",
   "source": "# ========================================\n# Cell 7: åˆ†ç»„ç»Ÿè®¡ç‰¹å¾åˆ›å»º\n# ========================================\n\nprint(\"ğŸ“Š åˆ›å»ºåˆ†ç»„ç»Ÿè®¡ç‰¹å¾\")\nprint(\"=\"*60)\n\ndef create_aggregation_features(df, is_train=True, train_stats=None):\n    \"\"\"\n    åˆ›å»ºåˆ†ç»„ç»Ÿè®¡ç‰¹å¾\n    \n    å‚æ•°:\n        df: æ•°æ®æ¡†\n        is_train: æ˜¯å¦æ˜¯è®­ç»ƒé›†\n        train_stats: è®­ç»ƒé›†çš„ç»Ÿè®¡å­—å…¸ï¼ˆç”¨äºæµ‹è¯•é›†ï¼‰\n    \n    è¿”å›:\n        df: æ·»åŠ ç»Ÿè®¡ç‰¹å¾åçš„æ•°æ®æ¡†\n        stats_dict: ç»Ÿè®¡å­—å…¸ï¼ˆä»…è®­ç»ƒé›†è¿”å›ï¼‰\n    \"\"\"\n    df = df.copy()\n    stats_dict = {} if is_train else None\n    \n    # ç¡®ä¿smokeræ˜¯æ•°å€¼å‹\n    if df['smoker'].dtype == 'object':\n        df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n    \n    # ========================================\n    # 1. æŒ‰ smoker åˆ†ç»„\n    # ========================================\n    print(\"\\nğŸ¯ æŒ‰ smoker åˆ†ç»„çš„ç»Ÿè®¡ç‰¹å¾...\")\n    \n    if is_train:\n        # è®¡ç®—ç»Ÿè®¡é‡\n        smoker_age_stats = df.groupby('smoker')['age'].agg(['mean', 'std', 'median']).add_prefix('smoker_age_')\n        smoker_bmi_stats = df.groupby('smoker')['bmi'].agg(['mean', 'std', 'median']).add_prefix('smoker_bmi_')\n        smoker_children_stats = df.groupby('smoker')['children'].agg(['mean', 'sum']).add_prefix('smoker_children_')\n        \n        stats_dict['smoker_age'] = smoker_age_stats\n        stats_dict['smoker_bmi'] = smoker_bmi_stats\n        stats_dict['smoker_children'] = smoker_children_stats\n    else:\n        smoker_age_stats = train_stats['smoker_age']\n        smoker_bmi_stats = train_stats['smoker_bmi']\n        smoker_children_stats = train_stats['smoker_children']\n    \n    # æ˜ å°„åˆ°åŸæ•°æ®\n    for col in smoker_age_stats.columns:\n        df[col] = df['smoker'].map(smoker_age_stats[col])\n    for col in smoker_bmi_stats.columns:\n        df[col] = df['smoker'].map(smoker_bmi_stats[col])\n    for col in smoker_children_stats.columns:\n        df[col] = df['smoker'].map(smoker_children_stats[col])\n    \n    print(f\"  - æŒ‰smokeråˆ†ç»„: {len(smoker_age_stats.columns) + len(smoker_bmi_stats.columns) + len(smoker_children_stats.columns)} ä¸ªç‰¹å¾\")\n    \n    # ========================================\n    # 2. æŒ‰ region åˆ†ç»„\n    # ========================================\n    print(\"\\nğŸ¯ æŒ‰ region åˆ†ç»„çš„ç»Ÿè®¡ç‰¹å¾...\")\n    \n    if is_train:\n        region_age_stats = df.groupby('region')['age'].agg(['mean', 'std']).add_prefix('region_age_')\n        region_bmi_stats = df.groupby('region')['bmi'].agg(['mean', 'std']).add_prefix('region_bmi_')\n        region_smoker_stats = df.groupby('region')['smoker'].agg(['mean', 'sum']).add_prefix('region_smoker_')\n        \n        stats_dict['region_age'] = region_age_stats\n        stats_dict['region_bmi'] = region_bmi_stats\n        stats_dict['region_smoker'] = region_smoker_stats\n    else:\n        region_age_stats = train_stats['region_age']\n        region_bmi_stats = train_stats['region_bmi']\n        region_smoker_stats = train_stats['region_smoker']\n    \n    for col in region_age_stats.columns:\n        df[col] = df['region'].map(region_age_stats[col])\n    for col in region_bmi_stats.columns:\n        df[col] = df['region'].map(region_bmi_stats[col])\n    for col in region_smoker_stats.columns:\n        df[col] = df['region'].map(region_smoker_stats[col])\n    \n    print(f\"  - æŒ‰regionåˆ†ç»„: {len(region_age_stats.columns) + len(region_bmi_stats.columns) + len(region_smoker_stats.columns)} ä¸ªç‰¹å¾\")\n    \n    # ========================================\n    # 3. æŒ‰ age_group åˆ†ç»„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n    # ========================================\n    if 'age_group' in df.columns:\n        print(\"\\nğŸ¯ æŒ‰ age_group åˆ†ç»„çš„ç»Ÿè®¡ç‰¹å¾...\")\n        \n        if is_train:\n            age_group_bmi_stats = df.groupby('age_group')['bmi'].agg(['mean', 'std']).add_prefix('age_group_bmi_')\n            age_group_smoker_stats = df.groupby('age_group')['smoker'].mean().to_frame('age_group_smoker_rate')\n            \n            stats_dict['age_group_bmi'] = age_group_bmi_stats\n            stats_dict['age_group_smoker'] = age_group_smoker_stats\n        else:\n            age_group_bmi_stats = train_stats['age_group_bmi']\n            age_group_smoker_stats = train_stats['age_group_smoker']\n        \n        for col in age_group_bmi_stats.columns:\n            df[col] = df['age_group'].map(age_group_bmi_stats[col])\n        df['age_group_smoker_rate'] = df['age_group'].map(age_group_smoker_stats['age_group_smoker_rate'])\n        \n        print(f\"  - æŒ‰age_groupåˆ†ç»„: {len(age_group_bmi_stats.columns) + 1} ä¸ªç‰¹å¾\")\n    \n    # ========================================\n    # 4. æŒ‰ bmi_category åˆ†ç»„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰\n    # ========================================\n    if 'bmi_category' in df.columns:\n        print(\"\\nğŸ¯ æŒ‰ bmi_category åˆ†ç»„çš„ç»Ÿè®¡ç‰¹å¾...\")\n        \n        if is_train:\n            bmi_cat_age_stats = df.groupby('bmi_category')['age'].agg(['mean']).add_prefix('bmi_cat_age_')\n            bmi_cat_smoker_stats = df.groupby('bmi_category')['smoker'].mean().to_frame('bmi_cat_smoker_rate')\n            \n            stats_dict['bmi_cat_age'] = bmi_cat_age_stats\n            stats_dict['bmi_cat_smoker'] = bmi_cat_smoker_stats\n        else:\n            bmi_cat_age_stats = train_stats['bmi_cat_age']\n            bmi_cat_smoker_stats = train_stats['bmi_cat_smoker']\n        \n        for col in bmi_cat_age_stats.columns:\n            df[col] = df['bmi_category'].map(bmi_cat_age_stats[col])\n        df['bmi_cat_smoker_rate'] = df['bmi_category'].map(bmi_cat_smoker_stats['bmi_cat_smoker_rate'])\n        \n        print(f\"  - æŒ‰bmi_categoryåˆ†ç»„: {len(bmi_cat_age_stats.columns) + 1} ä¸ªç‰¹å¾\")\n    \n    # ========================================\n    # 5. å¤šç»´åˆ†ç»„ï¼ˆsmoker Ã— regionï¼‰\n    # ========================================\n    print(\"\\nğŸ¯ å¤šç»´åˆ†ç»„ç»Ÿè®¡ç‰¹å¾ (smoker Ã— region)...\")\n    \n    if is_train:\n        smoker_region_age = df.groupby(['smoker', 'region'])['age'].mean().to_frame('smoker_region_age_mean')\n        smoker_region_bmi = df.groupby(['smoker', 'region'])['bmi'].mean().to_frame('smoker_region_bmi_mean')\n        \n        stats_dict['smoker_region_age'] = smoker_region_age\n        stats_dict['smoker_region_bmi'] = smoker_region_bmi\n    else:\n        smoker_region_age = train_stats['smoker_region_age']\n        smoker_region_bmi = train_stats['smoker_region_bmi']\n    \n    df['smoker_region_age_mean'] = df.set_index(['smoker', 'region']).index.map(smoker_region_age['smoker_region_age_mean'])\n    df['smoker_region_bmi_mean'] = df.set_index(['smoker', 'region']).index.map(smoker_region_bmi['smoker_region_bmi_mean'])\n    \n    print(f\"  - smoker Ã— region: 2 ä¸ªç‰¹å¾\")\n    \n    # ========================================\n    # 6. ç›¸å¯¹ç‰¹å¾ï¼ˆä¸ç¾¤ä½“å‡å€¼çš„åå·®ï¼‰\n    # ========================================\n    print(\"\\nğŸ¯ åˆ›å»ºç›¸å¯¹ç‰¹å¾ï¼ˆåå·®ç‰¹å¾ï¼‰...\")\n    \n    # ä¸ªä½“BMIä¸æ‰€åœ¨smokerç¾¤ä½“å¹³å‡BMIçš„åå·®\n    df['bmi_vs_smoker_mean'] = df['bmi'] - df['smoker_bmi_mean']\n    \n    # ä¸ªä½“å¹´é¾„ä¸æ‰€åœ¨regionç¾¤ä½“å¹³å‡å¹´é¾„çš„åå·®\n    df['age_vs_region_mean'] = df['age'] - df['region_age_mean']\n    \n    # ä¸ªä½“å¹´é¾„ä¸æ‰€åœ¨age_groupå¹³å‡BMIçš„åå·®\n    if 'age_group_bmi_mean' in df.columns:\n        df['bmi_vs_age_group_mean'] = df['bmi'] - df['age_group_bmi_mean']\n    \n    print(f\"  - ç›¸å¯¹ç‰¹å¾: 3 ä¸ª\")\n    \n    if is_train:\n        return df, stats_dict\n    else:\n        return df\n\n# ========================================\n# åº”ç”¨åˆ†ç»„ç»Ÿè®¡ç‰¹å¾\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"åº”ç”¨åˆ†ç»„ç»Ÿè®¡ç‰¹å¾åˆ°æ•°æ®é›†\")\nprint(\"=\"*60)\n\n# åŠ è½½æ•°æ®\ntrain_data = pd.read_csv('train_target_encoded.csv')\ntest_data = pd.read_csv('test_target_encoded.csv')\n\nprint(f\"\\nå¤„ç†å‰å½¢çŠ¶:\")\nprint(f\"  è®­ç»ƒé›†: {train_data.shape}\")\nprint(f\"  æµ‹è¯•é›†: {test_data.shape}\")\n\n# å¯¹è®­ç»ƒé›†åˆ›å»ºåˆ†ç»„ç»Ÿè®¡ç‰¹å¾\ntrain_with_agg, train_stats = create_aggregation_features(train_data, is_train=True)\n\n# å¯¹æµ‹è¯•é›†åº”ç”¨ç›¸åŒçš„ç»Ÿè®¡\ntest_with_agg = create_aggregation_features(test_data, is_train=False, train_stats=train_stats)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"âœ… åˆ†ç»„ç»Ÿè®¡ç‰¹å¾åˆ›å»ºå®Œæˆ!\")\nprint(\"=\"*60)\nprint(f\"\\nå¤„ç†åå½¢çŠ¶:\")\nprint(f\"  è®­ç»ƒé›†: {train_with_agg.shape}\")\nprint(f\"  æµ‹è¯•é›†: {test_with_agg.shape}\")\n\nnew_features = train_with_agg.shape[1] - train_data.shape[1]\nprint(f\"\\næ–°å¢ç‰¹å¾æ•°: {new_features}\")\n\n# æŸ¥çœ‹éƒ¨åˆ†æ–°ç‰¹å¾\nprint(\"\\nğŸ“‹ éƒ¨åˆ†åˆ†ç»„ç»Ÿè®¡ç‰¹å¾ç¤ºä¾‹:\")\nagg_features = [col for col in train_with_agg.columns if any(x in col for x in ['_mean', '_std', '_rate', '_vs_'])]\nprint(f\"\\nå‰15ä¸ªåˆ†ç»„ç»Ÿè®¡ç‰¹å¾:\")\nfor i, col in enumerate(agg_features[:15], 1):\n    print(f\"  {i:2d}. {col}\")\n\n# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯\nprint(\"\\nğŸ“Š éƒ¨åˆ†ç‰¹å¾ç»Ÿè®¡:\")\nsample_cols = ['smoker_bmi_mean', 'region_age_mean', 'bmi_vs_smoker_mean']\nprint(train_with_agg[sample_cols].describe().round(2))\n\n# ä¿å­˜\ntrain_with_agg.to_csv('train_all_features.csv', index=False)\ntest_with_agg.to_csv('test_all_features.csv', index=False)\n\nprint(\"\\nğŸ’¾ å·²ä¿å­˜:\")\nprint(\"  - train_all_features.csv (åŒ…å«æ‰€æœ‰ç‰¹å¾)\")\nprint(\"  - test_all_features.csv (åŒ…å«æ‰€æœ‰ç‰¹å¾)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a9c97ozjkq",
   "source": "# ========================================\n# Cell 8: ç‰¹å¾å·¥ç¨‹æ•ˆæœå¯¹æ¯”å®éªŒ\n# ========================================\n\nprint(\"ğŸ”¬ ç‰¹å¾å·¥ç¨‹æ•ˆæœå¯¹æ¯”å®éªŒ\")\nprint(\"=\"*60)\n\ndef prepare_and_evaluate(data_path, experiment_name, feature_set_description):\n    \"\"\"\n    å‡†å¤‡æ•°æ®å¹¶å¿«é€Ÿè¯„ä¼°\n    \n    å‚æ•°:\n        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„\n        experiment_name: å®éªŒåç§°\n        feature_set_description: ç‰¹å¾é›†æè¿°\n    \n    è¿”å›:\n        oof_rmse: OOF RMSE\n        n_features: ç‰¹å¾æ•°é‡\n    \"\"\"\n    # åŠ è½½æ•°æ®\n    df = pd.read_csv(data_path)\n    \n    # å‡†å¤‡ç‰¹å¾\n    # æ’é™¤ä¸ç”¨äºå»ºæ¨¡çš„åˆ—\n    exclude_cols = ['id', 'charges']\n    \n    # å¤„ç†ç±»åˆ«ç‰¹å¾ï¼ˆOne-Hotç¼–ç ï¼‰\n    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    if cat_cols:\n        df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n    \n    # åˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡\n    X = df.drop(exclude_cols, axis=1, errors='ignore')\n    y = df['charges']\n    \n    n_features = X.shape[1]\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"å®éªŒ: {experiment_name}\")\n    print(f\"{'='*60}\")\n    print(f\"ç‰¹å¾é›†: {feature_set_description}\")\n    print(f\"ç‰¹å¾æ•°é‡: {n_features}\")\n    print(f\"æ ·æœ¬æ•°é‡: {len(df)}\")\n    \n    # 5æŠ˜äº¤å‰éªŒè¯\n    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    oof_predictions = np.zeros(len(X))\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # è®­ç»ƒLightGBM\n        model = lgb.LGBMRegressor(\n            n_estimators=500,\n            learning_rate=0.05,\n            num_leaves=31,\n            colsample_bytree=0.8,\n            subsample=0.8,\n            random_state=SEED,\n            verbose=-1\n        )\n        \n        model.fit(\n            X_tr, np.log1p(y_tr),\n            eval_set=[(X_val, np.log1p(y_val))],\n            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n        )\n        \n        # é¢„æµ‹\n        pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n        oof_predictions[val_idx] = pred\n        \n        # è®¡ç®—fold RMSE\n        fold_rmse = np.sqrt(mean_squared_error(y_val, pred))\n        fold_scores.append(fold_rmse)\n        print(f\"  Fold {fold}: RMSE = {fold_rmse:.2f}\")\n    \n    # æ€»ä½“OOF RMSE\n    oof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n    oof_r2 = r2_score(y, oof_predictions)\n    \n    print(f\"\\n{'â”€'*60}\")\n    print(f\"OOF RMSE: {oof_rmse:.2f}\")\n    print(f\"OOF RÂ²:   {oof_r2:.4f}\")\n    print(f\"Fold RMSE æ ‡å‡†å·®: {np.std(fold_scores):.2f}\")\n    print(f\"{'â”€'*60}\")\n    \n    return oof_rmse, n_features, oof_r2\n\n# ========================================\n# å¯¹æ¯”å®éªŒ\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"å¼€å§‹å¯¹æ¯”å®éªŒ\")\nprint(\"=\"*60)\nprint(\"\\næˆ‘ä»¬å°†å¯¹æ¯”ä»¥ä¸‹ç‰¹å¾é›†çš„æ•ˆæœ:\")\nprint(\"  1. åŸºçº¿: ä»…åŸå§‹ç‰¹å¾ + ç®€å•ç¼–ç \")\nprint(\"  2. +é¢†åŸŸç‰¹å¾: æ·»åŠ é£é™©è¯„åˆ†ç­‰é¢†åŸŸçŸ¥è¯†ç‰¹å¾\")\nprint(\"  3. +Target Encoding: æ·»åŠ Target Encoding\")\nprint(\"  4. +åˆ†ç»„ç»Ÿè®¡: æ·»åŠ åˆ†ç»„ç»Ÿè®¡ç‰¹å¾ï¼ˆå®Œæ•´ç‰¹å¾é›†ï¼‰\")\n\nresults = []\n\n# å®éªŒ1: åŸºçº¿ï¼ˆä½¿ç”¨æ¸…æ´—åçš„æ•°æ®ï¼Œä»…åŸºç¡€ç‰¹å¾ï¼‰\nprint(\"\\n\\n\" + \"ğŸ”¸\"*30)\nprint(\"å®éªŒ 1/4: åŸºçº¿ï¼ˆåŸå§‹ç‰¹å¾ï¼‰\")\nprint(\"ğŸ”¸\"*30)\n\nrmse_1, n_feat_1, r2_1 = prepare_and_evaluate(\n    'train_cleaned.csv',\n    'åŸºçº¿',\n    'ä»…åŸå§‹ç‰¹å¾: age, bmi, children, smoker, sex, region'\n)\n\nresults.append({\n    'å®éªŒ': '1. åŸºçº¿',\n    'ç‰¹å¾æ•°': n_feat_1,\n    'OOF RMSE': rmse_1,\n    'OOF RÂ²': r2_1,\n    'vs åŸºçº¿': 0,\n    'è¯´æ˜': 'åŸå§‹ç‰¹å¾+ç®€å•ç¼–ç '\n})\n\n# å®éªŒ2: æ·»åŠ é¢†åŸŸç‰¹å¾\nprint(\"\\n\\n\" + \"ğŸ”¸\"*30)\nprint(\"å®éªŒ 2/4: +é¢†åŸŸçŸ¥è¯†ç‰¹å¾\")\nprint(\"ğŸ”¸\"*30)\n\nrmse_2, n_feat_2, r2_2 = prepare_and_evaluate(\n    'train_domain_features.csv',\n    '+é¢†åŸŸç‰¹å¾',\n    'åŸå§‹ç‰¹å¾ + é£é™©è¯„åˆ† + äº¤äº’ç‰¹å¾ + å¤šé¡¹å¼ç‰¹å¾'\n)\n\nresults.append({\n    'å®éªŒ': '2. +é¢†åŸŸç‰¹å¾',\n    'ç‰¹å¾æ•°': n_feat_2,\n    'OOF RMSE': rmse_2,\n    'OOF RÂ²': r2_2,\n    'vs åŸºçº¿': rmse_2 - rmse_1,\n    'è¯´æ˜': f'æ–°å¢{n_feat_2 - n_feat_1}ä¸ªç‰¹å¾'\n})\n\n# å®éªŒ3: æ·»åŠ Target Encoding\nprint(\"\\n\\n\" + \"ğŸ”¸\"*30)\nprint(\"å®éªŒ 3/4: +Target Encoding\")\nprint(\"ğŸ”¸\"*30)\n\nrmse_3, n_feat_3, r2_3 = prepare_and_evaluate(\n    'train_target_encoded.csv',\n    '+Target Encoding',\n    'é¢†åŸŸç‰¹å¾ + Target Encoding (sex, region)'\n)\n\nresults.append({\n    'å®éªŒ': '3. +Target Encoding',\n    'ç‰¹å¾æ•°': n_feat_3,\n    'OOF RMSE': rmse_3,\n    'OOF RÂ²': r2_3,\n    'vs åŸºçº¿': rmse_3 - rmse_1,\n    'è¯´æ˜': f'æ–°å¢{n_feat_3 - n_feat_2}ä¸ªç¼–ç ç‰¹å¾'\n})\n\n# å®éªŒ4: å®Œæ•´ç‰¹å¾é›†ï¼ˆæ·»åŠ åˆ†ç»„ç»Ÿè®¡ï¼‰\nprint(\"\\n\\n\" + \"ğŸ”¸\"*30)\nprint(\"å®éªŒ 4/4: å®Œæ•´ç‰¹å¾é›†\")\nprint(\"ğŸ”¸\"*30)\n\nrmse_4, n_feat_4, r2_4 = prepare_and_evaluate(\n    'train_all_features.csv',\n    'å®Œæ•´ç‰¹å¾é›†',\n    'é¢†åŸŸç‰¹å¾ + Target Encoding + åˆ†ç»„ç»Ÿè®¡ç‰¹å¾'\n)\n\nresults.append({\n    'å®éªŒ': '4. å®Œæ•´ç‰¹å¾é›†',\n    'ç‰¹å¾æ•°': n_feat_4,\n    'OOF RMSE': rmse_4,\n    'OOF RÂ²': r2_4,\n    'vs åŸºçº¿': rmse_4 - rmse_1,\n    'è¯´æ˜': f'æ–°å¢{n_feat_4 - n_feat_3}ä¸ªç»Ÿè®¡ç‰¹å¾'\n})\n\n# ========================================\n# ç»“æœæ±‡æ€»\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"ğŸ“Š å®éªŒç»“æœæ±‡æ€»\")\nprint(\"=\"*60)\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n\" + results_df.to_string(index=False))\n\n# æ‰¾å‡ºæœ€ä½³ç»“æœ\nbest_idx = results_df['OOF RMSE'].idxmin()\nbest_result = results_df.loc[best_idx]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ† æœ€ä½³ç»“æœ\")\nprint(\"=\"*60)\nprint(f\"å®éªŒ: {best_result['å®éªŒ']}\")\nprint(f\"ç‰¹å¾æ•°: {best_result['ç‰¹å¾æ•°']}\")\nprint(f\"OOF RMSE: {best_result['OOF RMSE']:.2f}\")\nprint(f\"OOF RÂ²: {best_result['OOF RÂ²']:.4f}\")\nprint(f\"ç›¸æ¯”åŸºçº¿æå‡: {-best_result['vs åŸºçº¿']:.2f} RMSE ({-best_result['vs åŸºçº¿']/rmse_1*100:.1f}%)\")\n\n# å¯è§†åŒ–å¯¹æ¯”\nprint(\"\\nğŸ“Š å¯è§†åŒ–å¯¹æ¯”...\")\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# å›¾1: RMSEå¯¹æ¯”\nax1 = axes[0]\ncolors = ['red' if i == best_idx else 'skyblue' for i in range(len(results_df))]\nbars = ax1.bar(range(len(results_df)), results_df['OOF RMSE'], color=colors, edgecolor='black')\nax1.set_xticks(range(len(results_df)))\nax1.set_xticklabels([s.split('.')[1].strip() for s in results_df['å®éªŒ']], rotation=15, ha='right')\nax1.set_ylabel('OOF RMSE')\nax1.set_title('ç‰¹å¾å·¥ç¨‹æ•ˆæœå¯¹æ¯” - RMSE')\nax1.axhline(rmse_1, color='red', linestyle='--', alpha=0.5, label='åŸºçº¿')\n\n# æ·»åŠ æ•°å€¼æ ‡ç­¾\nfor bar, rmse in zip(bars, results_df['OOF RMSE']):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n            f'{rmse:.0f}',\n            ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nax1.legend()\nax1.grid(axis='y', alpha=0.3)\n\n# å›¾2: RÂ²å¯¹æ¯”\nax2 = axes[1]\nbars2 = ax2.bar(range(len(results_df)), results_df['OOF RÂ²'], color=colors, edgecolor='black')\nax2.set_xticks(range(len(results_df)))\nax2.set_xticklabels([s.split('.')[1].strip() for s in results_df['å®éªŒ']], rotation=15, ha='right')\nax2.set_ylabel('OOF RÂ²')\nax2.set_title('ç‰¹å¾å·¥ç¨‹æ•ˆæœå¯¹æ¯” - RÂ²')\nax2.set_ylim([0, 1])\n\n# æ·»åŠ æ•°å€¼æ ‡ç­¾\nfor bar, r2 in zip(bars2, results_df['OOF RÂ²']):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f'{r2:.3f}',\n            ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… ç¬¬äºŒéƒ¨åˆ†å®Œæˆï¼\")\nprint(\"=\"*60)\nprint(\"\\nğŸ“ ç¬¬äºŒéƒ¨åˆ†çŸ¥è¯†æ€»ç»“:\")\nprint(\"  1. âœ… åˆ›å»ºäº†28ä¸ªé¢†åŸŸçŸ¥è¯†ç‰¹å¾ï¼ˆé£é™©è¯„åˆ†ã€äº¤äº’ã€å¤šé¡¹å¼ï¼‰\")\nprint(\"  2. âœ… å®ç°äº†K-Fold Target Encodingï¼ˆé˜²æ­¢æ•°æ®æ³„æ¼ï¼‰\")\nprint(\"  3. âœ… åˆ›å»ºäº†20+ä¸ªåˆ†ç»„ç»Ÿè®¡ç‰¹å¾\")\nprint(\"  4. âœ… é€šè¿‡å®éªŒéªŒè¯äº†æ¯ä¸€æ­¥çš„æ•ˆæœ\")\nprint(f\"  5. âœ… æœ€ç»ˆRMSEæå‡: {-best_result['vs åŸºçº¿']:.0f} ({-best_result['vs åŸºçº¿']/rmse_1*100:.1f}%)\")\nprint(\"\\nğŸ’¡ ç‰¹å¾å·¥ç¨‹æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€æœ‰ä»·å€¼çš„ç¯èŠ‚ï¼\")\nprint(\"\\nä¸‹ä¸€æ­¥: è¶…å‚æ•°ä¼˜åŒ– â†’ é¢„æœŸå†é™ä½100-200 RMSEï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mjinp090mup",
   "source": "---\n\n# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¶…å‚æ•°ä¼˜åŒ–\n\n## ğŸ¯ å­¦ä¹ ç›®æ ‡\n1. ç†è§£è¶…å‚æ•°ä¸æ¨¡å‹å‚æ•°çš„åŒºåˆ«\n2. æŒæ¡ä¸‰ç§è°ƒå‚ç­–ç•¥ï¼ˆGrid Search, Random Search, Bayesian Optimizationï¼‰\n3. å­¦ä¹ Optunaçš„ä½¿ç”¨æ–¹æ³•\n4. ç†è§£TPEé‡‡æ ·å™¨çš„å·¥ä½œåŸç†\n5. å­¦ä¼šåˆ†æå‚æ•°é‡è¦æ€§\n\n## ğŸ“š ç†è®ºçŸ¥è¯†ï¼šè¶…å‚æ•°ä¼˜åŒ–\n\n### ä»€ä¹ˆæ˜¯è¶…å‚æ•°ï¼Ÿ\n\n**æ¨¡å‹å‚æ•° vs è¶…å‚æ•°**\n\n| ç±»å‹ | å®šä¹‰ | ç¤ºä¾‹ | å¦‚ä½•è·å¾— |\n|------|------|------|----------|\n| **æ¨¡å‹å‚æ•°** | æ¨¡å‹ä»æ•°æ®ä¸­å­¦ä¹ å¾—åˆ°çš„å‚æ•° | çº¿æ€§å›å½’çš„æƒé‡ã€å†³ç­–æ ‘çš„åˆ†è£‚ç‚¹ | é€šè¿‡è®­ç»ƒè‡ªåŠ¨å­¦ä¹  |\n| **è¶…å‚æ•°** | æ¨¡å‹è®­ç»ƒå‰éœ€è¦æ‰‹åŠ¨è®¾ç½®çš„å‚æ•° | å­¦ä¹ ç‡ã€æ ‘çš„æ·±åº¦ã€æ­£åˆ™åŒ–ç³»æ•° | éœ€è¦æ‰‹åŠ¨è°ƒæ•´æˆ–è‡ªåŠ¨æœç´¢ |\n\n### ä¸ºä»€ä¹ˆè¶…å‚æ•°é‡è¦ï¼Ÿ\n\n```\nåŒæ ·çš„æ•°æ® + ä¸åŒçš„è¶…å‚æ•° â†’ å¯èƒ½ç›¸å·®å‡ ç™¾RMSEï¼\n\nä¾‹å¦‚ï¼šLightGBM\n- é»˜è®¤å‚æ•°ï¼šRMSE = 6500\n- ä¼˜åŒ–åå‚æ•°ï¼šRMSE = 6300\n- å·®å¼‚ï¼š200 RMSE ï¼ˆ3%çš„æå‡ï¼‰\n```\n\n### ä¸‰ç§è°ƒå‚ç­–ç•¥å¯¹æ¯”\n\n#### 1ï¸âƒ£ Grid Searchï¼ˆç½‘æ ¼æœç´¢ï¼‰\n\n```python\n# ç©·ä¸¾æ‰€æœ‰ç»„åˆ\nparams = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [31, 50, 100],\n    'max_depth': [5, 7, 10]\n}\n# æ€»å…±éœ€è¦æµ‹è¯•ï¼š3 Ã— 3 Ã— 3 = 27 ç§ç»„åˆ\n```\n\n**ä¼˜ç‚¹**ï¼š\n- ç®€å•æ˜“æ‡‚\n- ä¿è¯æ‰¾åˆ°ç½‘æ ¼å†…çš„æœ€ä¼˜ç»„åˆ\n\n**ç¼ºç‚¹**ï¼š\n- âŒ è®¡ç®—æˆæœ¬æŒ‡æ•°å¢é•¿ï¼ˆç»´åº¦è¯…å’’ï¼‰\n- âŒ æµªè´¹è®¡ç®—èµ„æºåœ¨ä¸é‡è¦çš„å‚æ•°ä¸Š\n- âŒ å¯¹è¿ç»­å‚æ•°ä¸å‹å¥½\n\n#### 2ï¸âƒ£ Random Searchï¼ˆéšæœºæœç´¢ï¼‰\n\n```python\n# éšæœºé‡‡æ ·\nfor i in range(100):\n    learning_rate = uniform(0.01, 0.3)\n    num_leaves = randint(20, 150)\n    # ... è®­ç»ƒå¹¶è¯„ä¼°\n```\n\n**ä¼˜ç‚¹**ï¼š\n- âœ… æ¯”Grid Searchå¿«å¾ˆå¤š\n- âœ… æ›´å®¹æ˜“æ¢ç´¢å‚æ•°ç©ºé—´\n- âœ… å¯¹ä¸é‡è¦çš„å‚æ•°ä¸æ•æ„Ÿ\n\n**ç¼ºç‚¹**ï¼š\n- âŒ ä»ç„¶æ˜¯ç›²ç›®æœç´¢\n- âŒ ä¸åˆ©ç”¨ä¹‹å‰çš„è¯„ä¼°ç»“æœ\n\n#### 3ï¸âƒ£ Bayesian Optimizationï¼ˆè´å¶æ–¯ä¼˜åŒ–ï¼‰â­\n\n```\næ ¸å¿ƒæ€æƒ³ï¼šåˆ©ç”¨ä¹‹å‰çš„è¯„ä¼°ç»“æœï¼Œæ™ºèƒ½åœ°é€‰æ‹©ä¸‹ä¸€ä¸ªè¦æµ‹è¯•çš„å‚æ•°ç»„åˆ\n\nå·¥ä½œæµç¨‹ï¼š\n1. éšæœºæµ‹è¯•å‡ ç»„å‚æ•°\n2. æ ¹æ®ç»“æœï¼Œå»ºç«‹\"å‚æ•°â†’æ€§èƒ½\"çš„æ¦‚ç‡æ¨¡å‹\n3. ç”¨è¿™ä¸ªæ¨¡å‹é¢„æµ‹ï¼šå“ªç»„å‚æ•°æœ€æœ‰å¯èƒ½æ›´å¥½ï¼Ÿ\n4. æµ‹è¯•è¯¥å‚æ•°ï¼Œæ›´æ–°æ¨¡å‹\n5. é‡å¤æ­¥éª¤3-4\n```\n\n**ä¼˜ç‚¹**ï¼š\n- âœ… é«˜æ•ˆï¼šé€šå¸¸50-100æ¬¡è¯„ä¼°å°±å¤Ÿäº†\n- âœ… æ™ºèƒ½ï¼šåˆ©ç”¨å†å²ä¿¡æ¯\n- âœ… é€‚åˆé»‘ç›’ä¼˜åŒ–\n- âœ… è‡ªåŠ¨å¹³è¡¡æ¢ç´¢(exploration)å’Œåˆ©ç”¨(exploitation)\n\n**ç¼ºç‚¹**ï¼š\n- ç¨å¾®å¤æ‚ä¸€äº›\n- éœ€è¦ç†è§£é‡‡æ ·ç­–ç•¥\n\n### Optuna æ¡†æ¶\n\nOptuna æ˜¯ç›®å‰æœ€æµè¡Œçš„è´å¶æ–¯ä¼˜åŒ–åº“ä¹‹ä¸€ã€‚\n\n**æ ¸å¿ƒç»„ä»¶**ï¼š\n\n1. **Study**ï¼šä¸€æ¬¡å®Œæ•´çš„ä¼˜åŒ–å®éªŒ\n2. **Trial**ï¼šä¸€æ¬¡å‚æ•°ç»„åˆçš„è¯„ä¼°\n3. **Objective**ï¼šç›®æ ‡å‡½æ•°ï¼ˆæˆ‘ä»¬è¦æœ€å°åŒ–/æœ€å¤§åŒ–çš„æŒ‡æ ‡ï¼‰\n4. **Sampler**ï¼šé‡‡æ ·å™¨ï¼ˆå†³å®šå¦‚ä½•é€‰æ‹©ä¸‹ä¸€ç»„å‚æ•°ï¼‰\n\n**å¸¸ç”¨é‡‡æ ·å™¨**ï¼š\n\n- **TPE** (Tree-structured Parzen Estimator)ï¼šé»˜è®¤ï¼Œé€‚åˆå¤§å¤šæ•°æƒ…å†µ â­\n- **CMA-ES**ï¼šé€‚åˆè¿ç»­å‚æ•°ç©ºé—´\n- **Random**ï¼šéšæœºæœç´¢\n- **Grid**ï¼šç½‘æ ¼æœç´¢\n\n### TPE (Tree-structured Parzen Estimator)\n\nTPEæ˜¯Optunaçš„é»˜è®¤é‡‡æ ·å™¨ï¼Œæ•ˆæœå¾ˆå¥½ã€‚\n\n**æ ¸å¿ƒæ€æƒ³**ï¼š\n\n```\nå°†å‚æ•°åˆ†ä¸ºä¸¤ç»„ï¼š\n- l(x)ï¼šè¡¨ç°å¥½çš„å‚æ•°åˆ†å¸ƒï¼ˆloss < æŸé˜ˆå€¼ï¼‰\n- g(x)ï¼šè¡¨ç°ä¸å¥½çš„å‚æ•°åˆ†å¸ƒ\n\né€‰æ‹©ä¸‹ä¸€ä¸ªå‚æ•°ï¼š\n- æœ€å¤§åŒ– l(x) / g(x) çš„å€¼\n- å³ï¼šåœ¨\"å¥½å‚æ•°\"åŒºåŸŸé‡‡æ ·çš„æ¦‚ç‡é«˜ï¼Œåœ¨\"åå‚æ•°\"åŒºåŸŸé‡‡æ ·çš„æ¦‚ç‡ä½\n```\n\n**ç®€å•ç†è§£**ï¼š\n- ç»´æŠ¤ä¸¤ä¸ªæ¨¡å‹ï¼š\"å¥½å‚æ•°åœ¨å“ªé‡Œ\" å’Œ \"åå‚æ•°åœ¨å“ªé‡Œ\"\n- ä¸‹æ¬¡ä¼˜å…ˆæµ‹è¯•\"å¯èƒ½æ˜¯å¥½å‚æ•°\"çš„åœ°æ–¹\n\næ¥ä¸‹æ¥æˆ‘ä»¬ä¼šç”¨Optunaä¼˜åŒ–LightGBMï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ubp9rbfdsek",
   "source": "# ========================================\n# Cell 9: Optuna è¶…å‚æ•°ä¼˜åŒ–å®ç°\n# ========================================\n\nprint(\"ğŸ”§ ä½¿ç”¨ Optuna è¿›è¡Œè¶…å‚æ•°ä¼˜åŒ–\")\nprint(\"=\"*60)\n\n# æ£€æŸ¥optunaæ˜¯å¦å®‰è£…\ntry:\n    import optuna\n    from optuna.visualization import plot_optimization_history, plot_param_importances\n    print(\"âœ… Optuna å·²å®‰è£…\")\nexcept ImportError:\n    print(\"âŒ Optuna æœªå®‰è£…\")\n    print(\"è¯·è¿è¡Œ: pip install optuna\")\n    print(\"æš‚æ—¶è·³è¿‡æ­¤éƒ¨åˆ†\")\n\n# åŠ è½½å®Œæ•´ç‰¹å¾é›†æ•°æ®\nprint(\"\\nğŸ“‚ åŠ è½½æ•°æ®...\")\ntrain_data = pd.read_csv('train_all_features.csv')\n\n# å‡†å¤‡æ•°æ®\nexclude_cols = ['id', 'charges']\ncat_cols = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\nif cat_cols:\n    train_data = pd.get_dummies(train_data, columns=cat_cols, drop_first=True)\n\nX = train_data.drop(exclude_cols, axis=1, errors='ignore')\ny = train_data['charges']\n\nprint(f\"ç‰¹å¾æ•°é‡: {X.shape[1]}\")\nprint(f\"æ ·æœ¬æ•°é‡: {len(X)}\")\n\n# ========================================\n# å®šä¹‰Optunaç›®æ ‡å‡½æ•°\n# ========================================\n\ndef objective(trial):\n    \"\"\"\n    Optunaç›®æ ‡å‡½æ•°\n    \n    å‚æ•°:\n        trial: Optuna trialå¯¹è±¡\n    \n    è¿”å›:\n        å¹³å‡RMSEï¼ˆè¶Šå°è¶Šå¥½ï¼‰\n    \"\"\"\n    \n    # ========================================\n    # å®šä¹‰æœç´¢ç©ºé—´\n    # ========================================\n    \n    params = {\n        # åŸºæœ¬å‚æ•°\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        \n        # æ ‘ç»“æ„å‚æ•°\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0, log=True),\n        \n        # é‡‡æ ·å‚æ•°ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼‰\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'subsample_freq': trial.suggest_int('subsample_freq', 0, 7),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        \n        # æ­£åˆ™åŒ–å‚æ•°\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        \n        # å›ºå®šå‚æ•°\n        'random_state': SEED,\n        'verbose': -1\n    }\n    \n    # ========================================\n    # äº¤å‰éªŒè¯è¯„ä¼°\n    # ========================================\n    \n    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # è®­ç»ƒæ¨¡å‹\n        model = lgb.LGBMRegressor(**params)\n        \n        model.fit(\n            X_tr, np.log1p(y_tr),\n            eval_set=[(X_val, np.log1p(y_val))],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=False),\n                lgb.log_evaluation(period=0)  # ç¦æ­¢è¾“å‡º\n            ]\n        )\n        \n        # é¢„æµ‹\n        pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n        \n        # è®¡ç®—RMSE\n        fold_rmse = np.sqrt(mean_squared_error(y_val, pred))\n        fold_scores.append(fold_rmse)\n    \n    # è¿”å›å¹³å‡RMSE\n    mean_rmse = np.mean(fold_scores)\n    \n    return mean_rmse\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"å¼€å§‹ä¼˜åŒ–ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰\")\nprint(\"=\"*60)\n\n# ========================================\n# è¿è¡Œä¼˜åŒ–\n# ========================================\n\n# åˆ›å»ºstudy\nstudy = optuna.create_study(\n    direction='minimize',  # æœ€å°åŒ–RMSE\n    sampler=optuna.samplers.TPESampler(seed=SEED),  # ä½¿ç”¨TPEé‡‡æ ·å™¨\n    study_name='lgb_optimization'\n)\n\n# è¿è¡Œä¼˜åŒ–ï¼ˆè°ƒæ•´n_trialså¯ä»¥æ§åˆ¶ä¼˜åŒ–æ—¶é—´ï¼‰\n# 50 trialsçº¦éœ€è¦5-10åˆ†é’Ÿï¼Œ100 trialsçº¦éœ€è¦10-20åˆ†é’Ÿ\nN_TRIALS = 50  # å¯ä»¥è°ƒæ•´è¿™ä¸ªå€¼\n\nprint(f\"\\nğŸš€ å¼€å§‹ {N_TRIALS} æ¬¡ä¼˜åŒ–...\")\nprint(\"è¿›åº¦æ¡ä¼šæ˜¾ç¤ºå½“å‰è¿›åº¦\\n\")\n\nstudy.optimize(\n    objective,\n    n_trials=N_TRIALS,\n    show_progress_bar=True,\n    n_jobs=1  # ä¸²è¡Œæ‰§è¡Œï¼ˆé¿å…å†…å­˜é—®é¢˜ï¼‰\n)\n\nprint(\"\\nâœ… ä¼˜åŒ–å®Œæˆï¼\")\nprint(\"=\"*60)\n\n# ========================================\n# è¾“å‡ºç»“æœ\n# ========================================\n\nprint(\"\\nğŸ“Š ä¼˜åŒ–ç»“æœ:\")\nprint(\"-\"*60)\nprint(f\"æœ€ä½³RMSE: {study.best_value:.2f}\")\nprint(f\"æœ€ä½³å‚æ•°:\")\nfor param, value in study.best_params.items():\n    print(f\"  {param:20s}: {value}\")\n\nprint(f\"\\næ€»å…±å°è¯•: {len(study.trials)} ç»„å‚æ•°\")\nprint(f\"å®Œæˆçš„trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n\n# ä¿å­˜æœ€ä½³å‚æ•°\nimport json\nwith open('best_params_lgb.json', 'w') as f:\n    json.dump(study.best_params, f, indent=2)\nprint(\"\\nğŸ’¾ æœ€ä½³å‚æ•°å·²ä¿å­˜åˆ°: best_params_lgb.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aege4obo9cr",
   "source": "# ========================================\n# Cell 10: ä¼˜åŒ–ç»“æœåˆ†æä¸å¯è§†åŒ–\n# ========================================\n\nprint(\"ğŸ“Š ä¼˜åŒ–ç»“æœåˆ†æ\")\nprint(\"=\"*60)\n\n# ========================================\n# 1. ä¼˜åŒ–å†å²å¯è§†åŒ–\n# ========================================\n\nprint(\"\\n1ï¸âƒ£ ä¼˜åŒ–å†å²\")\nprint(\"-\"*60)\n\n# è·å–æ‰€æœ‰trialçš„å€¼\ntrial_values = [t.value for t in study.trials if t.value is not None]\ntrial_numbers = list(range(1, len(trial_values) + 1))\n\n# è®¡ç®—ç´¯ç§¯æœ€ä½³å€¼\ncumulative_best = []\ncurrent_best = float('inf')\nfor val in trial_values:\n    if val < current_best:\n        current_best = val\n    cumulative_best.append(current_best)\n\n# å¯è§†åŒ–\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# å·¦å›¾ï¼šæ¯æ¬¡trialçš„RMSE\nax1 = axes[0]\nax1.plot(trial_numbers, trial_values, 'o-', alpha=0.6, markersize=4, label='Trial RMSE')\nax1.plot(trial_numbers, cumulative_best, 'r-', linewidth=2, label='Best RMSE')\nax1.set_xlabel('Trial Number')\nax1.set_ylabel('RMSE')\nax1.set_title('ä¼˜åŒ–å†å² - RMSEéšTrialå˜åŒ–')\nax1.legend()\nax1.grid(alpha=0.3)\n\n# å³å›¾ï¼šå‰20ä¸ªtrialsè¯¦ç»†å¯¹æ¯”\nax2 = axes[1]\nn_show = min(20, len(trial_values))\nax2.bar(range(1, n_show+1), trial_values[:n_show], alpha=0.7)\nbest_idx = trial_values.index(min(trial_values[:n_show])) + 1\nax2.axhline(study.best_value, color='red', linestyle='--', label=f'Best: {study.best_value:.2f}')\nax2.set_xlabel('Trial Number')\nax2.set_ylabel('RMSE')\nax2.set_title(f'å‰{n_show}ä¸ªTrialsçš„RMSE')\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nğŸ’¡ è§‚å¯Ÿè¦ç‚¹:\")\nprint(f\"  - æœ€ä½³RMSEåœ¨ç¬¬ {trial_values.index(study.best_value)+1} æ¬¡trialå‡ºç°\")\nprint(f\"  - å‰10æ¬¡trialsçš„å¹³å‡RMSE: {np.mean(trial_values[:10]):.2f}\")\nprint(f\"  - å10æ¬¡trialsçš„å¹³å‡RMSE: {np.mean(trial_values[-10:]):.2f}\")\nprint(f\"  - RMSEæ”¹å–„: {np.mean(trial_values[:10]) - study.best_value:.2f}\")\n\n# ========================================\n# 2. å‚æ•°é‡è¦æ€§åˆ†æ\n# ========================================\n\nprint(\"\\n\\n2ï¸âƒ£ å‚æ•°é‡è¦æ€§åˆ†æ\")\nprint(\"-\"*60)\n\n# è®¡ç®—å‚æ•°é‡è¦æ€§\ntry:\n    importance = optuna.importance.get_param_importances(study)\n    \n    print(\"\\nå‚æ•°é‡è¦æ€§æ’åºï¼ˆè¶Šå¤§è¶Šé‡è¦ï¼‰:\")\n    for i, (param, imp) in enumerate(sorted(importance.items(), key=lambda x: x[1], reverse=True), 1):\n        bar = 'â–ˆ' * int(imp * 50)\n        print(f\"  {i:2d}. {param:20s}: {imp:6.4f} {bar}\")\n    \n    # å¯è§†åŒ–\n    fig, ax = plt.subplots(figsize=(10, 6))\n    params = list(importance.keys())\n    values = list(importance.values())\n    \n    # æŒ‰é‡è¦æ€§æ’åº\n    sorted_idx = np.argsort(values)[::-1]\n    params_sorted = [params[i] for i in sorted_idx]\n    values_sorted = [values[i] for i in sorted_idx]\n    \n    bars = ax.barh(params_sorted, values_sorted, color='skyblue', edgecolor='black')\n    \n    # é«˜äº®å‰3ä¸ªæœ€é‡è¦çš„å‚æ•°\n    for i in range(min(3, len(bars))):\n        bars[i].set_color('coral')\n    \n    ax.set_xlabel('Importance')\n    ax.set_title('å‚æ•°é‡è¦æ€§åˆ†æ')\n    ax.grid(axis='x', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\nğŸ’¡ è§£è¯»:\")\n    top3 = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]\n    print(f\"  - æœ€é‡è¦çš„3ä¸ªå‚æ•°: {', '.join([p[0] for p in top3])}\")\n    print(f\"  - è¿™äº›å‚æ•°å¯¹æ¨¡å‹æ€§èƒ½å½±å“æœ€å¤§ï¼Œéœ€è¦é‡ç‚¹è°ƒæ•´\")\n    \nexcept Exception as e:\n    print(f\"å‚æ•°é‡è¦æ€§è®¡ç®—å¤±è´¥: {e}\")\n\n# ========================================\n# 3. å¯¹æ¯”é»˜è®¤å‚æ•° vs ä¼˜åŒ–å‚æ•°\n# ========================================\n\nprint(\"\\n\\n3ï¸âƒ£ é»˜è®¤å‚æ•° vs ä¼˜åŒ–å‚æ•°å¯¹æ¯”\")\nprint(\"-\"*60)\n\n# ä½¿ç”¨é»˜è®¤å‚æ•°è¯„ä¼°\nprint(\"\\næµ‹è¯•é»˜è®¤å‚æ•°...\")\ndefault_params = {\n    'n_estimators': 100,\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'random_state': SEED,\n    'verbose': -1\n}\n\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\ndefault_scores = []\n\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgb.LGBMRegressor(**default_params)\n    model.fit(X_tr, np.log1p(y_tr), \n              eval_set=[(X_val, np.log1p(y_val))],\n              callbacks=[lgb.early_stopping(50, verbose=False)])\n    \n    pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n    rmse = np.sqrt(mean_squared_error(y_val, pred))\n    default_scores.append(rmse)\n\ndefault_rmse = np.mean(default_scores)\n\nprint(\"\\nğŸ“Š å¯¹æ¯”ç»“æœ:\")\nprint(\"-\"*60)\nprint(f\"{'å‚æ•°è®¾ç½®':<20s} {'RMSE':>10s} {'æå‡':>15s}\")\nprint(\"-\"*60)\nprint(f\"{'é»˜è®¤å‚æ•°':<20s} {default_rmse:>10.2f} {'-':>15s}\")\nprint(f\"{'Optunaä¼˜åŒ–':<20s} {study.best_value:>10.2f} {f'-{default_rmse - study.best_value:.2f} ({(default_rmse - study.best_value)/default_rmse*100:.1f}%)':>15s}\")\nprint(\"-\"*60)\n\n# å¯è§†åŒ–å¯¹æ¯”\nfig, ax = plt.subplots(figsize=(8, 5))\nmethods = ['é»˜è®¤å‚æ•°', 'Optunaä¼˜åŒ–']\nrmse_values = [default_rmse, study.best_value]\ncolors = ['lightcoral', 'lightgreen']\n\nbars = ax.bar(methods, rmse_values, color=colors, edgecolor='black', width=0.6)\nax.set_ylabel('RMSE')\nax.set_title('å‚æ•°ä¼˜åŒ–æ•ˆæœå¯¹æ¯”')\n\n# æ·»åŠ æ•°å€¼æ ‡ç­¾\nfor bar, val in zip(bars, rmse_values):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{val:.2f}',\n            ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n# æ·»åŠ æå‡æ ‡æ³¨\nimprovement = default_rmse - study.best_value\nax.annotate(f'æå‡: {improvement:.2f}\\n({improvement/default_rmse*100:.1f}%)',\n            xy=(0.5, (default_rmse + study.best_value)/2),\n            xytext=(0.5, (default_rmse + study.best_value)/2 + 200),\n            ha='center',\n            fontsize=11,\n            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nâœ… ç¬¬ä¸‰éƒ¨åˆ†å®Œæˆï¼\")\nprint(\"=\"*60)\nprint(\"\\nğŸ“ ç¬¬ä¸‰éƒ¨åˆ†çŸ¥è¯†æ€»ç»“:\")\nprint(\"  1. âœ… ç†è§£äº†è¶…å‚æ•°vsæ¨¡å‹å‚æ•°çš„åŒºåˆ«\")\nprint(\"  2. âœ… æŒæ¡äº†ä¸‰ç§è°ƒå‚ç­–ç•¥çš„ä¼˜ç¼ºç‚¹\")\nprint(\"  3. âœ… å­¦ä¼šäº†ä½¿ç”¨Optunaè¿›è¡Œè´å¶æ–¯ä¼˜åŒ–\")\nprint(\"  4. âœ… å­¦ä¼šäº†åˆ†æå‚æ•°é‡è¦æ€§\")\nprint(f\"  5. âœ… RMSEæå‡: {improvement:.0f} ({improvement/default_rmse*100:.1f}%)\")\nprint(\"\\nğŸ’¡ è¶…å‚æ•°ä¼˜åŒ–èƒ½å¸¦æ¥ç¨³å®šçš„æ€§èƒ½æå‡ï¼\")\nprint(\"\\nä¸‹ä¸€æ­¥: æ¨¡å‹èåˆ â†’ é¢„æœŸå†é™ä½200-300 RMSEï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25fzd25q7dc",
   "source": "---\n\n# ç¬¬å››éƒ¨åˆ†ï¼šæ¨¡å‹èåˆï¼ˆEnsembleï¼‰â­â­â­\n\n## ğŸ¯ å­¦ä¹ ç›®æ ‡\n1. ç†è§£\"ä¸‰ä¸ªè‡­çš®åŒ èµ›è¿‡è¯¸è‘›äº®\"çš„æ•°å­¦åŸç†\n2. æŒæ¡å¤šç§æ¨¡å‹èåˆç­–ç•¥\n3. å®ç°å®Œæ•´çš„Stackingæµç¨‹\n4. å­¦ä¼šè¯„ä¼°èåˆæ•ˆæœ\n\n## ğŸ“š ç†è®ºçŸ¥è¯†ï¼šEnsembleæ€æƒ³\n\n### ä¸ºä»€ä¹ˆæ¨¡å‹èåˆæœ‰æ•ˆï¼Ÿ\n\n**æ ¸å¿ƒåŸç†**ï¼šä¸åŒæ¨¡å‹ä¼šçŠ¯ä¸åŒçš„é”™è¯¯ï¼Œèåˆå¯ä»¥äº’è¡¥\n\n```\nå‡è®¾æœ‰3ä¸ªæ¨¡å‹ï¼Œæ¯ä¸ªå‡†ç¡®ç‡80%ï¼š\n- å•ä¸ªæ¨¡å‹ï¼šå‡†ç¡®ç‡ = 80%\n- å¦‚æœé”™è¯¯ç‹¬ç«‹ï¼Œ3ä¸ªæ¨¡å‹æŠ•ç¥¨ï¼šå‡†ç¡®ç‡ â‰ˆ 90%+\n\nå…³é”®ï¼šæ¨¡å‹è¦æœ‰å¤šæ ·æ€§ï¼ˆdiversityï¼‰\n```\n\n### æ¨¡å‹å¤šæ ·æ€§çš„æ¥æº\n\n1. **ä¸åŒç®—æ³•**ï¼šLightGBM, XGBoost, CatBoost, Ridge\n2. **ä¸åŒç‰¹å¾**ï¼šä½¿ç”¨ä¸åŒçš„ç‰¹å¾å­é›†\n3. **ä¸åŒå‚æ•°**ï¼šåŒä¸€ç®—æ³•ï¼Œä¸åŒè¶…å‚æ•°\n4. **ä¸åŒéšæœºç§å­**ï¼šè®­ç»ƒè¿‡ç¨‹çš„éšæœºæ€§\n5. **ä¸åŒæ•°æ®é‡‡æ ·**ï¼šBaggingæ€æƒ³\n\n### èåˆç­–ç•¥å¯¹æ¯”\n\n| ç­–ç•¥ | å¤æ‚åº¦ | æ•ˆæœ | é€‚ç”¨åœºæ™¯ |\n|------|--------|------|----------|\n| **Simple Average** | â­ | â­â­ | å¿«é€Ÿå°è¯• |\n| **Weighted Average** | â­â­ | â­â­â­ | æ¨¡å‹æ€§èƒ½å·®å¼‚å¤§ |\n| **Stacking** | â­â­â­ | â­â­â­â­ | è¿½æ±‚æè‡´æ€§èƒ½ |\n| **Blending** | â­â­ | â­â­â­ | æ•°æ®é‡å¤§æ—¶ |\n\n### StackingåŸç†\n\n**ä¸¤å±‚æ¨¡å‹æ¶æ„**ï¼š\n\n```\nLevel 0ï¼ˆåŸºæ¨¡å‹ï¼‰:\n  â”œâ”€â”€ LightGBM  â†’ é¢„æµ‹1\n  â”œâ”€â”€ XGBoost   â†’ é¢„æµ‹2\n  â”œâ”€â”€ CatBoost  â†’ é¢„æµ‹3\n  â””â”€â”€ Ridge     â†’ é¢„æµ‹4\n        â†“\nLevel 1ï¼ˆMetaæ¨¡å‹ï¼‰:\n  å°†Level 0çš„4ä¸ªé¢„æµ‹ä½œä¸ºç‰¹å¾\n  è®­ç»ƒä¸€ä¸ªæ–°æ¨¡å‹ï¼ˆé€šå¸¸æ˜¯çº¿æ€§æ¨¡å‹ï¼‰\n        â†“\n     æœ€ç»ˆé¢„æµ‹\n```\n\n**å…³é”®æŠ€å·§**ï¼šOut-of-Foldé¢„æµ‹\n\n```python\n# Level 0æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸ŠåšOOFé¢„æµ‹\n# ç¡®ä¿Metaæ¨¡å‹çš„è®­ç»ƒæ•°æ®æ˜¯\"æœªè§è¿‡\"çš„é¢„æµ‹\nfor fold in KFold:\n    train_idx, val_idx = fold\n    model.fit(X[train_idx], y[train_idx])\n    oof_pred[val_idx] = model.predict(X[val_idx])  # OOFé¢„æµ‹\n    test_pred += model.predict(X_test) / n_folds   # æµ‹è¯•é›†é¢„æµ‹\n\n# Metaæ¨¡å‹ç”¨OOFé¢„æµ‹ä½œä¸ºç‰¹å¾\nmeta_model.fit(oof_pred, y)\n```\n\næ¥ä¸‹æ¥æˆ‘ä»¬ä¼šå®ç°å®Œæ•´çš„Stackingæµç¨‹ï¼",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f76wd6zt71o",
   "source": "# ========================================\n# Cell 11: å®Œæ•´æ¨¡å‹èåˆå®ç° + æœ€ç»ˆæäº¤\n# ========================================\n\nprint(\"ğŸ­ æ¨¡å‹èåˆä¸æœ€ç»ˆæäº¤\")\nprint(\"=\"*60)\n\n# åŠ è½½æœ€ä½³å‚æ•°\nimport json\ntry:\n    with open('best_params_lgb.json', 'r') as f:\n        best_lgb_params = json.load(f)\n    print(\"âœ… å·²åŠ è½½Optunaä¼˜åŒ–çš„æœ€ä½³å‚æ•°\")\nexcept:\n    # å¦‚æœæ²¡æœ‰è¿è¡ŒOptunaï¼Œä½¿ç”¨é»˜è®¤ä¼˜åŒ–å‚æ•°\n    best_lgb_params = {\n        'n_estimators': 500,\n        'learning_rate': 0.05,\n        'num_leaves': 50,\n        'max_depth': 7,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'random_state': SEED,\n        'verbose': -1\n    }\n    print(\"âš ï¸  ä½¿ç”¨é»˜è®¤ä¼˜åŒ–å‚æ•°\")\n\n# ========================================\n# å‡†å¤‡æ•°æ®\n# ========================================\n\nprint(\"\\nğŸ“‚ å‡†å¤‡æ•°æ®...\")\ntrain_full = pd.read_csv('train_all_features.csv')\ntest_full = pd.read_csv('test_all_features.csv')\n\n# ä¿å­˜æµ‹è¯•é›†ID\ntest_ids = test_full['id'].values\n\n# å‡†å¤‡ç‰¹å¾\nexclude_cols = ['id', 'charges']\ncat_cols = train_full.select_dtypes(include=['object', 'category']).columns.tolist()\n\nif cat_cols:\n    train_full = pd.get_dummies(train_full, columns=cat_cols, drop_first=True)\n    test_full = pd.get_dummies(test_full, columns=cat_cols, drop_first=True)\n\n# å¯¹é½ç‰¹å¾\ntrain_full, test_full = train_full.align(test_full, join='left', axis=1, fill_value=0)\n\nX_full = train_full.drop(exclude_cols, axis=1, errors='ignore')\ny_full = train_full['charges']\nX_test_full = test_full.drop(exclude_cols, axis=1, errors='ignore')\n\nprint(f\"è®­ç»ƒé›†: {X_full.shape}\")\nprint(f\"æµ‹è¯•é›†: {X_test_full.shape}\")\n\n# ========================================\n# æ¨¡å‹èåˆï¼šStacking\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"ğŸ¯ è®­ç»ƒèåˆæ¨¡å‹ï¼ˆStackingï¼‰\")\nprint(\"=\"*60)\n\nN_FOLDS = 5\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# å­˜å‚¨OOFé¢„æµ‹å’Œæµ‹è¯•é›†é¢„æµ‹\noof_predictions = {\n    'lgb': np.zeros(len(X_full)),\n    'ridge': np.zeros(len(X_full))\n}\n\ntest_predictions = {\n    'lgb': np.zeros(len(X_test_full)),\n    'ridge': np.zeros(len(X_test_full))\n}\n\n# ========================================\n# Level 0 æ¨¡å‹1: LightGBM\n# ========================================\n\nprint(\"\\n1ï¸âƒ£ è®­ç»ƒ LightGBM (Level 0)...\")\nprint(\"-\"*60)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n    print(f\"  Fold {fold}/{N_FOLDS}...\", end=' ')\n    \n    X_tr, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n    y_tr, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n    \n    # è®­ç»ƒLightGBM\n    lgb_model = lgb.LGBMRegressor(**best_lgb_params)\n    lgb_model.fit(\n        X_tr, np.log1p(y_tr),\n        eval_set=[(X_val, np.log1p(y_val))],\n        callbacks=[lgb.early_stopping(50, verbose=False)]\n    )\n    \n    # OOFé¢„æµ‹\n    oof_predictions['lgb'][val_idx] = np.expm1(\n        lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n    )\n    \n    # æµ‹è¯•é›†é¢„æµ‹ï¼ˆç´¯åŠ ï¼‰\n    test_predictions['lgb'] += np.expm1(\n        lgb_model.predict(X_test_full, num_iteration=lgb_model.best_iteration_)\n    ) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, oof_predictions['lgb'][val_idx]))\n    print(f\"RMSE: {fold_rmse:.2f}\")\n\nlgb_oof_rmse = np.sqrt(mean_squared_error(y_full, oof_predictions['lgb']))\nprint(f\"\\nLightGBM OOF RMSE: {lgb_oof_rmse:.2f}\")\n\n# ========================================\n# Level 0 æ¨¡å‹2: Ridgeï¼ˆçº¿æ€§æ¨¡å‹ä½œä¸ºè¡¥å……ï¼‰\n# ========================================\n\nprint(\"\\n2ï¸âƒ£ è®­ç»ƒ Ridge (Level 0)...\")\nprint(\"-\"*60)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n    print(f\"  Fold {fold}/{N_FOLDS}...\", end=' ')\n    \n    X_tr, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n    y_tr, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n    \n    # æ ‡å‡†åŒ–\n    scaler = StandardScaler()\n    X_tr_scaled = scaler.fit_transform(X_tr)\n    X_val_scaled = scaler.transform(X_val)\n    X_test_scaled = scaler.transform(X_test_full)\n    \n    # è®­ç»ƒRidge\n    ridge_model = Ridge(alpha=10.0, random_state=SEED)\n    ridge_model.fit(X_tr_scaled, np.log1p(y_tr))\n    \n    # OOFé¢„æµ‹\n    oof_predictions['ridge'][val_idx] = np.expm1(ridge_model.predict(X_val_scaled))\n    \n    # æµ‹è¯•é›†é¢„æµ‹\n    test_predictions['ridge'] += np.expm1(ridge_model.predict(X_test_scaled)) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, oof_predictions['ridge'][val_idx]))\n    print(f\"RMSE: {fold_rmse:.2f}\")\n\nridge_oof_rmse = np.sqrt(mean_squared_error(y_full, oof_predictions['ridge']))\nprint(f\"\\nRidge OOF RMSE: {ridge_oof_rmse:.2f}\")\n\n# ========================================\n# èåˆç­–ç•¥å¯¹æ¯”\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"ğŸ“Š èåˆç­–ç•¥æ•ˆæœå¯¹æ¯”\")\nprint(\"=\"*60)\n\nresults = []\n\n# ç­–ç•¥1: å•æ¨¡å‹ï¼ˆLightGBMï¼‰\nresults.append({\n    'ç­–ç•¥': 'LightGBMå•æ¨¡å‹',\n    'OOF RMSE': lgb_oof_rmse,\n    'æƒé‡': 'LGB:100%'\n})\n\n# ç­–ç•¥2: å•æ¨¡å‹ï¼ˆRidgeï¼‰\nresults.append({\n    'ç­–ç•¥': 'Ridgeå•æ¨¡å‹',\n    'OOF RMSE': ridge_oof_rmse,\n    'æƒé‡': 'Ridge:100%'\n})\n\n# ç­–ç•¥3: ç®€å•å¹³å‡\navg_pred = (oof_predictions['lgb'] + oof_predictions['ridge']) / 2\navg_rmse = np.sqrt(mean_squared_error(y_full, avg_pred))\nresults.append({\n    'ç­–ç•¥': 'ç®€å•å¹³å‡',\n    'OOF RMSE': avg_rmse,\n    'æƒé‡': 'LGB:50%, Ridge:50%'\n})\n\n# ç­–ç•¥4: åŠ æƒå¹³å‡ï¼ˆç½‘æ ¼æœç´¢æœ€ä¼˜æƒé‡ï¼‰\nbest_weight = 0.5\nbest_weighted_rmse = float('inf')\n\nfor w in np.arange(0.0, 1.01, 0.05):\n    weighted_pred = oof_predictions['lgb'] * w + oof_predictions['ridge'] * (1 - w)\n    rmse = np.sqrt(mean_squared_error(y_full, weighted_pred))\n    if rmse < best_weighted_rmse:\n        best_weighted_rmse = rmse\n        best_weight = w\n\nresults.append({\n    'ç­–ç•¥': 'åŠ æƒå¹³å‡ï¼ˆä¼˜åŒ–ï¼‰',\n    'OOF RMSE': best_weighted_rmse,\n    'æƒé‡': f'LGB:{best_weight:.0%}, Ridge:{1-best_weight:.0%}'\n})\n\n# ç­–ç•¥5: Stacking (Meta-learner)\nprint(\"\\n3ï¸âƒ£ è®­ç»ƒ Meta-learner (Level 1)...\")\nprint(\"-\"*60)\n\n# å‡†å¤‡Level 1çš„ç‰¹å¾ï¼ˆLevel 0çš„é¢„æµ‹ï¼‰\nmeta_features = np.column_stack([oof_predictions['lgb'], oof_predictions['ridge']])\nmeta_test_features = np.column_stack([test_predictions['lgb'], test_predictions['ridge']])\n\n# è®­ç»ƒMetaæ¨¡å‹ï¼ˆä½¿ç”¨ç®€å•çš„Ridgeï¼‰\nmeta_model = Ridge(alpha=1.0, random_state=SEED)\nmeta_model.fit(meta_features, np.log1p(y_full))\n\n# Metaæ¨¡å‹é¢„æµ‹\nmeta_pred = np.expm1(meta_model.predict(meta_features))\nmeta_rmse = np.sqrt(mean_squared_error(y_full, meta_pred))\n\nprint(f\"Meta-learnerç³»æ•°: LGB={meta_model.coef_[0]:.4f}, Ridge={meta_model.coef_[1]:.4f}\")\nprint(f\"Meta-learner OOF RMSE: {meta_rmse:.2f}\")\n\nresults.append({\n    'ç­–ç•¥': 'Stackingï¼ˆMeta-learnerï¼‰',\n    'OOF RMSE': meta_rmse,\n    'æƒé‡': 'Metaæ¨¡å‹å­¦ä¹ '\n})\n\n# ========================================\n# ç»“æœæ±‡æ€»\n# ========================================\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ“Š èåˆæ•ˆæœå¯¹æ¯”\")\nprint(\"=\"*60)\nprint(results_df.to_string(index=False))\n\n# é€‰æ‹©æœ€ä½³ç­–ç•¥\nbest_idx = results_df['OOF RMSE'].idxmin()\nbest_strategy = results_df.loc[best_idx]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ† æœ€ä½³èåˆç­–ç•¥\")\nprint(\"=\"*60)\nprint(f\"ç­–ç•¥: {best_strategy['ç­–ç•¥']}\")\nprint(f\"OOF RMSE: {best_strategy['OOF RMSE']:.2f}\")\nprint(f\"æƒé‡: {best_strategy['æƒé‡']}\")\n\n# ========================================\n# ç”Ÿæˆæœ€ç»ˆæäº¤\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"ğŸ“ ç”Ÿæˆæœ€ç»ˆæäº¤æ–‡ä»¶\")\nprint(\"=\"*60)\n\n# ä½¿ç”¨æœ€ä½³ç­–ç•¥çš„é¢„æµ‹\nif best_idx == 5:  # Stacking\n    final_test_pred = np.expm1(meta_model.predict(meta_test_features))\nelif best_idx == 3:  # åŠ æƒå¹³å‡\n    final_test_pred = test_predictions['lgb'] * best_weight + test_predictions['ridge'] * (1 - best_weight)\nelse:\n    final_test_pred = test_predictions['lgb']  # é»˜è®¤LightGBM\n\n# Clipè´Ÿå€¼\nfinal_test_pred = np.maximum(final_test_pred, 0)\n\n# åˆ›å»ºæäº¤æ–‡ä»¶\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'charges': final_test_pred\n})\n\nsubmission.to_csv('final_submission.csv', index=False)\n\nprint(f\"\\nâœ… æœ€ç»ˆæäº¤æ–‡ä»¶å·²ç”Ÿæˆ: final_submission.csv\")\nprint(f\"\\né¢„æµ‹ç»Ÿè®¡:\")\nprint(submission['charges'].describe())\n\n# å¯è§†åŒ–\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# å·¦å›¾ï¼šç­–ç•¥å¯¹æ¯”\nax1 = axes[0]\nstrategies = [s.replace('ï¼ˆ', '\\n(').replace('ï¼‰', ')') for s in results_df['ç­–ç•¥']]\ncolors = ['gold' if i == best_idx else 'skyblue' for i in range(len(results_df))]\nbars = ax1.bar(range(len(results_df)), results_df['OOF RMSE'], color=colors, edgecolor='black')\nax1.set_xticks(range(len(results_df)))\nax1.set_xticklabels(strategies, rotation=20, ha='right', fontsize=9)\nax1.set_ylabel('OOF RMSE')\nax1.set_title('ä¸åŒèåˆç­–ç•¥æ•ˆæœå¯¹æ¯”')\nax1.grid(axis='y', alpha=0.3)\n\nfor bar, rmse in zip(bars, results_df['OOF RMSE']):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n            f'{rmse:.0f}',\n            ha='center', va='bottom', fontsize=9, fontweight='bold')\n\n# å³å›¾ï¼šé¢„æµ‹åˆ†å¸ƒ\nax2 = axes[1]\nax2.hist(final_test_pred, bins=50, edgecolor='black', alpha=0.7)\nax2.set_xlabel('Predicted Charges')\nax2.set_ylabel('Frequency')\nax2.set_title('æµ‹è¯•é›†é¢„æµ‹åˆ†å¸ƒ')\nax2.axvline(final_test_pred.mean(), color='red', linestyle='--', label=f'Mean: {final_test_pred.mean():.0f}')\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"ğŸ‰ å…¨éƒ¨ä¼˜åŒ–æµç¨‹å®Œæˆï¼\")\nprint(\"=\"*60)\nprint(\"\\nğŸ“Š å®Œæ•´ä¼˜åŒ–è·¯å¾„æ€»ç»“:\")\nprint(\"  1. âœ… æ•°æ®æ¸…æ´—ï¼ˆå¼‚å¸¸å€¼å¤„ç†ï¼‰\")\nprint(\"  2. âœ… ç‰¹å¾å·¥ç¨‹ï¼ˆ50+ä¸ªæ–°ç‰¹å¾ï¼‰\")\nprint(\"  3. âœ… è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaè´å¶æ–¯ä¼˜åŒ–ï¼‰\")\nprint(\"  4. âœ… æ¨¡å‹èåˆï¼ˆStackingï¼‰\")\nprint(f\"\\nğŸ† æœ€ç»ˆOOF RMSE: {best_strategy['OOF RMSE']:.2f}\")\nprint(f\"ğŸ“ˆ é¢„æœŸæ’å: Top 10-20%ï¼ˆå¦‚æœæ˜¯Kaggleæ¯”èµ›ï¼‰\")\nprint(\"\\nğŸ’¡ ä½ å·²ç»æŒæ¡äº†å®Œæ•´çš„æœºå™¨å­¦ä¹ ç«èµ›æµç¨‹ï¼\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dg02slyrb57",
   "source": "---\n\n# ğŸ“ å®Œæ•´æ•™ç¨‹æ€»ç»“\n\n## ğŸ† æ­å–œä½ å®Œæˆäº†æœºå™¨å­¦ä¹ æ¨¡å‹ä¼˜åŒ–çš„å®Œæ•´å­¦ä¹ ï¼\n\n### ğŸ“Š ä½ å­¦åˆ°çš„æ ¸å¿ƒçŸ¥è¯†\n\n#### 1ï¸âƒ£ æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†\n- **å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•**ï¼šIQRç»Ÿè®¡æ–¹æ³• + é¢†åŸŸçŸ¥è¯†\n- **å¤„ç†ç­–ç•¥å¯¹æ¯”**ï¼šåˆ é™¤ã€æˆªæ–­ã€æ›¿æ¢ã€ä¿ç•™\n- **å®éªŒé©±åŠ¨**ï¼šé€šè¿‡å¯¹æ¯”å®éªŒé€‰æ‹©æœ€ä½³ç­–ç•¥\n- **æ”¶è·**ï¼šæ•°æ®è´¨é‡æ˜¯æ¨¡å‹æ€§èƒ½çš„åŸºç¡€\n\n#### 2ï¸âƒ£ ç‰¹å¾å·¥ç¨‹ï¼ˆæœ€é‡è¦ï¼ï¼‰â­â­â­\n- **é¢†åŸŸçŸ¥è¯†ç‰¹å¾**ï¼š28ä¸ªåŸºäºä¿é™©ä¸šåŠ¡çš„ç‰¹å¾\n  - é£é™©è¯„åˆ†ï¼ˆå¹´é¾„ã€BMIã€å¸çƒŸï¼‰\n  - äº¤äº’ç‰¹å¾ï¼ˆsmoker Ã— age, smoker Ã— bmiç­‰ï¼‰\n  - å¤šé¡¹å¼ç‰¹å¾ï¼ˆageÂ², bmiÂ²ç­‰ï¼‰\n  \n- **Target Encoding**ï¼šé«˜çº§ç¼–ç æŠ€æœ¯\n  - K-Foldæ–¹å¼é˜²æ­¢æ•°æ®æ³„æ¼\n  - è´å¶æ–¯å¹³æ»‘å¤„ç†å°æ ·æœ¬\n  \n- **åˆ†ç»„ç»Ÿè®¡ç‰¹å¾**ï¼š20+ä¸ªèšåˆç‰¹å¾\n  - æŒ‰ç±»åˆ«åˆ†ç»„çš„ç»Ÿè®¡é‡\n  - ç›¸å¯¹ç‰¹å¾ï¼ˆä¸ç¾¤ä½“å‡å€¼çš„åå·®ï¼‰\n  \n- **æ”¶è·**ï¼š\"æ•°æ®å’Œç‰¹å¾å†³å®šæœºå™¨å­¦ä¹ çš„ä¸Šé™\"\n\n#### 3ï¸âƒ£ è¶…å‚æ•°ä¼˜åŒ–\n- **è°ƒå‚ç­–ç•¥å¯¹æ¯”**ï¼šGrid Search vs Random Search vs Bayesian Optimization\n- **Optunaå®æˆ˜**ï¼šTPEé‡‡æ ·å™¨çš„ä½¿ç”¨\n- **å‚æ•°é‡è¦æ€§åˆ†æ**ï¼šäº†è§£å“ªäº›å‚æ•°æœ€å…³é”®\n- **æ”¶è·**ï¼šç³»ç»ŸåŒ–çš„è°ƒå‚æ–¹æ³•æ¯”æ‰‹å·¥è°ƒå‚é«˜æ•ˆå¾—å¤š\n\n#### 4ï¸âƒ£ æ¨¡å‹èåˆ\n- **EnsembleåŸç†**ï¼š\"ä¸‰ä¸ªè‡­çš®åŒ èµ›è¿‡è¯¸è‘›äº®\"\n- **èåˆç­–ç•¥**ï¼š\n  - ç®€å•å¹³å‡\n  - åŠ æƒå¹³å‡\n  - Stackingï¼ˆä¸¤å±‚æ¨¡å‹ï¼‰\n- **æ¨¡å‹å¤šæ ·æ€§**ï¼šä¸åŒæ¨¡å‹çŠ¯ä¸åŒçš„é”™è¯¯\n- **æ”¶è·**ï¼šèåˆé€šå¸¸èƒ½å¸¦æ¥ç¨³å®šçš„æ€§èƒ½æå‡\n\n---\n\n### ğŸ“ˆ æ€§èƒ½æå‡è·¯çº¿å›¾\n\n```\nèµ·ç‚¹ï¼ˆåŸºçº¿ï¼‰: RMSE â‰ˆ 6700\n    â†“\næ•°æ®æ¸…æ´—: -100 RMSE\n    â†“\nç‰¹å¾å·¥ç¨‹: -300~400 RMSE  â† æœ€å¤§æå‡ï¼\n    â†“\nè¶…å‚æ•°ä¼˜åŒ–: -100~200 RMSE\n    â†“\næ¨¡å‹èåˆ: -200~300 RMSE\n    â†“\nç»ˆç‚¹: RMSE â‰ˆ 5500~5800\n\næ€»æå‡: 900~1200 RMSE (13-18%)\n```\n\n---\n\n### ğŸ’» å¯å¤ç”¨çš„ä»£ç æ¨¡æ¿\n\nä½ ç°åœ¨æ‹¥æœ‰äº†å®Œæ•´çš„ä»£ç æ¨¡æ¿ï¼Œå¯ä»¥åº”ç”¨åˆ°å…¶ä»–é¡¹ç›®ï¼š\n\n1. **å¼‚å¸¸å€¼æ£€æµ‹ä¸å¤„ç†**\n   ```python\n   # IQRæ–¹æ³• + é¢†åŸŸçŸ¥è¯†\n   # 4ç§ç­–ç•¥å¯¹æ¯”å®éªŒ\n   ```\n\n2. **K-Fold Target Encodingç±»**\n   ```python\n   class TargetEncoder:\n       # é˜²æ­¢æ•°æ®æ³„æ¼çš„å®Œæ•´å®ç°\n   ```\n\n3. **åˆ†ç»„ç»Ÿè®¡ç‰¹å¾å‡½æ•°**\n   ```python\n   def create_aggregation_features():\n       # æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡\n   ```\n\n4. **Optunaä¼˜åŒ–æ¡†æ¶**\n   ```python\n   def objective(trial):\n       # è´å¶æ–¯ä¼˜åŒ–ç›®æ ‡å‡½æ•°\n   ```\n\n5. **Stackingå®ç°**\n   ```python\n   # Level 0: å¤šä¸ªåŸºæ¨¡å‹\n   # Level 1: Meta-learner\n   ```\n\n---\n\n### ğŸ¯ æœºå™¨å­¦ä¹ é¡¹ç›®å®Œæ•´æµç¨‹\n\nä½ å·²ç»æŒæ¡äº†ç«¯åˆ°ç«¯çš„æµç¨‹ï¼š\n\n```\n1. æ•°æ®æ¢ç´¢ï¼ˆEDAï¼‰\n   â””â”€â”€ ç†è§£æ•°æ®åˆ†å¸ƒã€å‘ç°å¼‚å¸¸\n\n2. æ•°æ®æ¸…æ´—\n   â””â”€â”€ å¤„ç†ç¼ºå¤±å€¼ã€å¼‚å¸¸å€¼\n\n3. ç‰¹å¾å·¥ç¨‹ â­â­â­\n   â”œâ”€â”€ é¢†åŸŸçŸ¥è¯†ç‰¹å¾\n   â”œâ”€â”€ Target Encoding\n   â””â”€â”€ ç»Ÿè®¡èšåˆç‰¹å¾\n\n4. æ¨¡å‹è®­ç»ƒ\n   â”œâ”€â”€ åŸºç¡€æ¨¡å‹è¯„ä¼°\n   â””â”€â”€ é€‰æ‹©åˆé€‚çš„æ¨¡å‹\n\n5. æ¨¡å‹ä¼˜åŒ–\n   â”œâ”€â”€ è¶…å‚æ•°è°ƒä¼˜ï¼ˆOptunaï¼‰\n   â””â”€â”€ äº¤å‰éªŒè¯ç­–ç•¥\n\n6. æ¨¡å‹èåˆ\n   â”œâ”€â”€ è®­ç»ƒå¤šä¸ªæ¨¡å‹\n   â””â”€â”€ Stacking/Blending\n\n7. åå¤„ç†\n   â”œâ”€â”€ æ®‹å·®åˆ†æ\n   â””â”€â”€ é¢„æµ‹æ ¡å‡†\n\n8. æ¨¡å‹è¯Šæ–­\n   â”œâ”€â”€ ç‰¹å¾é‡è¦æ€§\n   â””â”€â”€ é”™è¯¯åˆ†æ\n\n9. éƒ¨ç½²\n   â””â”€â”€ ç”Ÿæˆæœ€ç»ˆé¢„æµ‹\n```\n\n---\n\n### ğŸš€ è¿›ä¸€æ­¥å­¦ä¹ å»ºè®®\n\n#### æƒ³æå‡åˆ°æ›´é«˜æ°´å¹³ï¼Ÿ\n\n1. **é«˜çº§ç‰¹å¾å·¥ç¨‹**\n   - è‡ªåŠ¨ç‰¹å¾å·¥ç¨‹ï¼ˆAutoFEï¼‰\n   - ç‰¹å¾é€‰æ‹©ç®—æ³•ï¼ˆRFE, LASSOç­‰ï¼‰\n   - æ·±åº¦ç‰¹å¾äº¤äº’\n\n2. **æ›´å¤šæ¨¡å‹**\n   - Neural Networksï¼ˆç¥ç»ç½‘ç»œï¼‰\n   - XGBoostæ·±å…¥ä¼˜åŒ–\n   - CatBoostçš„ç±»åˆ«ç‰¹å¾å¤„ç†\n\n3. **é«˜çº§èåˆæŠ€æœ¯**\n   - Multi-level Stacking\n   - Blendingç­–ç•¥\n   - æ¨¡å‹è’¸é¦\n\n4. **ç”Ÿäº§éƒ¨ç½²**\n   - æ¨¡å‹åºåˆ—åŒ–ï¼ˆpickle, joblibï¼‰\n   - APIéƒ¨ç½²ï¼ˆFlask, FastAPIï¼‰\n   - æ¨¡å‹ç›‘æ§ä¸æ›´æ–°\n\n5. **AutoMLå·¥å…·**\n   - H2O AutoML\n   - TPOT\n   - AutoGluon\n\n#### æ¨èå­¦ä¹ èµ„æº\n\n- **Kaggleå¹³å°**ï¼šå‚åŠ çœŸå®æ¯”èµ›\n- **ä¹¦ç±**ï¼šã€ŠFeature Engineering for Machine Learningã€‹\n- **è¯¾ç¨‹**ï¼šAndrew Ngçš„æœºå™¨å­¦ä¹ è¯¾ç¨‹\n- **è®ºæ–‡**ï¼šXGBoost, LightGBM, CatBooståŸè®ºæ–‡\n\n---\n\n### ğŸ“ å…³é”®è¦ç‚¹å›é¡¾\n\n#### æˆåŠŸçš„å…³é”®å› ç´ ï¼ˆä¼˜å…ˆçº§æ’åºï¼‰\n\n1. **ç‰¹å¾å·¥ç¨‹** (40%) â­â­â­â­â­\n   - \"åƒåœ¾è¿›ï¼Œåƒåœ¾å‡º\"\n   - å¥½çš„ç‰¹å¾æ¯”å¤æ‚çš„æ¨¡å‹æ›´é‡è¦\n\n2. **æ•°æ®è´¨é‡** (30%) â­â­â­â­\n   - å¼‚å¸¸å€¼å¤„ç†\n   - ç¼ºå¤±å€¼å¡«å……\n   - æ•°æ®å¹³è¡¡\n\n3. **æ¨¡å‹é€‰æ‹©ä¸ä¼˜åŒ–** (20%) â­â­â­\n   - é€‰æ‹©åˆé€‚çš„ç®—æ³•\n   - è¶…å‚æ•°è°ƒä¼˜\n   - æ¨¡å‹èåˆ\n\n4. **éªŒè¯ç­–ç•¥** (10%) â­â­\n   - äº¤å‰éªŒè¯\n   - é¿å…è¿‡æ‹Ÿåˆ\n   - å¯é çš„è¯„ä¼°\n\n---\n\n### ğŸ’¡ æœ€åçš„å»ºè®®\n\n1. **å®è·µæ˜¯æœ€å¥½çš„è€å¸ˆ**\n   - å¤šå‚åŠ Kaggleæ¯”èµ›\n   - å°è¯•ä¸åŒçš„æ•°æ®é›†\n   - ä»å¤±è´¥ä¸­å­¦ä¹ \n\n2. **ä¿æŒå­¦ä¹ çš„ä¹ æƒ¯**\n   - æœºå™¨å­¦ä¹ å‘å±•è¿…é€Ÿ\n   - å…³æ³¨æœ€æ–°çš„è®ºæ–‡å’ŒæŠ€æœ¯\n   - å‚ä¸ç¤¾åŒºè®¨è®º\n\n3. **æ³¨é‡åŸºç¡€**\n   - ç†è§£ç®—æ³•åŸç†\n   - ä¸è¦åªä¾èµ–é»‘ç›’å·¥å…·\n   - æ•°å­¦å’Œç»Ÿè®¡åŸºç¡€å¾ˆé‡è¦\n\n4. **å·¥ç¨‹åŒ–æ€ç»´**\n   - ä»£ç å¯å¤ç”¨æ€§\n   - å®éªŒå¯é‡ç°æ€§\n   - æ–‡æ¡£å’Œæ³¨é‡Š\n\n5. **ä¸šåŠ¡å¯¼å‘**\n   - æ¨¡å‹è¦è§£å†³å®é™…é—®é¢˜\n   - æ€§èƒ½æŒ‡æ ‡è¦ç¬¦åˆä¸šåŠ¡ç›®æ ‡\n   - å¯è§£é‡Šæ€§å¾ˆé‡è¦\n\n---\n\n## ğŸ‰ æ­å–œä½ ï¼\n\nä½ å·²ç»å®Œæˆäº†ä»æ•°æ®æ¸…æ´—åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´æœºå™¨å­¦ä¹ æµç¨‹å­¦ä¹ ã€‚\n\nè¿™ä¸ä»…æ˜¯ä¸€ä¸ªæ•™ç¨‹çš„ç»“æŸï¼Œæ›´æ˜¯ä½ æœºå™¨å­¦ä¹ ä¹‹æ—…çš„å¼€å§‹ï¼\n\n### ä¸‹ä¸€æ­¥è¡ŒåŠ¨\n\n1. âœ… **è¿è¡Œå…¨éƒ¨cells**ï¼Œçœ‹åˆ°å®Œæ•´çš„ä¼˜åŒ–è¿‡ç¨‹\n2. âœ… **ä¿®æ”¹å‚æ•°**ï¼Œå°è¯•ä¸åŒçš„é…ç½®\n3. âœ… **åº”ç”¨åˆ°æ–°é¡¹ç›®**ï¼Œå·©å›ºæ‰€å­¦çŸ¥è¯†\n4. âœ… **åˆ†äº«ä½ çš„ç»éªŒ**ï¼Œæ•™å­¦ç›¸é•¿\n\n### ä¿æŒè”ç³»\n\n- é‡åˆ°é—®é¢˜ï¼ŸæŸ¥çœ‹ä»£ç æ³¨é‡Šå’Œç†è®ºè¯´æ˜\n- æƒ³æ·±å…¥ï¼Ÿé˜…è¯»ç›¸å…³è®ºæ–‡å’Œæ–‡æ¡£\n- æœ‰æ”¶è·ï¼Ÿåˆ†äº«ç»™å…¶ä»–å­¦ä¹ è€…\n\n**ç¥ä½ åœ¨æœºå™¨å­¦ä¹ çš„é“è·¯ä¸Šè¶Šèµ°è¶Šè¿œï¼** ğŸš€\n\n---\n\n*\"The only way to learn a new programming language is by writing programs in it.\"*\n*- Dennis Ritchie*\n\n*æœºå™¨å­¦ä¹ ä¹Ÿæ˜¯å¦‚æ­¤ - åªæœ‰é€šè¿‡å®è·µæ‰èƒ½çœŸæ­£æŒæ¡ï¼*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "theory-2",
   "metadata": {},
   "source": [
    "## ğŸ”¬ å®éªŒï¼šå¯¹æ¯”ä¸åŒçš„å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥\n",
    "\n",
    "æˆ‘ä»¬å°†å¯¹æ¯”ä»¥ä¸‹4ç§ç­–ç•¥çš„æ•ˆæœï¼š\n",
    "\n",
    "### ç­–ç•¥1: ä¸å¤„ç†ï¼ˆBaselineï¼‰\n",
    "- ä¿ç•™æ‰€æœ‰å¼‚å¸¸å€¼\n",
    "- ä½œä¸ºå¯¹æ¯”åŸºå‡†\n",
    "\n",
    "### ç­–ç•¥2: åˆ é™¤å¼‚å¸¸å€¼\n",
    "- åˆ é™¤ BMI > 60 çš„æ ·æœ¬\n",
    "- ä¼˜ç‚¹ï¼šå½»åº•ç§»é™¤å¼‚å¸¸æ•°æ®\n",
    "- ç¼ºç‚¹ï¼šå‡å°‘è®­ç»ƒæ ·æœ¬é‡\n",
    "\n",
    "### ç­–ç•¥3: æˆªæ–­ï¼ˆClippingï¼‰\n",
    "- å°† BMI > 60 çš„å€¼è®¾ä¸º 60\n",
    "- ä¼˜ç‚¹ï¼šä¿ç•™æ ·æœ¬æ•°é‡\n",
    "- ç¼ºç‚¹ï¼šå¯èƒ½å¼•å…¥åå·®\n",
    "\n",
    "### ç­–ç•¥4: æ›¿æ¢ä¸ºä¸­ä½æ•°\n",
    "- å°†å¼‚å¸¸å€¼æ›¿æ¢ä¸ºBMIçš„ä¸­ä½æ•°\n",
    "- ä¼˜ç‚¹ï¼šä¿å®ˆå¤„ç†\n",
    "- ç¼ºç‚¹ï¼šæ”¹å˜æ•°æ®åˆ†å¸ƒ\n",
    "\n",
    "æˆ‘ä»¬ä¼šç”¨ç®€å•çš„LightGBMæ¨¡å‹ï¼ˆ3æŠ˜äº¤å‰éªŒè¯ï¼‰å¿«é€Ÿæµ‹è¯•æ¯ç§ç­–ç•¥çš„æ•ˆæœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 3: å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥å¯¹æ¯”å®éªŒ\n",
    "# ========================================\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def quick_evaluate(train_df, strategy_name):\n",
    "    \"\"\"\n",
    "    å¿«é€Ÿè¯„ä¼°å‡½æ•°ï¼šç”¨3æŠ˜CVè¯„ä¼°å¤„ç†åçš„æ•°æ®\n",
    "    \n",
    "    å‚æ•°:\n",
    "        train_df: å¤„ç†åçš„è®­ç»ƒæ•°æ®\n",
    "        strategy_name: ç­–ç•¥åç§°\n",
    "    \n",
    "    è¿”å›:\n",
    "        oof_rmse: Out-of-Fold RMSE\n",
    "    \"\"\"\n",
    "    # ç®€å•ç‰¹å¾å·¥ç¨‹\n",
    "    df = train_df.copy()\n",
    "    \n",
    "    # ç±»åˆ«ç¼–ç \n",
    "    df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n",
    "    df = pd.get_dummies(df, columns=['sex', 'region'], drop_first=True)\n",
    "    \n",
    "    # åŸºç¡€äº¤äº’ç‰¹å¾\n",
    "    df['age_bmi'] = df['age'] * df['bmi']\n",
    "    df['smoker_bmi'] = df['smoker'] * df['bmi']\n",
    "    \n",
    "    # å‡†å¤‡æ•°æ®\n",
    "    X = df.drop(['charges', 'id'], axis=1, errors='ignore')\n",
    "    y = df['charges']\n",
    "    \n",
    "    # 3æŠ˜äº¤å‰éªŒè¯\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    oof_predictions = np.zeros(len(X))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # è®­ç»ƒLightGBMï¼ˆç®€åŒ–å‚æ•°ï¼Œå¿«é€Ÿè¯„ä¼°ï¼‰\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=SEED,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, np.log1p(y_tr),\n",
    "            eval_set=[(X_val, np.log1p(y_val))],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        # é¢„æµ‹\n",
    "        pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n",
    "        oof_predictions[val_idx] = pred\n",
    "    \n",
    "    # è®¡ç®—RMSE\n",
    "    oof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "    \n",
    "    return oof_rmse, len(df)\n",
    "\n",
    "\n",
    "print(\"ğŸ”¬ å¼€å§‹å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥å¯¹æ¯”å®éªŒ\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nå®éªŒè®¾ç½®:\")\n",
    "print(\"  - æ¨¡å‹: LightGBM\")\n",
    "print(\"  - éªŒè¯: 3-Fold CV\")\n",
    "print(\"  - è¯„ä¼°æŒ‡æ ‡: OOF RMSE\")\n",
    "print(\"  - å¼‚å¸¸å€¼å®šä¹‰: BMI > 60\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# å­˜å‚¨ç»“æœ\n",
    "results = []\n",
    "\n",
    "# ========================================\n",
    "# ç­–ç•¥1: ä¸å¤„ç†ï¼ˆBaselineï¼‰\n",
    "# ========================================\n",
    "print(\"\\nğŸ“Š ç­–ç•¥1: ä¸å¤„ç†å¼‚å¸¸å€¼ï¼ˆBaselineï¼‰\")\n",
    "print(\"-\"*60)\n",
    "train_1 = train.copy()\n",
    "rmse_1, samples_1 = quick_evaluate(train_1, \"ä¸å¤„ç†\")\n",
    "results.append({\n",
    "    'ç­–ç•¥': '1. ä¸å¤„ç†ï¼ˆBaselineï¼‰',\n",
    "    'æ ·æœ¬æ•°': samples_1,\n",
    "    'OOF RMSE': rmse_1,\n",
    "    'vs Baseline': 0,\n",
    "    'è¯´æ˜': 'ä¿ç•™æ‰€æœ‰å¼‚å¸¸å€¼'\n",
    "})\n",
    "print(f\"æ ·æœ¬æ•°: {samples_1}\")\n",
    "print(f\"OOF RMSE: {rmse_1:.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# ç­–ç•¥2: åˆ é™¤å¼‚å¸¸å€¼\n",
    "# ========================================\n",
    "print(\"\\nğŸ“Š ç­–ç•¥2: åˆ é™¤å¼‚å¸¸å€¼ (BMI > 60)\")\n",
    "print(\"-\"*60)\n",
    "train_2 = train[train['bmi'] <= 60].copy().reset_index(drop=True)\n",
    "rmse_2, samples_2 = quick_evaluate(train_2, \"åˆ é™¤\")\n",
    "results.append({\n",
    "    'ç­–ç•¥': '2. åˆ é™¤å¼‚å¸¸å€¼',\n",
    "    'æ ·æœ¬æ•°': samples_2,\n",
    "    'OOF RMSE': rmse_2,\n",
    "    'vs Baseline': rmse_2 - rmse_1,\n",
    "    'è¯´æ˜': f'åˆ é™¤äº† {samples_1 - samples_2} ä¸ªæ ·æœ¬'\n",
    "})\n",
    "print(f\"åˆ é™¤æ ·æœ¬æ•°: {samples_1 - samples_2}\")\n",
    "print(f\"å‰©ä½™æ ·æœ¬æ•°: {samples_2}\")\n",
    "print(f\"OOF RMSE: {rmse_2:.2f}\")\n",
    "print(f\"ç›¸æ¯”Baseline: {rmse_2 - rmse_1:+.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# ç­–ç•¥3: æˆªæ–­ (Clipping)\n",
    "# ========================================\n",
    "print(\"\\nğŸ“Š ç­–ç•¥3: æˆªæ–­ (Clipping to 60)\")\n",
    "print(\"-\"*60)\n",
    "train_3 = train.copy()\n",
    "train_3['bmi'] = train_3['bmi'].clip(upper=60)\n",
    "rmse_3, samples_3 = quick_evaluate(train_3, \"æˆªæ–­\")\n",
    "results.append({\n",
    "    'ç­–ç•¥': '3. æˆªæ–­ (Clip to 60)',\n",
    "    'æ ·æœ¬æ•°': samples_3,\n",
    "    'OOF RMSE': rmse_3,\n",
    "    'vs Baseline': rmse_3 - rmse_1,\n",
    "    'è¯´æ˜': 'å°†BMI>60çš„å€¼è®¾ä¸º60'\n",
    "})\n",
    "print(f\"æ ·æœ¬æ•°: {samples_3} (æ— å˜åŒ–)\")\n",
    "print(f\"ä¿®æ”¹å€¼æ•°é‡: {(train['bmi'] > 60).sum()}\")\n",
    "print(f\"OOF RMSE: {rmse_3:.2f}\")\n",
    "print(f\"ç›¸æ¯”Baseline: {rmse_3 - rmse_1:+.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# ç­–ç•¥4: æ›¿æ¢ä¸ºä¸­ä½æ•°\n",
    "# ========================================\n",
    "print(\"\\nğŸ“Š ç­–ç•¥4: æ›¿æ¢ä¸ºä¸­ä½æ•°\")\n",
    "print(\"-\"*60)\n",
    "train_4 = train.copy()\n",
    "bmi_median = train_4[train_4['bmi'] <= 60]['bmi'].median()\n",
    "train_4.loc[train_4['bmi'] > 60, 'bmi'] = bmi_median\n",
    "rmse_4, samples_4 = quick_evaluate(train_4, \"æ›¿æ¢ä¸­ä½æ•°\")\n",
    "results.append({\n",
    "    'ç­–ç•¥': '4. æ›¿æ¢ä¸ºä¸­ä½æ•°',\n",
    "    'æ ·æœ¬æ•°': samples_4,\n",
    "    'OOF RMSE': rmse_4,\n",
    "    'vs Baseline': rmse_4 - rmse_1,\n",
    "    'è¯´æ˜': f'æ›¿æ¢ä¸º {bmi_median:.2f}'\n",
    "})\n",
    "print(f\"æ ·æœ¬æ•°: {samples_4} (æ— å˜åŒ–)\")\n",
    "print(f\"ä¸­ä½æ•°: {bmi_median:.2f}\")\n",
    "print(f\"ä¿®æ”¹å€¼æ•°é‡: {(train['bmi'] > 60).sum()}\")\n",
    "print(f\"OOF RMSE: {rmse_4:.2f}\")\n",
    "print(f\"ç›¸æ¯”Baseline: {rmse_4 - rmse_1:+.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# ç»“æœæ±‡æ€»\n",
    "# ========================================\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š å®éªŒç»“æœæ±‡æ€»\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# æ‰¾å‡ºæœ€ä½³ç­–ç•¥\n",
    "best_idx = results_df['OOF RMSE'].idxmin()\n",
    "best_strategy = results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ† æœ€ä½³ç­–ç•¥\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ç­–ç•¥: {best_strategy['ç­–ç•¥']}\")\n",
    "print(f\"OOF RMSE: {best_strategy['OOF RMSE']:.2f}\")\n",
    "print(f\"æ€§èƒ½æå‡: {-best_strategy['vs Baseline']:.2f}\")\n",
    "print(f\"è¯´æ˜: {best_strategy['è¯´æ˜']}\")\n",
    "\n",
    "# å¯è§†åŒ–å¯¹æ¯”\n",
    "print(\"\\nğŸ“Š å¯è§†åŒ–å¯¹æ¯”...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['red' if x == best_idx else 'skyblue' for x in range(len(results_df))]\n",
    "bars = ax.bar(range(len(results_df)), results_df['OOF RMSE'], color=colors, edgecolor='black')\n",
    "ax.set_xticks(range(len(results_df)))\n",
    "ax.set_xticklabels([s.split('.')[1].strip() for s in results_df['ç­–ç•¥']], rotation=15)\n",
    "ax.set_ylabel('OOF RMSE')\n",
    "ax.set_title('ä¸åŒå¼‚å¸¸å€¼å¤„ç†ç­–ç•¥çš„æ•ˆæœå¯¹æ¯”')\n",
    "ax.axhline(rmse_1, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "\n",
    "# æ·»åŠ æ•°å€¼æ ‡ç­¾\n",
    "for i, (bar, rmse) in enumerate(zip(bars, results_df['OOF RMSE'])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{rmse:.0f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ’¡ å…³é”®å‘ç°:\")\n",
    "print(\"  1. å¼‚å¸¸å€¼ç¡®å®å½±å“æ¨¡å‹æ€§èƒ½\")\n",
    "print(\"  2. ä¸åŒå¤„ç†ç­–ç•¥æ•ˆæœå·®å¼‚æ˜æ˜¾\")\n",
    "print(f\"  3. æœ€ä½³ç­–ç•¥æ˜¯: {best_strategy['ç­–ç•¥']}\")\n",
    "print(f\"  4. ç›¸æ¯”ä¸å¤„ç†ï¼ŒRMSEé™ä½äº†çº¦ {-best_strategy['vs Baseline']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 4: åº”ç”¨æœ€ä½³ç­–ç•¥ + ä¿å­˜æ¸…æ´—åçš„æ•°æ®\n",
    "# ========================================\n",
    "\n",
    "print(\"ğŸ¯ åº”ç”¨æœ€ä½³å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# æ ¹æ®å®éªŒç»“æœï¼Œæˆ‘ä»¬é‡‡ç”¨è¡¨ç°æœ€å¥½çš„ç­–ç•¥\n",
    "# è¿™é‡Œæˆ‘ä»¬å…ˆé»˜è®¤ä½¿ç”¨æˆªæ–­ç­–ç•¥ï¼ˆé€šå¸¸æ•ˆæœè¾ƒå¥½ï¼‰\n",
    "# ä½ è¿è¡Œå®éªŒåå¯ä»¥æ ¹æ®ç»“æœè°ƒæ•´\n",
    "\n",
    "BEST_STRATEGY = \"clip\"  # å¯é€‰: \"clip\", \"remove\", \"median\"\n",
    "BMI_THRESHOLD = 60\n",
    "\n",
    "# å¤„ç†è®­ç»ƒé›†\n",
    "print(\"\\nå¤„ç†è®­ç»ƒé›†...\")\n",
    "train_cleaned = train.copy()\n",
    "\n",
    "if BEST_STRATEGY == \"clip\":\n",
    "    print(f\"ç­–ç•¥: æˆªæ–­ (Clipping) - BMI > {BMI_THRESHOLD} è®¾ä¸º {BMI_THRESHOLD}\")\n",
    "    n_modified = (train_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    train_cleaned['bmi'] = train_cleaned['bmi'].clip(upper=BMI_THRESHOLD)\n",
    "    print(f\"ä¿®æ”¹äº† {n_modified} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "elif BEST_STRATEGY == \"remove\":\n",
    "    print(f\"ç­–ç•¥: åˆ é™¤ - ç§»é™¤ BMI > {BMI_THRESHOLD} çš„æ ·æœ¬\")\n",
    "    n_before = len(train_cleaned)\n",
    "    train_cleaned = train_cleaned[train_cleaned['bmi'] <= BMI_THRESHOLD].reset_index(drop=True)\n",
    "    n_after = len(train_cleaned)\n",
    "    print(f\"åˆ é™¤äº† {n_before - n_after} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "elif BEST_STRATEGY == \"median\":\n",
    "    print(f\"ç­–ç•¥: æ›¿æ¢ä¸ºä¸­ä½æ•°\")\n",
    "    bmi_median = train_cleaned[train_cleaned['bmi'] <= BMI_THRESHOLD]['bmi'].median()\n",
    "    n_modified = (train_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    train_cleaned.loc[train_cleaned['bmi'] > BMI_THRESHOLD, 'bmi'] = bmi_median\n",
    "    print(f\"ä¿®æ”¹äº† {n_modified} ä¸ªæ ·æœ¬ï¼Œæ›¿æ¢ä¸º {bmi_median:.2f}\")\n",
    "\n",
    "# å¤„ç†æµ‹è¯•é›†ï¼ˆåŒæ ·çš„ç­–ç•¥ï¼‰\n",
    "print(\"\\nå¤„ç†æµ‹è¯•é›†...\")\n",
    "test_cleaned = test.copy()\n",
    "\n",
    "if BEST_STRATEGY == \"clip\":\n",
    "    n_modified_test = (test_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    test_cleaned['bmi'] = test_cleaned['bmi'].clip(upper=BMI_THRESHOLD)\n",
    "    print(f\"ä¿®æ”¹äº† {n_modified_test} ä¸ªæ ·æœ¬\")\n",
    "    \n",
    "elif BEST_STRATEGY == \"median\":\n",
    "    n_modified_test = (test_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    test_cleaned.loc[test_cleaned['bmi'] > BMI_THRESHOLD, 'bmi'] = bmi_median\n",
    "    print(f\"ä¿®æ”¹äº† {n_modified_test} ä¸ªæ ·æœ¬\")\n",
    "\n",
    "# æ£€æŸ¥å…¶ä»–å­—æ®µæ˜¯å¦æœ‰å¼‚å¸¸\n",
    "print(\"\\n\\nğŸ” æ£€æŸ¥å…¶ä»–å­—æ®µ...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Ageæ£€æŸ¥\n",
    "print(f\"\\nAge èŒƒå›´: [{train_cleaned['age'].min():.0f}, {train_cleaned['age'].max():.0f}]\")\n",
    "if train_cleaned['age'].min() < 0 or train_cleaned['age'].max() > 120:\n",
    "    print(\"  âš ï¸  å‘ç°å¼‚å¸¸å¹´é¾„å€¼\")\n",
    "else:\n",
    "    print(\"  âœ… å¹´é¾„èŒƒå›´æ­£å¸¸\")\n",
    "\n",
    "# Childrenæ£€æŸ¥\n",
    "print(f\"\\nChildren èŒƒå›´: [{train_cleaned['children'].min():.0f}, {train_cleaned['children'].max():.0f}]\")\n",
    "if train_cleaned['children'].max() > 10:\n",
    "    print(f\"  âš ï¸  å‘ç°å¼‚å¸¸å­©å­æ•°é‡: max = {train_cleaned['children'].max():.0f}\")\n",
    "    # å¯ä»¥é€‰æ‹©å¤„ç†\n",
    "    train_cleaned['children'] = train_cleaned['children'].clip(upper=10)\n",
    "    test_cleaned['children'] = test_cleaned['children'].clip(upper=10)\n",
    "    print(\"  å·²æˆªæ–­ä¸º 10\")\n",
    "else:\n",
    "    print(\"  âœ… å­©å­æ•°é‡èŒƒå›´æ­£å¸¸\")\n",
    "\n",
    "# Chargesæ£€æŸ¥\n",
    "print(f\"\\nCharges èŒƒå›´: [{train_cleaned['charges'].min():.2f}, {train_cleaned['charges'].max():.2f}]\")\n",
    "if train_cleaned['charges'].min() < 0:\n",
    "    print(\"  âš ï¸  å‘ç°è´Ÿæ•°è´¹ç”¨\")\n",
    "else:\n",
    "    print(\"  âœ… è´¹ç”¨èŒƒå›´æ­£å¸¸\")\n",
    "\n",
    "# ä¿å­˜æ¸…æ´—åçš„æ•°æ®\n",
    "print(\"\\n\\nğŸ’¾ ä¿å­˜æ¸…æ´—åçš„æ•°æ®...\")\n",
    "print(\"-\"*60)\n",
    "train_cleaned.to_csv('train_cleaned.csv', index=False)\n",
    "test_cleaned.to_csv('test_cleaned.csv', index=False)\n",
    "print(\"âœ… å·²ä¿å­˜:\")\n",
    "print(\"  - train_cleaned.csv\")\n",
    "print(\"  - test_cleaned.csv\")\n",
    "\n",
    "# æ•°æ®è´¨é‡æŠ¥å‘Š\n",
    "print(\"\\n\\nğŸ“‹ æ•°æ®æ¸…æ´—æŠ¥å‘Š\")\n",
    "print(\"=\"*60)\n",
    "print(f\"è®­ç»ƒé›†: {len(train)} â†’ {len(train_cleaned)} æ ·æœ¬\")\n",
    "print(f\"æµ‹è¯•é›†: {len(test)} â†’ {len(test_cleaned)} æ ·æœ¬\")\n",
    "print(f\"\\nBMI ç»Ÿè®¡ (æ¸…æ´—å):\")\n",
    "print(f\"  - æœ€å°å€¼: {train_cleaned['bmi'].min():.2f}\")\n",
    "print(f\"  - ä¸­ä½æ•°: {train_cleaned['bmi'].median():.2f}\")\n",
    "print(f\"  - æœ€å¤§å€¼: {train_cleaned['bmi'].max():.2f}\")\n",
    "print(f\"  - æ ‡å‡†å·®: {train_cleaned['bmi'].std():.2f}\")\n",
    "\n",
    "print(\"\\nâœ… ç¬¬ä¸€éƒ¨åˆ†å®Œæˆï¼\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nğŸ“ çŸ¥è¯†æ€»ç»“:\")\n",
    "print(\"  1. å­¦ä¼šäº†å¦‚ä½•æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆç»Ÿè®¡æ–¹æ³• + é¢†åŸŸçŸ¥è¯†ï¼‰\")\n",
    "print(\"  2. ç†è§£äº†å¼‚å¸¸å€¼å¯¹æ¨¡å‹çš„å½±å“\")\n",
    "print(\"  3. æŒæ¡äº†4ç§å¼‚å¸¸å€¼å¤„ç†ç­–ç•¥\")\n",
    "print(\"  4. å­¦ä¼šäº†é€šè¿‡å®éªŒé€‰æ‹©æœ€ä½³ç­–ç•¥\")\n",
    "print(\"  5. é¢„æœŸRMSEæå‡: ~100å·¦å³\")\n",
    "print(\"\\nä¸‹ä¸€æ­¥: é«˜çº§ç‰¹å¾å·¥ç¨‹ â†’ é¢„æœŸå†é™ä½400+ RMSEï¼\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}