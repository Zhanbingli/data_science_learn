{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# 🚀 机器学习模型优化完整教程\n",
    "\n",
    "## 📊 教程概览\n",
    "\n",
    "**目标**: 系统学习机器学习模型优化的完整流程，将 RMSE 从 6697 优化到 5600-5800（提升13-16%）\n",
    "\n",
    "### 📚 学习路线图\n",
    "\n",
    "```\n",
    "第一部分：环境准备与数据清洗\n",
    "  ├── 异常值检测与处理\n",
    "  └── 数据质量优化\n",
    "  预期提升: -100 RMSE\n",
    "\n",
    "第二部分：高级特征工程 ⭐⭐⭐\n",
    "  ├── 领域知识特征\n",
    "  ├── Target Encoding\n",
    "  ├── 分组统计特征\n",
    "  └── 特征选择\n",
    "  预期提升: -400 RMSE\n",
    "\n",
    "第三部分：超参数优化\n",
    "  ├── Optuna 贝叶斯优化\n",
    "  └── 参数重要性分析\n",
    "  预期提升: -150 RMSE\n",
    "\n",
    "第四部分：验证策略优化\n",
    "  └── 分层交叉验证\n",
    "  预期提升: 稳定性提升\n",
    "\n",
    "第五部分：模型融合 ⭐⭐⭐\n",
    "  ├── XGBoost + CatBoost\n",
    "  ├── 加权平均\n",
    "  └── Stacking\n",
    "  预期提升: -300 RMSE\n",
    "\n",
    "第六部分：后处理优化\n",
    "  └── 残差分析与校准\n",
    "  预期提升: -100 RMSE\n",
    "\n",
    "第七部分：模型诊断\n",
    "  └── SHAP 可解释性分析\n",
    "```\n",
    "\n",
    "### 🎯 性能提升预期\n",
    "\n",
    "| 阶段 | RMSE | 提升 | 累计提升 |\n",
    "|------|------|------|----------|\n",
    "| 基线（当前） | 6697 | - | - |\n",
    "| 数据清洗 | 6600 | -97 | -97 |\n",
    "| 特征工程 | 6200 | -400 | -497 |\n",
    "| 超参数优化 | 6050 | -150 | -647 |\n",
    "| 验证策略 | 6000 | -50 | -697 |\n",
    "| 模型融合 | 5700 | -300 | -997 |\n",
    "| 后处理 | 5600 | -100 | -1097 |\n",
    "| **最终目标** | **5600** | - | **-16.4%** |\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 学习建议\n",
    "\n",
    "1. **按顺序执行每个cell** - 每个cell都有明确的学习目标\n",
    "2. **仔细阅读理论部分** - 理解\"为什么\"比\"怎么做\"更重要\n",
    "3. **观察每个优化的效果** - 对比RMSE的变化\n",
    "4. **尝试修改参数** - 动手实验才能真正掌握\n",
    "5. **记录关键知识点** - 可以在笔记本中添加自己的思考\n",
    "\n",
    "---\n",
    "\n",
    "## 🔧 环境要求\n",
    "\n",
    "需要安装的库：\n",
    "- `optuna` - 超参数优化\n",
    "- `xgboost` - XGBoost模型\n",
    "- `catboost` - CatBoost模型\n",
    "- `shap` - 模型可解释性（可选）\n",
    "\n",
    "已有的库：\n",
    "- pandas, numpy, sklearn, lightgbm, matplotlib, seaborn\n",
    "\n",
    "现在开始学习之旅！🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-title",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 第一部分：环境准备与数据清洗\n",
    "\n",
    "## 🎯 学习目标\n",
    "1. 理解异常值对模型的影响\n",
    "2. 掌握异常值检测方法\n",
    "3. 学习不同的异常值处理策略\n",
    "4. 通过实验验证处理效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 1: 导入库 + 环境检查\n",
    "# ========================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 设置中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # Mac\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"✅ 基础库导入成功！\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 检查并安装必要的库\n",
    "print(\"\\n🔍 检查额外依赖...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 检查 optuna\n",
    "try:\n",
    "    import optuna\n",
    "    print(\"✅ optuna 已安装 (版本: {})\".format(optuna.__version__))\n",
    "except ImportError:\n",
    "    print(\"❌ optuna 未安装\")\n",
    "    print(\"   安装命令: pip install optuna\")\n",
    "\n",
    "# 检查 xgboost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    print(\"✅ xgboost 已安装 (版本: {})\".format(xgb.__version__))\n",
    "except ImportError:\n",
    "    print(\"❌ xgboost 未安装\")\n",
    "    print(\"   安装命令: pip install xgboost\")\n",
    "\n",
    "# 检查 catboost\n",
    "try:\n",
    "    import catboost as cb\n",
    "    print(\"✅ catboost 已安装 (版本: {})\".format(cb.__version__))\n",
    "except ImportError:\n",
    "    print(\"❌ catboost 未安装\")\n",
    "    print(\"   安装命令: pip install catboost\")\n",
    "\n",
    "# 检查 shap\n",
    "try:\n",
    "    import shap\n",
    "    print(\"✅ shap 已安装 (版本: {})\".format(shap.__version__))\n",
    "except ImportError:\n",
    "    print(\"⚠️  shap 未安装（可选，用于模型解释）\")\n",
    "    print(\"   安装命令: pip install shap\")\n",
    "\n",
    "print(\"\\n💡 提示: 如果有未安装的库，请在终端运行对应的安装命令\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theory-1",
   "metadata": {},
   "source": [
    "## 📚 理论知识：异常值的影响\n",
    "\n",
    "### 什么是异常值？\n",
    "\n",
    "异常值（Outliers）是指数据集中与其他观测值显著不同的数据点。它们可能是：\n",
    "1. **数据错误** - 录入错误、传感器故障等\n",
    "2. **真实的极端情况** - 罕见但真实存在的情况\n",
    "\n",
    "### 异常值对模型的影响\n",
    "\n",
    "#### 1️⃣ 对线性模型的影响（严重）\n",
    "```\n",
    "线性回归使用最小二乘法：\n",
    "Loss = Σ(y_true - y_pred)²\n",
    "\n",
    "异常值会产生巨大的误差平方，导致：\n",
    "- 模型参数被\"拉偏\"\n",
    "- 预测性能下降\n",
    "```\n",
    "\n",
    "#### 2️⃣ 对树模型的影响（较小但仍存在）\n",
    "```\n",
    "树模型虽然对异常值相对鲁棒，但：\n",
    "- 异常值可能成为单独的叶子节点\n",
    "- 影响特征重要性计算\n",
    "- 降低模型泛化能力\n",
    "```\n",
    "\n",
    "### 在我们的数据中\n",
    "\n",
    "从之前的探索中发现：\n",
    "- **BMI 最大值 = 29330.99** ← 这显然是错误数据！\n",
    "- 正常人类BMI范围：15-50（极端肥胖也很少超过60）\n",
    "- BMI = 体重(kg) / 身高²(m²)\n",
    "\n",
    "**例子**：\n",
    "- BMI = 29330 意味着：假设身高1.7m，体重需要 29330 × 1.7² ≈ 84,735 kg！\n",
    "- 这是不可能的，必定是数据错误\n",
    "\n",
    "### 异常值检测方法\n",
    "\n",
    "#### 方法1: 统计方法\n",
    "```python\n",
    "# IQR方法（四分位距）\n",
    "Q1 = df['feature'].quantile(0.25)\n",
    "Q3 = df['feature'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower = Q1 - 1.5 * IQR\n",
    "upper = Q3 + 1.5 * IQR\n",
    "```\n",
    "\n",
    "#### 方法2: 领域知识\n",
    "```python\n",
    "# 基于业务常识\n",
    "# 例如：BMI通常在 15-60 之间\n",
    "outliers = df[df['bmi'] > 60]\n",
    "```\n",
    "\n",
    "#### 方法3: 可视化\n",
    "```python\n",
    "# 箱线图、散点图等直观展示\n",
    "```\n",
    "\n",
    "### 异常值处理策略\n",
    "\n",
    "| 策略 | 优点 | 缺点 | 适用场景 |\n",
    "|------|------|------|----------|\n",
    "| **删除** | 简单直接 | 损失数据 | 数据量大，异常值少 |\n",
    "| **截断（Clip）** | 保留数据量 | 可能引入偏差 | 异常值是极端真实值 |\n",
    "| **替换（中位数/均值）** | 保留样本 | 改变分布 | 缺失数据填充 |\n",
    "| **保留** | 完整性 | 影响模型 | 异常值有意义 |\n",
    "| **单独建模** | 精细化 | 复杂 | 异常值有特殊模式 |\n",
    "\n",
    "接下来我们会通过实验对比不同策略的效果！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 2: 加载数据 + 异常值检测\n",
    "# ========================================\n",
    "\n",
    "print(\"📂 加载数据...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 加载数据\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"训练集形状: {train.shape}\")\n",
    "print(f\"测试集形状: {test.shape}\")\n",
    "print(\"\\n前5行数据:\")\n",
    "print(train.head())\n",
    "\n",
    "# ========================================\n",
    "# 异常值检测\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\\n🔍 异常值检测分析\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. 统计描述\n",
    "print(\"\\n📊 数值特征统计（重点关注BMI）:\")\n",
    "print(train[['age', 'bmi', 'children', 'charges']].describe())\n",
    "\n",
    "# 2. BMI 异常值检测\n",
    "print(\"\\n\\n🎯 BMI 字段详细分析:\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# 领域知识：正常BMI范围\n",
    "NORMAL_BMI_MIN = 15\n",
    "NORMAL_BMI_MAX = 60  # 极端肥胖的上限\n",
    "\n",
    "# 统计方法：IQR\n",
    "Q1 = train['bmi'].quantile(0.25)\n",
    "Q3 = train['bmi'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "iqr_lower = Q1 - 1.5 * IQR\n",
    "iqr_upper = Q3 + 1.5 * IQR\n",
    "\n",
    "print(f\"BMI 基本统计:\")\n",
    "print(f\"  - 最小值: {train['bmi'].min():.2f}\")\n",
    "print(f\"  - 25分位: {Q1:.2f}\")\n",
    "print(f\"  - 中位数: {train['bmi'].median():.2f}\")\n",
    "print(f\"  - 75分位: {Q3:.2f}\")\n",
    "print(f\"  - 最大值: {train['bmi'].max():.2f} ← 明显异常！\")\n",
    "print(f\"  - 均值: {train['bmi'].mean():.2f}\")\n",
    "print(f\"  - 标准差: {train['bmi'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nIQR方法检测:\")\n",
    "print(f\"  - IQR = {IQR:.2f}\")\n",
    "print(f\"  - 下界 = Q1 - 1.5*IQR = {iqr_lower:.2f}\")\n",
    "print(f\"  - 上界 = Q3 + 1.5*IQR = {iqr_upper:.2f}\")\n",
    "\n",
    "# 统计异常值数量\n",
    "outliers_iqr = train[(train['bmi'] < iqr_lower) | (train['bmi'] > iqr_upper)]\n",
    "outliers_domain = train[(train['bmi'] < NORMAL_BMI_MIN) | (train['bmi'] > NORMAL_BMI_MAX)]\n",
    "\n",
    "print(f\"\\n异常值统计:\")\n",
    "print(f\"  - IQR方法检测到: {len(outliers_iqr)} 个异常值 ({len(outliers_iqr)/len(train)*100:.2f}%)\")\n",
    "print(f\"  - 领域知识检测到: {len(outliers_domain)} 个异常值 ({len(outliers_domain)/len(train)*100:.2f}%)\")\n",
    "\n",
    "# 显示极端异常值\n",
    "extreme_outliers = train[train['bmi'] > 100]\n",
    "print(f\"\\n⚠️  极端异常值 (BMI > 100): {len(extreme_outliers)} 个\")\n",
    "if len(extreme_outliers) > 0:\n",
    "    print(\"\\n前10个极端异常样本:\")\n",
    "    print(extreme_outliers[['id', 'age', 'bmi', 'charges']].head(10))\n",
    "\n",
    "# 可视化\n",
    "print(\"\\n\\n📊 可视化分析...\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. BMI 直方图\n",
    "axes[0, 0].hist(train['bmi'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].axvline(NORMAL_BMI_MAX, color='red', linestyle='--', label=f'正常上限 ({NORMAL_BMI_MAX})')\n",
    "axes[0, 0].set_xlabel('BMI')\n",
    "axes[0, 0].set_ylabel('频数')\n",
    "axes[0, 0].set_title('BMI 分布（包含异常值）')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. BMI 箱线图\n",
    "axes[0, 1].boxplot(train['bmi'])\n",
    "axes[0, 1].set_ylabel('BMI')\n",
    "axes[0, 1].set_title('BMI 箱线图')\n",
    "axes[0, 1].axhline(NORMAL_BMI_MAX, color='red', linestyle='--', label='正常上限')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. 过滤异常值后的直方图\n",
    "normal_bmi = train[(train['bmi'] >= NORMAL_BMI_MIN) & (train['bmi'] <= NORMAL_BMI_MAX)]['bmi']\n",
    "axes[1, 0].hist(normal_bmi, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "axes[1, 0].set_xlabel('BMI')\n",
    "axes[1, 0].set_ylabel('频数')\n",
    "axes[1, 0].set_title('BMI 分布（过滤异常值后）')\n",
    "\n",
    "# 4. BMI vs Charges 散点图\n",
    "axes[1, 1].scatter(train['bmi'], train['charges'], alpha=0.5, s=10)\n",
    "axes[1, 1].axvline(NORMAL_BMI_MAX, color='red', linestyle='--', label='异常值分界线')\n",
    "axes[1, 1].set_xlabel('BMI')\n",
    "axes[1, 1].set_ylabel('Charges')\n",
    "axes[1, 1].set_title('BMI vs Charges')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 观察要点:\")\n",
    "print(\"  1. 左上图：BMI分布严重右偏，有极端异常值\")\n",
    "print(\"  2. 右上图：箱线图显示大量异常点\")\n",
    "print(\"  3. 左下图：过滤后的BMI呈现正态分布\")\n",
    "print(\"  4. 右下图：异常BMI值的样本费用分布异常\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4xdroultufd",
   "source": "---\n\n# 第二部分：高级特征工程 ⭐⭐⭐\n\n## 🎯 学习目标\n1. 理解特征工程在机器学习中的核心地位\n2. 掌握领域知识特征的创建方法\n3. 学习Target Encoding技术及防止数据泄漏的方法\n4. 掌握分组统计特征的创建\n5. 学会评估特征的有效性\n\n## 💡 为什么特征工程最重要?\n\n在机器学习界有一句名言：\n> **\"数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已\"**\n\n### 特征工程的价值\n\n```\n好的特征工程 > 复杂的模型调参\n\n例子：\n- 仅用线性回归 + 优秀特征 → 可能超过 随机森林 + 原始特征\n- Kaggle比赛中，前几名的差异往往在于特征工程，而非模型选择\n```\n\n### 特征工程金字塔\n\n```\nLevel 4: 自动特征工程（AutoFE）\n          ↑ 使用工具自动生成特征\n          \nLevel 3: 高阶特征\n          ↑ Target Encoding, 统计聚合，深度交互\n          \nLevel 2: 基础组合特征\n          ↑ 简单交互，多项式，分箱\n          \nLevel 1: 原始特征处理\n          ↑ 缺失值填充，类别编码，标准化\n          \nLevel 0: 原始数据\n```\n\n我们会从Level 1逐步向上构建！",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "k9zbaepozja",
   "source": "## 📚 理论知识：保险领域的业务逻辑\n\n### 保险费用的决定因素\n\n在医疗保险定价中，保险公司主要考虑以下风险因素：\n\n#### 1️⃣ 吸烟状态（Smoker）- 最重要！\n```\n吸烟者的医疗费用通常是非吸烟者的 2-3 倍\n\n原因：\n- 更高的心血管疾病风险\n- 癌症风险增加\n- 呼吸系统疾病\n- 预期寿命缩短\n```\n\n#### 2️⃣ 年龄（Age）\n```\n费用随年龄增长：\n- 18-30岁：基础费用较低\n- 30-50岁：逐渐增加\n- 50+岁：显著增加（慢性病多发期）\n\n关键节点：\n- 40岁：健康风险开始显著上升\n- 60岁：进入老年期，费用大幅增加\n```\n\n#### 3️⃣ BMI（Body Mass Index）\n```\nBMI分类（WHO标准）：\n- < 18.5：体重不足\n- 18.5-25：正常\n- 25-30：超重\n- 30-35：肥胖（I级）\n- 35-40：肥胖（II级）\n- > 40：病态肥胖（III级）\n\n医疗成本关系：\n- 正常BMI：基准费用\n- 超重/肥胖：费用增加 20-50%\n- 病态肥胖：费用可能翻倍\n```\n\n#### 4️⃣ 交互效应\n```\n重要的交互：\n1. 吸烟 × BMI\n   - 吸烟 + 肥胖 = 风险叠加（非线性）\n   - 影响：心血管疾病风险指数级增长\n\n2. 吸烟 × 年龄\n   - 年龄越大，吸烟的累积伤害越明显\n   \n3. 年龄 × BMI\n   - 老年肥胖者的医疗费用显著高于年轻肥胖者\n   \n4. 有孩子 × 家庭计划\n   - 家庭成员数量影响医疗需求\n```\n\n### 特征创建策略\n\n基于上述领域知识，我们可以创建：\n\n**风险评分特征**：\n- 吸烟风险分数\n- 年龄风险分数\n- BMI风险分数\n- 综合风险评分\n\n**交互特征**：\n- smoker × age\n- smoker × bmi\n- age × bmi\n- smoker × age × bmi（三阶交互）\n\n**分箱特征**：\n- age_group（年龄段）\n- bmi_category（BMI分类）\n- risk_level（风险等级）\n\n接下来我们会逐一实现这些特征！",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "bndn6bhxne7",
   "source": "# ========================================\n# Cell 5: 领域知识特征创建\n# ========================================\n\nprint(\"🏗️ 创建领域知识特征\")\nprint(\"=\"*60)\n\n# 加载清洗后的数据\nprint(\"\\n📂 加载清洗后的数据...\")\ntrain_clean = pd.read_csv('train_cleaned.csv')\ntest_clean = pd.read_csv('test_cleaned.csv')\nprint(f\"训练集: {train_clean.shape}\")\nprint(f\"测试集: {test_clean.shape}\")\n\ndef create_domain_features(df):\n    \"\"\"\n    基于保险领域知识创建特征\n    \n    参数:\n        df: 原始数据框\n    \n    返回:\n        df: 添加了新特征的数据框\n    \"\"\"\n    df = df.copy()\n    \n    # ========================================\n    # 1. 年龄相关特征\n    # ========================================\n    print(\"\\n🎯 创建年龄相关特征...\")\n    \n    # 年龄分组（基于医疗风险阶段）\n    df['age_group'] = pd.cut(\n        df['age'],\n        bins=[0, 18, 30, 40, 50, 65, 100],\n        labels=['teenager', 'young_adult', 'adult', 'middle_age', 'senior', 'elderly']\n    )\n    \n    # 年龄风险分数（指数增长模式）\n    # 40岁以下风险较低，之后快速增长\n    df['age_risk_score'] = df['age'].apply(lambda x: \n        1.0 if x < 30 else\n        1.5 if x < 40 else\n        2.0 if x < 50 else\n        3.0 if x < 60 else\n        4.5\n    )\n    \n    # 是否老年人（60岁以上）\n    df['is_senior'] = (df['age'] >= 60).astype(int)\n    \n    # 是否高风险年龄（50岁以上）\n    df['is_high_risk_age'] = (df['age'] >= 50).astype(int)\n    \n    print(f\"  - age_group: 6个年龄段\")\n    print(f\"  - age_risk_score: 风险评分 (1.0-4.5)\")\n    print(f\"  - is_senior: 是否老年人\")\n    print(f\"  - is_high_risk_age: 是否高风险年龄\")\n    \n    # ========================================\n    # 2. BMI相关特征\n    # ========================================\n    print(\"\\n🎯 创建BMI相关特征...\")\n    \n    # BMI分类（WHO标准）\n    df['bmi_category'] = pd.cut(\n        df['bmi'],\n        bins=[0, 18.5, 25, 30, 35, 40, 100],\n        labels=['underweight', 'normal', 'overweight', 'obese_1', 'obese_2', 'obese_3']\n    )\n    \n    # BMI风险分数\n    df['bmi_risk_score'] = df['bmi'].apply(lambda x:\n        1.2 if x < 18.5 else  # 体重不足也有风险\n        1.0 if x < 25 else    # 正常\n        1.3 if x < 30 else    # 超重\n        1.8 if x < 35 else    # 肥胖I级\n        2.5 if x < 40 else    # 肥胖II级\n        3.5                    # 病态肥胖\n    )\n    \n    # 是否肥胖\n    df['is_obese'] = (df['bmi'] >= 30).astype(int)\n    \n    # 是否病态肥胖\n    df['is_severely_obese'] = (df['bmi'] >= 35).astype(int)\n    \n    # BMI偏离正常值的程度\n    # 正常BMI中心值为21.75（18.5-25的中点）\n    df['bmi_deviation'] = np.abs(df['bmi'] - 21.75)\n    \n    print(f\"  - bmi_category: 6个BMI分类\")\n    print(f\"  - bmi_risk_score: 风险评分 (1.0-3.5)\")\n    print(f\"  - is_obese: 是否肥胖\")\n    print(f\"  - is_severely_obese: 是否病态肥胖\")\n    print(f\"  - bmi_deviation: BMI偏离度\")\n    \n    # ========================================\n    # 3. 吸烟相关特征\n    # ========================================\n    print(\"\\n🎯 创建吸烟相关特征...\")\n    \n    # 先编码smoker（如果还是字符串）\n    if df['smoker'].dtype == 'object':\n        df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n    \n    # 吸烟风险分数（吸烟者风险是非吸烟者的2.5倍）\n    df['smoker_risk_score'] = df['smoker'].apply(lambda x: 2.5 if x == 1 else 1.0)\n    \n    print(f\"  - smoker: 编码为 0/1\")\n    print(f\"  - smoker_risk_score: 风险评分 (1.0 或 2.5)\")\n    \n    # ========================================\n    # 4. 家庭相关特征\n    # ========================================\n    print(\"\\n🎯 创建家庭相关特征...\")\n    \n    # 是否有孩子\n    df['has_children'] = (df['children'] > 0).astype(int)\n    \n    # 家庭规模（假设有配偶）\n    df['family_size'] = df['children'] + 2  # 本人 + 配偶 + 孩子\n    \n    # 多子女家庭\n    df['large_family'] = (df['children'] >= 3).astype(int)\n    \n    print(f\"  - has_children: 是否有孩子\")\n    print(f\"  - family_size: 家庭规模\")\n    print(f\"  - large_family: 是否多子女\")\n    \n    # ========================================\n    # 5. 综合风险评分\n    # ========================================\n    print(\"\\n🎯 创建综合风险评分...\")\n    \n    # 方法1: 简单加权平均\n    df['risk_score_simple'] = (\n        df['age_risk_score'] * 0.3 +\n        df['bmi_risk_score'] * 0.3 +\n        df['smoker_risk_score'] * 0.4  # 吸烟影响最大\n    )\n    \n    # 方法2: 乘法交互（风险叠加）\n    df['risk_score_multiplicative'] = (\n        df['age_risk_score'] *\n        df['bmi_risk_score'] *\n        df['smoker_risk_score']\n    )\n    \n    # 高风险人群标识（多个高风险因素叠加）\n    df['high_risk_count'] = (\n        df['is_high_risk_age'] +\n        df['is_obese'] +\n        df['smoker']\n    )\n    \n    # 是否极高风险（3个因素都有）\n    df['is_very_high_risk'] = (df['high_risk_count'] >= 3).astype(int)\n    \n    print(f\"  - risk_score_simple: 加权风险分\")\n    print(f\"  - risk_score_multiplicative: 乘法风险分\")\n    print(f\"  - high_risk_count: 高风险因素数量\")\n    print(f\"  - is_very_high_risk: 是否极高风险\")\n    \n    # ========================================\n    # 6. 交互特征\n    # ========================================\n    print(\"\\n🎯 创建交互特征...\")\n    \n    # 核心交互（这些交互在保险定价中特别重要）\n    df['smoker_age'] = df['smoker'] * df['age']\n    df['smoker_bmi'] = df['smoker'] * df['bmi']\n    df['age_bmi'] = df['age'] * df['bmi']\n    \n    # 三阶交互（吸烟 × 年龄 × BMI）\n    df['smoker_age_bmi'] = df['smoker'] * df['age'] * df['bmi']\n    \n    # 吸烟 × 肥胖（特别危险的组合）\n    df['smoker_and_obese'] = df['smoker'] * df['is_obese']\n    \n    # 老年 × 肥胖\n    df['senior_and_obese'] = df['is_senior'] * df['is_obese']\n    \n    # 吸烟 × 老年\n    df['smoker_and_senior'] = df['smoker'] * df['is_senior']\n    \n    print(f\"  - smoker_age, smoker_bmi, age_bmi: 二阶交互\")\n    print(f\"  - smoker_age_bmi: 三阶交互\")\n    print(f\"  - smoker_and_obese: 吸烟×肥胖\")\n    print(f\"  - senior_and_obese: 老年×肥胖\")\n    print(f\"  - smoker_and_senior: 吸烟×老年\")\n    \n    # ========================================\n    # 7. 多项式特征\n    # ========================================\n    print(\"\\n🎯 创建多项式特征...\")\n    \n    # 年龄和BMI的平方（捕捉非线性关系）\n    df['age_squared'] = df['age'] ** 2\n    df['bmi_squared'] = df['bmi'] ** 2\n    \n    # 立方项（更高阶的非线性）\n    df['age_cubed'] = df['age'] ** 3\n    \n    print(f\"  - age_squared, bmi_squared: 平方项\")\n    print(f\"  - age_cubed: 立方项\")\n    \n    return df\n\n# 应用特征工程\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"开始特征工程...\")\nprint(\"=\"*60)\n\ntrain_fe = create_domain_features(train_clean)\ntest_fe = create_domain_features(test_clean)\n\nprint(\"\\n\\n✅ 领域知识特征创建完成！\")\nprint(\"=\"*60)\nprint(f\"\\n原始特征数: {train_clean.shape[1]}\")\nprint(f\"新增特征后: {train_fe.shape[1]}\")\nprint(f\"新增特征数: {train_fe.shape[1] - train_clean.shape[1]}\")\n\n# 查看新特征\nprint(\"\\n📋 新增的特征列表:\")\nnew_cols = [col for col in train_fe.columns if col not in train_clean.columns]\nfor i, col in enumerate(new_cols, 1):\n    print(f\"  {i:2d}. {col}\")\n\n# 查看部分特征的统计信息\nprint(\"\\n📊 部分新特征的统计信息:\")\nfeature_cols = ['age_risk_score', 'bmi_risk_score', 'smoker_risk_score', \n                'risk_score_simple', 'risk_score_multiplicative']\nprint(train_fe[feature_cols].describe())\n\n# 查看极高风险人群占比\nprint(f\"\\n⚠️  极高风险人群占比: {train_fe['is_very_high_risk'].mean()*100:.2f}%\")\nprint(f\"   (同时满足：年龄≥50 + BMI≥30 + 吸烟)\")\n\n# 保存特征工程后的数据（暂时不保存类别特征的one-hot编码版本）\nprint(\"\\n💾 保存特征工程后的数据...\")\ntrain_fe.to_csv('train_domain_features.csv', index=False)\ntest_fe.to_csv('test_domain_features.csv', index=False)\nprint(\"✅ 已保存:\")\nprint(\"  - train_domain_features.csv\")\nprint(\"  - test_domain_features.csv\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "stfderigb5o",
   "source": "## 📚 理论知识：Target Encoding\n\n### 什么是 Target Encoding？\n\nTarget Encoding（目标编码）是一种强大的类别变量编码方法，特别适用于：\n- 高基数类别特征（类别数量很多）\n- 类别特征与目标变量有强相关性的情况\n\n### 原理\n\n**基本思想**：用该类别对应的目标变量平均值来替代类别值\n\n```python\n# 例如：region 列\nnortheast → 该region下所有样本的平均charges\nnorthwest → 该region下所有样本的平均charges\nsoutheast → 该region下所有样本的平均charges\nsouthwest → 该region下所有样本的平均charges\n```\n\n### 为什么比 One-Hot Encoding 更好？\n\n| 方法 | 优点 | 缺点 |\n|------|------|------|\n| **One-Hot** | 简单，无数据泄漏 | 高基数时维度爆炸，无法捕捉类别的\"数值意义\" |\n| **Label Encoding** | 维度不增加 | 引入了不存在的顺序关系 |\n| **Target Encoding** | 维度不增加，捕捉类别与目标的关系 | 需要防止过拟合和数据泄漏 |\n\n### 示例对比\n\n假设我们有以下数据：\n\n```\nregion     | charges\n-----------|---------\nnortheast  | 10000\nnortheast  | 12000\nnorthwest  | 8000\nnorthwest  | 9000\n```\n\n**One-Hot Encoding**:\n```\nnortheast_0  northeast_1  northwest_0  northwest_1\n1            0            0            0            → 10000\n1            0            0            0            → 12000\n0            1            0            0            → 8000\n0            1            0            0            → 9000\n```\n\n**Target Encoding**:\n```\nregion_encoded\n11000  → (10000+12000)/2\n11000\n8500   → (8000+9000)/2\n8500\n```\n\n**优势明显**：Target Encoding直接告诉模型\"northeast的平均费用是11000\"！\n\n### ⚠️ 关键问题：数据泄漏\n\n**什么是数据泄漏？**\n\n如果我们直接用全局平均值编码，会发生：\n```python\n# 错误做法❌\ntrain['region_encoded'] = train.groupby('region')['charges'].transform('mean')\n```\n\n**问题**：每个样本的编码值包含了它自己的目标值信息！\n- 这在训练时会让模型\"作弊\"\n- 在测试时无法复现（测试集没有目标值）\n\n### ✅ 正确做法：K-Fold Target Encoding\n\n**核心思想**：使用交叉验证方式，确保每个样本的编码值不包含自己的目标值\n\n```python\n# 伪代码\nfor fold in KFold:\n    train_idx, val_idx = fold\n    \n    # 用训练集计算均值\n    encoding_map = train[train_idx].groupby('region')['charges'].mean()\n    \n    # 应用到验证集（验证集样本的编码不包含自己）\n    train.loc[val_idx, 'region_encoded'] = train.loc[val_idx, 'region'].map(encoding_map)\n```\n\n### 增强技巧\n\n#### 1. 平滑处理（Smoothing）\n\n避免小样本类别的不稳定估计：\n\n```python\nglobal_mean = train['charges'].mean()\ncategory_stats = train.groupby('region').agg({\n    'charges': ['mean', 'count']\n})\n\n# 贝叶斯平滑\nsmoothing_factor = 10  # 超参数\nsmoothed_mean = (\n    (category_stats['mean'] * category_stats['count'] + global_mean * smoothing_factor) /\n    (category_stats['count'] + smoothing_factor)\n)\n```\n\n**作用**：小样本类别会向全局均值\"收缩\"\n\n#### 2. 添加噪声\n\n防止过拟合：\n\n```python\nencoded_values = encoded_values + np.random.normal(0, std, size=len(encoded_values))\n```\n\n接下来我们会实现完整的 K-Fold Target Encoding！",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "b2tjqm9wkvh",
   "source": "# ========================================\n# Cell 6: Target Encoding 实现\n# ========================================\n\nprint(\"🎯 实现 K-Fold Target Encoding\")\nprint(\"=\"*60)\n\nfrom sklearn.model_selection import KFold\n\nclass TargetEncoder:\n    \"\"\"\n    K-Fold Target Encoding 编码器\n    \n    防止数据泄漏的关键：\n    - 训练集：使用K-Fold方式，每个样本的编码不包含自己\n    - 测试集：使用全部训练数据的统计值\n    \"\"\"\n    \n    def __init__(self, columns, n_splits=5, smoothing=10, random_state=42):\n        \"\"\"\n        参数:\n            columns: 需要编码的列名列表\n            n_splits: K折数量\n            smoothing: 平滑参数（越大越向全局均值收缩）\n            random_state: 随机种子\n        \"\"\"\n        self.columns = columns\n        self.n_splits = n_splits\n        self.smoothing = smoothing\n        self.random_state = random_state\n        self.global_mean = None\n        self.encoding_maps = {}  # 存储每个类别特征的编码映射\n        \n    def fit_transform(self, X, y):\n        \"\"\"\n        对训练集进行K-Fold编码\n        \n        参数:\n            X: 特征数据框\n            y: 目标变量\n        \n        返回:\n            X_encoded: 编码后的数据框\n        \"\"\"\n        X_encoded = X.copy()\n        self.global_mean = y.mean()\n        \n        print(f\"\\n全局平均值: {self.global_mean:.2f}\")\n        print(f\"使用 {self.n_splits}-Fold 编码，平滑参数={self.smoothing}\")\n        \n        for col in self.columns:\n            print(f\"\\n编码特征: {col}\")\n            \n            # 为每个列创建新的编码列名\n            encoded_col = f'{col}_encoded'\n            X_encoded[encoded_col] = 0.0\n            \n            # K-Fold编码\n            kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.random_state)\n            \n            for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n                # 在训练集上计算统计值\n                train_stats = pd.DataFrame({\n                    'mean': y.iloc[train_idx].groupby(X[col].iloc[train_idx]).mean(),\n                    'count': y.iloc[train_idx].groupby(X[col].iloc[train_idx]).count()\n                })\n                \n                # 平滑处理（贝叶斯平滑）\n                smoothed_mean = (\n                    (train_stats['mean'] * train_stats['count'] + \n                     self.global_mean * self.smoothing) /\n                    (train_stats['count'] + self.smoothing)\n                )\n                \n                # 应用到验证集\n                X_encoded.loc[X.index[val_idx], encoded_col] = (\n                    X[col].iloc[val_idx].map(smoothed_mean).fillna(self.global_mean)\n                )\n            \n            # 保存完整训练集的编码映射（用于测试集）\n            full_stats = pd.DataFrame({\n                'mean': y.groupby(X[col]).mean(),\n                'count': y.groupby(X[col]).count()\n            })\n            \n            self.encoding_maps[col] = (\n                (full_stats['mean'] * full_stats['count'] + \n                 self.global_mean * self.smoothing) /\n                (full_stats['count'] + self.smoothing)\n            )\n            \n            # 显示编码结果\n            print(f\"  类别数量: {X[col].nunique()}\")\n            print(f\"  编码值范围: [{X_encoded[encoded_col].min():.2f}, {X_encoded[encoded_col].max():.2f}]\")\n            print(f\"  样本编码映射（前5个类别）:\")\n            for category in X[col].unique()[:5]:\n                mean_val = self.encoding_maps[col].get(category, self.global_mean)\n                print(f\"    {category}: {mean_val:.2f}\")\n        \n        return X_encoded\n    \n    def transform(self, X):\n        \"\"\"\n        对测试集进行编码\n        \n        参数:\n            X: 测试集特征数据框\n        \n        返回:\n            X_encoded: 编码后的数据框\n        \"\"\"\n        X_encoded = X.copy()\n        \n        for col in self.columns:\n            encoded_col = f'{col}_encoded'\n            # 使用训练集的编码映射\n            X_encoded[encoded_col] = (\n                X[col].map(self.encoding_maps[col]).fillna(self.global_mean)\n            )\n        \n        return X_encoded\n\n\n# ========================================\n# 应用 Target Encoding\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"应用 Target Encoding 到类别特征\")\nprint(\"=\"*60)\n\n# 加载数据\ntrain_fe = pd.read_csv('train_domain_features.csv')\ntest_fe = pd.read_csv('test_domain_features.csv')\n\n# 需要进行target encoding的类别特征\ncategorical_cols = ['sex', 'region']\n\nprint(f\"\\n待编码的类别特征: {categorical_cols}\")\n\n# 初始化编码器\nencoder = TargetEncoder(\n    columns=categorical_cols,\n    n_splits=5,\n    smoothing=10,  # 平滑参数（可以调整）\n    random_state=SEED\n)\n\n# 编码训练集（K-Fold方式）\nX_train = train_fe.drop(['charges', 'id'], axis=1, errors='ignore')\ny_train = train_fe['charges']\n\nX_train_encoded = encoder.fit_transform(X_train, y_train)\n\n# 编码测试集\nX_test = test_fe.drop(['id'], axis=1, errors='ignore')\nX_test_encoded = encoder.transform(X_test)\n\nprint(\"\\n✅ Target Encoding 完成!\")\nprint(f\"训练集编码后形状: {X_train_encoded.shape}\")\nprint(f\"测试集编码后形状: {X_test_encoded.shape}\")\n\n# 对比原始类别特征和编码后的特征\nprint(\"\\n📊 编码效果对比（以 region 为例）:\")\nprint(\"\\n原始 region 分布:\")\nprint(train_fe['region'].value_counts())\n\nprint(\"\\n编码后 region_encoded 的统计:\")\ncomparison = train_fe[['region', 'charges']].copy()\ncomparison['region_encoded'] = X_train_encoded['region_encoded']\nregion_comparison = comparison.groupby('region').agg({\n    'charges': ['mean', 'count'],\n    'region_encoded': 'mean'\n}).round(2)\nprint(region_comparison)\n\nprint(\"\\n💡 观察要点:\")\nprint(\"  1. region_encoded 的值接近该region的平均charges\")\nprint(\"  2. 但由于K-Fold和平滑，每个样本的编码值略有不同\")\nprint(\"  3. 这样既保留了类别信息，又避免了数据泄漏\")\n\n# 保存编码后的数据\ntrain_fe_encoded = train_fe[['id', 'charges']].copy()\ntrain_fe_encoded = pd.concat([train_fe_encoded, X_train_encoded], axis=1)\n\ntest_fe_encoded = test_fe[['id']].copy()\ntest_fe_encoded = pd.concat([test_fe_encoded, X_test_encoded], axis=1)\n\ntrain_fe_encoded.to_csv('train_target_encoded.csv', index=False)\ntest_fe_encoded.to_csv('test_target_encoded.csv', index=False)\n\nprint(\"\\n💾 已保存:\")\nprint(\"  - train_target_encoded.csv\")\nprint(\"  - test_target_encoded.csv\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "08qphbix49hu",
   "source": "## 📚 理论知识：分组统计特征\n\n### 什么是分组统计特征？\n\n分组统计特征（Aggregation Features）是指按某个或某几个类别分组后，对数值特征进行统计计算得到的特征。\n\n### 核心思想\n\n```\n不同的群体有不同的特征分布\n\n例如：\n- 吸烟者的平均BMI是多少？\n- 不同地区的平均年龄是多少？\n- 每个年龄段中吸烟者的比例是多少？\n```\n\n### 常用的统计量\n\n| 统计量 | 含义 | 适用场景 |\n|--------|------|----------|\n| **mean** | 平均值 | 捕捉中心趋势 |\n| **median** | 中位数 | 对异常值鲁棒 |\n| **std** | 标准差 | 衡量离散程度 |\n| **min/max** | 最小/最大值 | 捕捉极值信息 |\n| **count** | 数量 | 群体大小信息 |\n| **sum** | 总和 | 累积效应 |\n\n### 示例\n\n```python\n# 按 smoker 分组，计算 BMI 的统计量\ngroup_stats = df.groupby('smoker')['bmi'].agg(['mean', 'std', 'median'])\n\n# 将统计量映射回原数据\ndf['smoker_bmi_mean'] = df['smoker'].map(group_stats['mean'])\ndf['smoker_bmi_std'] = df['smoker'].map(group_stats['std'])\n```\n\n### 业务含义示例\n\n在我们的保险数据中：\n\n**按吸烟状态分组**：\n- `smoker_age_mean`: 吸烟者/非吸烟者的平均年龄\n  - 如果吸烟者普遍年轻，可能风险不同\n  \n- `smoker_bmi_mean`: 吸烟者/非吸烟者的平均BMI\n  - 吸烟与肥胖的相关性\n\n**按地区分组**：\n- `region_age_mean`: 该地区的平均年龄\n  - 反映地区人口结构\n  \n- `region_bmi_std`: 该地区BMI的标准差\n  - 反映地区健康状况的差异性\n\n**多维分组**：\n- `smoker_region_charges_mean`: 不同地区吸烟者的平均费用\n  - 捕捉地区×吸烟的交互效应\n\n### 与Target Encoding的区别\n\n| 特征类型 | 分组键 | 统计对象 | 用途 |\n|----------|--------|----------|------|\n| **Target Encoding** | 类别特征 | 目标变量 | 直接编码类别信息 |\n| **分组统计** | 类别特征 | 其他数值特征 | 捕捉群体特征分布 |\n\n两者可以互补！",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "udbmaso9ho9",
   "source": "# ========================================\n# Cell 7: 分组统计特征创建\n# ========================================\n\nprint(\"📊 创建分组统计特征\")\nprint(\"=\"*60)\n\ndef create_aggregation_features(df, is_train=True, train_stats=None):\n    \"\"\"\n    创建分组统计特征\n    \n    参数:\n        df: 数据框\n        is_train: 是否是训练集\n        train_stats: 训练集的统计字典（用于测试集）\n    \n    返回:\n        df: 添加统计特征后的数据框\n        stats_dict: 统计字典（仅训练集返回）\n    \"\"\"\n    df = df.copy()\n    stats_dict = {} if is_train else None\n    \n    # 确保smoker是数值型\n    if df['smoker'].dtype == 'object':\n        df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n    \n    # ========================================\n    # 1. 按 smoker 分组\n    # ========================================\n    print(\"\\n🎯 按 smoker 分组的统计特征...\")\n    \n    if is_train:\n        # 计算统计量\n        smoker_age_stats = df.groupby('smoker')['age'].agg(['mean', 'std', 'median']).add_prefix('smoker_age_')\n        smoker_bmi_stats = df.groupby('smoker')['bmi'].agg(['mean', 'std', 'median']).add_prefix('smoker_bmi_')\n        smoker_children_stats = df.groupby('smoker')['children'].agg(['mean', 'sum']).add_prefix('smoker_children_')\n        \n        stats_dict['smoker_age'] = smoker_age_stats\n        stats_dict['smoker_bmi'] = smoker_bmi_stats\n        stats_dict['smoker_children'] = smoker_children_stats\n    else:\n        smoker_age_stats = train_stats['smoker_age']\n        smoker_bmi_stats = train_stats['smoker_bmi']\n        smoker_children_stats = train_stats['smoker_children']\n    \n    # 映射到原数据\n    for col in smoker_age_stats.columns:\n        df[col] = df['smoker'].map(smoker_age_stats[col])\n    for col in smoker_bmi_stats.columns:\n        df[col] = df['smoker'].map(smoker_bmi_stats[col])\n    for col in smoker_children_stats.columns:\n        df[col] = df['smoker'].map(smoker_children_stats[col])\n    \n    print(f\"  - 按smoker分组: {len(smoker_age_stats.columns) + len(smoker_bmi_stats.columns) + len(smoker_children_stats.columns)} 个特征\")\n    \n    # ========================================\n    # 2. 按 region 分组\n    # ========================================\n    print(\"\\n🎯 按 region 分组的统计特征...\")\n    \n    if is_train:\n        region_age_stats = df.groupby('region')['age'].agg(['mean', 'std']).add_prefix('region_age_')\n        region_bmi_stats = df.groupby('region')['bmi'].agg(['mean', 'std']).add_prefix('region_bmi_')\n        region_smoker_stats = df.groupby('region')['smoker'].agg(['mean', 'sum']).add_prefix('region_smoker_')\n        \n        stats_dict['region_age'] = region_age_stats\n        stats_dict['region_bmi'] = region_bmi_stats\n        stats_dict['region_smoker'] = region_smoker_stats\n    else:\n        region_age_stats = train_stats['region_age']\n        region_bmi_stats = train_stats['region_bmi']\n        region_smoker_stats = train_stats['region_smoker']\n    \n    for col in region_age_stats.columns:\n        df[col] = df['region'].map(region_age_stats[col])\n    for col in region_bmi_stats.columns:\n        df[col] = df['region'].map(region_bmi_stats[col])\n    for col in region_smoker_stats.columns:\n        df[col] = df['region'].map(region_smoker_stats[col])\n    \n    print(f\"  - 按region分组: {len(region_age_stats.columns) + len(region_bmi_stats.columns) + len(region_smoker_stats.columns)} 个特征\")\n    \n    # ========================================\n    # 3. 按 age_group 分组（如果存在）\n    # ========================================\n    if 'age_group' in df.columns:\n        print(\"\\n🎯 按 age_group 分组的统计特征...\")\n        \n        if is_train:\n            age_group_bmi_stats = df.groupby('age_group')['bmi'].agg(['mean', 'std']).add_prefix('age_group_bmi_')\n            age_group_smoker_stats = df.groupby('age_group')['smoker'].mean().to_frame('age_group_smoker_rate')\n            \n            stats_dict['age_group_bmi'] = age_group_bmi_stats\n            stats_dict['age_group_smoker'] = age_group_smoker_stats\n        else:\n            age_group_bmi_stats = train_stats['age_group_bmi']\n            age_group_smoker_stats = train_stats['age_group_smoker']\n        \n        for col in age_group_bmi_stats.columns:\n            df[col] = df['age_group'].map(age_group_bmi_stats[col])\n        df['age_group_smoker_rate'] = df['age_group'].map(age_group_smoker_stats['age_group_smoker_rate'])\n        \n        print(f\"  - 按age_group分组: {len(age_group_bmi_stats.columns) + 1} 个特征\")\n    \n    # ========================================\n    # 4. 按 bmi_category 分组（如果存在）\n    # ========================================\n    if 'bmi_category' in df.columns:\n        print(\"\\n🎯 按 bmi_category 分组的统计特征...\")\n        \n        if is_train:\n            bmi_cat_age_stats = df.groupby('bmi_category')['age'].agg(['mean']).add_prefix('bmi_cat_age_')\n            bmi_cat_smoker_stats = df.groupby('bmi_category')['smoker'].mean().to_frame('bmi_cat_smoker_rate')\n            \n            stats_dict['bmi_cat_age'] = bmi_cat_age_stats\n            stats_dict['bmi_cat_smoker'] = bmi_cat_smoker_stats\n        else:\n            bmi_cat_age_stats = train_stats['bmi_cat_age']\n            bmi_cat_smoker_stats = train_stats['bmi_cat_smoker']\n        \n        for col in bmi_cat_age_stats.columns:\n            df[col] = df['bmi_category'].map(bmi_cat_age_stats[col])\n        df['bmi_cat_smoker_rate'] = df['bmi_category'].map(bmi_cat_smoker_stats['bmi_cat_smoker_rate'])\n        \n        print(f\"  - 按bmi_category分组: {len(bmi_cat_age_stats.columns) + 1} 个特征\")\n    \n    # ========================================\n    # 5. 多维分组（smoker × region）\n    # ========================================\n    print(\"\\n🎯 多维分组统计特征 (smoker × region)...\")\n    \n    if is_train:\n        smoker_region_age = df.groupby(['smoker', 'region'])['age'].mean().to_frame('smoker_region_age_mean')\n        smoker_region_bmi = df.groupby(['smoker', 'region'])['bmi'].mean().to_frame('smoker_region_bmi_mean')\n        \n        stats_dict['smoker_region_age'] = smoker_region_age\n        stats_dict['smoker_region_bmi'] = smoker_region_bmi\n    else:\n        smoker_region_age = train_stats['smoker_region_age']\n        smoker_region_bmi = train_stats['smoker_region_bmi']\n    \n    df['smoker_region_age_mean'] = df.set_index(['smoker', 'region']).index.map(smoker_region_age['smoker_region_age_mean'])\n    df['smoker_region_bmi_mean'] = df.set_index(['smoker', 'region']).index.map(smoker_region_bmi['smoker_region_bmi_mean'])\n    \n    print(f\"  - smoker × region: 2 个特征\")\n    \n    # ========================================\n    # 6. 相对特征（与群体均值的偏差）\n    # ========================================\n    print(\"\\n🎯 创建相对特征（偏差特征）...\")\n    \n    # 个体BMI与所在smoker群体平均BMI的偏差\n    df['bmi_vs_smoker_mean'] = df['bmi'] - df['smoker_bmi_mean']\n    \n    # 个体年龄与所在region群体平均年龄的偏差\n    df['age_vs_region_mean'] = df['age'] - df['region_age_mean']\n    \n    # 个体年龄与所在age_group平均BMI的偏差\n    if 'age_group_bmi_mean' in df.columns:\n        df['bmi_vs_age_group_mean'] = df['bmi'] - df['age_group_bmi_mean']\n    \n    print(f\"  - 相对特征: 3 个\")\n    \n    if is_train:\n        return df, stats_dict\n    else:\n        return df\n\n# ========================================\n# 应用分组统计特征\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"应用分组统计特征到数据集\")\nprint(\"=\"*60)\n\n# 加载数据\ntrain_data = pd.read_csv('train_target_encoded.csv')\ntest_data = pd.read_csv('test_target_encoded.csv')\n\nprint(f\"\\n处理前形状:\")\nprint(f\"  训练集: {train_data.shape}\")\nprint(f\"  测试集: {test_data.shape}\")\n\n# 对训练集创建分组统计特征\ntrain_with_agg, train_stats = create_aggregation_features(train_data, is_train=True)\n\n# 对测试集应用相同的统计\ntest_with_agg = create_aggregation_features(test_data, is_train=False, train_stats=train_stats)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"✅ 分组统计特征创建完成!\")\nprint(\"=\"*60)\nprint(f\"\\n处理后形状:\")\nprint(f\"  训练集: {train_with_agg.shape}\")\nprint(f\"  测试集: {test_with_agg.shape}\")\n\nnew_features = train_with_agg.shape[1] - train_data.shape[1]\nprint(f\"\\n新增特征数: {new_features}\")\n\n# 查看部分新特征\nprint(\"\\n📋 部分分组统计特征示例:\")\nagg_features = [col for col in train_with_agg.columns if any(x in col for x in ['_mean', '_std', '_rate', '_vs_'])]\nprint(f\"\\n前15个分组统计特征:\")\nfor i, col in enumerate(agg_features[:15], 1):\n    print(f\"  {i:2d}. {col}\")\n\n# 查看统计信息\nprint(\"\\n📊 部分特征统计:\")\nsample_cols = ['smoker_bmi_mean', 'region_age_mean', 'bmi_vs_smoker_mean']\nprint(train_with_agg[sample_cols].describe().round(2))\n\n# 保存\ntrain_with_agg.to_csv('train_all_features.csv', index=False)\ntest_with_agg.to_csv('test_all_features.csv', index=False)\n\nprint(\"\\n💾 已保存:\")\nprint(\"  - train_all_features.csv (包含所有特征)\")\nprint(\"  - test_all_features.csv (包含所有特征)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a9c97ozjkq",
   "source": "# ========================================\n# Cell 8: 特征工程效果对比实验\n# ========================================\n\nprint(\"🔬 特征工程效果对比实验\")\nprint(\"=\"*60)\n\ndef prepare_and_evaluate(data_path, experiment_name, feature_set_description):\n    \"\"\"\n    准备数据并快速评估\n    \n    参数:\n        data_path: 数据文件路径\n        experiment_name: 实验名称\n        feature_set_description: 特征集描述\n    \n    返回:\n        oof_rmse: OOF RMSE\n        n_features: 特征数量\n    \"\"\"\n    # 加载数据\n    df = pd.read_csv(data_path)\n    \n    # 准备特征\n    # 排除不用于建模的列\n    exclude_cols = ['id', 'charges']\n    \n    # 处理类别特征（One-Hot编码）\n    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n    if cat_cols:\n        df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n    \n    # 分离特征和目标\n    X = df.drop(exclude_cols, axis=1, errors='ignore')\n    y = df['charges']\n    \n    n_features = X.shape[1]\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"实验: {experiment_name}\")\n    print(f\"{'='*60}\")\n    print(f\"特征集: {feature_set_description}\")\n    print(f\"特征数量: {n_features}\")\n    print(f\"样本数量: {len(df)}\")\n    \n    # 5折交叉验证\n    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    oof_predictions = np.zeros(len(X))\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # 训练LightGBM\n        model = lgb.LGBMRegressor(\n            n_estimators=500,\n            learning_rate=0.05,\n            num_leaves=31,\n            colsample_bytree=0.8,\n            subsample=0.8,\n            random_state=SEED,\n            verbose=-1\n        )\n        \n        model.fit(\n            X_tr, np.log1p(y_tr),\n            eval_set=[(X_val, np.log1p(y_val))],\n            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n        )\n        \n        # 预测\n        pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n        oof_predictions[val_idx] = pred\n        \n        # 计算fold RMSE\n        fold_rmse = np.sqrt(mean_squared_error(y_val, pred))\n        fold_scores.append(fold_rmse)\n        print(f\"  Fold {fold}: RMSE = {fold_rmse:.2f}\")\n    \n    # 总体OOF RMSE\n    oof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n    oof_r2 = r2_score(y, oof_predictions)\n    \n    print(f\"\\n{'─'*60}\")\n    print(f\"OOF RMSE: {oof_rmse:.2f}\")\n    print(f\"OOF R²:   {oof_r2:.4f}\")\n    print(f\"Fold RMSE 标准差: {np.std(fold_scores):.2f}\")\n    print(f\"{'─'*60}\")\n    \n    return oof_rmse, n_features, oof_r2\n\n# ========================================\n# 对比实验\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"开始对比实验\")\nprint(\"=\"*60)\nprint(\"\\n我们将对比以下特征集的效果:\")\nprint(\"  1. 基线: 仅原始特征 + 简单编码\")\nprint(\"  2. +领域特征: 添加风险评分等领域知识特征\")\nprint(\"  3. +Target Encoding: 添加Target Encoding\")\nprint(\"  4. +分组统计: 添加分组统计特征（完整特征集）\")\n\nresults = []\n\n# 实验1: 基线（使用清洗后的数据，仅基础特征）\nprint(\"\\n\\n\" + \"🔸\"*30)\nprint(\"实验 1/4: 基线（原始特征）\")\nprint(\"🔸\"*30)\n\nrmse_1, n_feat_1, r2_1 = prepare_and_evaluate(\n    'train_cleaned.csv',\n    '基线',\n    '仅原始特征: age, bmi, children, smoker, sex, region'\n)\n\nresults.append({\n    '实验': '1. 基线',\n    '特征数': n_feat_1,\n    'OOF RMSE': rmse_1,\n    'OOF R²': r2_1,\n    'vs 基线': 0,\n    '说明': '原始特征+简单编码'\n})\n\n# 实验2: 添加领域特征\nprint(\"\\n\\n\" + \"🔸\"*30)\nprint(\"实验 2/4: +领域知识特征\")\nprint(\"🔸\"*30)\n\nrmse_2, n_feat_2, r2_2 = prepare_and_evaluate(\n    'train_domain_features.csv',\n    '+领域特征',\n    '原始特征 + 风险评分 + 交互特征 + 多项式特征'\n)\n\nresults.append({\n    '实验': '2. +领域特征',\n    '特征数': n_feat_2,\n    'OOF RMSE': rmse_2,\n    'OOF R²': r2_2,\n    'vs 基线': rmse_2 - rmse_1,\n    '说明': f'新增{n_feat_2 - n_feat_1}个特征'\n})\n\n# 实验3: 添加Target Encoding\nprint(\"\\n\\n\" + \"🔸\"*30)\nprint(\"实验 3/4: +Target Encoding\")\nprint(\"🔸\"*30)\n\nrmse_3, n_feat_3, r2_3 = prepare_and_evaluate(\n    'train_target_encoded.csv',\n    '+Target Encoding',\n    '领域特征 + Target Encoding (sex, region)'\n)\n\nresults.append({\n    '实验': '3. +Target Encoding',\n    '特征数': n_feat_3,\n    'OOF RMSE': rmse_3,\n    'OOF R²': r2_3,\n    'vs 基线': rmse_3 - rmse_1,\n    '说明': f'新增{n_feat_3 - n_feat_2}个编码特征'\n})\n\n# 实验4: 完整特征集（添加分组统计）\nprint(\"\\n\\n\" + \"🔸\"*30)\nprint(\"实验 4/4: 完整特征集\")\nprint(\"🔸\"*30)\n\nrmse_4, n_feat_4, r2_4 = prepare_and_evaluate(\n    'train_all_features.csv',\n    '完整特征集',\n    '领域特征 + Target Encoding + 分组统计特征'\n)\n\nresults.append({\n    '实验': '4. 完整特征集',\n    '特征数': n_feat_4,\n    'OOF RMSE': rmse_4,\n    'OOF R²': r2_4,\n    'vs 基线': rmse_4 - rmse_1,\n    '说明': f'新增{n_feat_4 - n_feat_3}个统计特征'\n})\n\n# ========================================\n# 结果汇总\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"📊 实验结果汇总\")\nprint(\"=\"*60)\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n\" + results_df.to_string(index=False))\n\n# 找出最佳结果\nbest_idx = results_df['OOF RMSE'].idxmin()\nbest_result = results_df.loc[best_idx]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"🏆 最佳结果\")\nprint(\"=\"*60)\nprint(f\"实验: {best_result['实验']}\")\nprint(f\"特征数: {best_result['特征数']}\")\nprint(f\"OOF RMSE: {best_result['OOF RMSE']:.2f}\")\nprint(f\"OOF R²: {best_result['OOF R²']:.4f}\")\nprint(f\"相比基线提升: {-best_result['vs 基线']:.2f} RMSE ({-best_result['vs 基线']/rmse_1*100:.1f}%)\")\n\n# 可视化对比\nprint(\"\\n📊 可视化对比...\")\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 图1: RMSE对比\nax1 = axes[0]\ncolors = ['red' if i == best_idx else 'skyblue' for i in range(len(results_df))]\nbars = ax1.bar(range(len(results_df)), results_df['OOF RMSE'], color=colors, edgecolor='black')\nax1.set_xticks(range(len(results_df)))\nax1.set_xticklabels([s.split('.')[1].strip() for s in results_df['实验']], rotation=15, ha='right')\nax1.set_ylabel('OOF RMSE')\nax1.set_title('特征工程效果对比 - RMSE')\nax1.axhline(rmse_1, color='red', linestyle='--', alpha=0.5, label='基线')\n\n# 添加数值标签\nfor bar, rmse in zip(bars, results_df['OOF RMSE']):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n            f'{rmse:.0f}',\n            ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nax1.legend()\nax1.grid(axis='y', alpha=0.3)\n\n# 图2: R²对比\nax2 = axes[1]\nbars2 = ax2.bar(range(len(results_df)), results_df['OOF R²'], color=colors, edgecolor='black')\nax2.set_xticks(range(len(results_df)))\nax2.set_xticklabels([s.split('.')[1].strip() for s in results_df['实验']], rotation=15, ha='right')\nax2.set_ylabel('OOF R²')\nax2.set_title('特征工程效果对比 - R²')\nax2.set_ylim([0, 1])\n\n# 添加数值标签\nfor bar, r2 in zip(bars2, results_df['OOF R²']):\n    height = bar.get_height()\n    ax2.text(bar.get_x() + bar.get_width()/2., height,\n            f'{r2:.3f}',\n            ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✅ 第二部分完成！\")\nprint(\"=\"*60)\nprint(\"\\n🎓 第二部分知识总结:\")\nprint(\"  1. ✅ 创建了28个领域知识特征（风险评分、交互、多项式）\")\nprint(\"  2. ✅ 实现了K-Fold Target Encoding（防止数据泄漏）\")\nprint(\"  3. ✅ 创建了20+个分组统计特征\")\nprint(\"  4. ✅ 通过实验验证了每一步的效果\")\nprint(f\"  5. ✅ 最终RMSE提升: {-best_result['vs 基线']:.0f} ({-best_result['vs 基线']/rmse_1*100:.1f}%)\")\nprint(\"\\n💡 特征工程是机器学习中最有价值的环节！\")\nprint(\"\\n下一步: 超参数优化 → 预期再降低100-200 RMSE！\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mjinp090mup",
   "source": "---\n\n# 第三部分：超参数优化\n\n## 🎯 学习目标\n1. 理解超参数与模型参数的区别\n2. 掌握三种调参策略（Grid Search, Random Search, Bayesian Optimization）\n3. 学习Optuna的使用方法\n4. 理解TPE采样器的工作原理\n5. 学会分析参数重要性\n\n## 📚 理论知识：超参数优化\n\n### 什么是超参数？\n\n**模型参数 vs 超参数**\n\n| 类型 | 定义 | 示例 | 如何获得 |\n|------|------|------|----------|\n| **模型参数** | 模型从数据中学习得到的参数 | 线性回归的权重、决策树的分裂点 | 通过训练自动学习 |\n| **超参数** | 模型训练前需要手动设置的参数 | 学习率、树的深度、正则化系数 | 需要手动调整或自动搜索 |\n\n### 为什么超参数重要？\n\n```\n同样的数据 + 不同的超参数 → 可能相差几百RMSE！\n\n例如：LightGBM\n- 默认参数：RMSE = 6500\n- 优化后参数：RMSE = 6300\n- 差异：200 RMSE （3%的提升）\n```\n\n### 三种调参策略对比\n\n#### 1️⃣ Grid Search（网格搜索）\n\n```python\n# 穷举所有组合\nparams = {\n    'learning_rate': [0.01, 0.05, 0.1],\n    'num_leaves': [31, 50, 100],\n    'max_depth': [5, 7, 10]\n}\n# 总共需要测试：3 × 3 × 3 = 27 种组合\n```\n\n**优点**：\n- 简单易懂\n- 保证找到网格内的最优组合\n\n**缺点**：\n- ❌ 计算成本指数增长（维度诅咒）\n- ❌ 浪费计算资源在不重要的参数上\n- ❌ 对连续参数不友好\n\n#### 2️⃣ Random Search（随机搜索）\n\n```python\n# 随机采样\nfor i in range(100):\n    learning_rate = uniform(0.01, 0.3)\n    num_leaves = randint(20, 150)\n    # ... 训练并评估\n```\n\n**优点**：\n- ✅ 比Grid Search快很多\n- ✅ 更容易探索参数空间\n- ✅ 对不重要的参数不敏感\n\n**缺点**：\n- ❌ 仍然是盲目搜索\n- ❌ 不利用之前的评估结果\n\n#### 3️⃣ Bayesian Optimization（贝叶斯优化）⭐\n\n```\n核心思想：利用之前的评估结果，智能地选择下一个要测试的参数组合\n\n工作流程：\n1. 随机测试几组参数\n2. 根据结果，建立\"参数→性能\"的概率模型\n3. 用这个模型预测：哪组参数最有可能更好？\n4. 测试该参数，更新模型\n5. 重复步骤3-4\n```\n\n**优点**：\n- ✅ 高效：通常50-100次评估就够了\n- ✅ 智能：利用历史信息\n- ✅ 适合黑盒优化\n- ✅ 自动平衡探索(exploration)和利用(exploitation)\n\n**缺点**：\n- 稍微复杂一些\n- 需要理解采样策略\n\n### Optuna 框架\n\nOptuna 是目前最流行的贝叶斯优化库之一。\n\n**核心组件**：\n\n1. **Study**：一次完整的优化实验\n2. **Trial**：一次参数组合的评估\n3. **Objective**：目标函数（我们要最小化/最大化的指标）\n4. **Sampler**：采样器（决定如何选择下一组参数）\n\n**常用采样器**：\n\n- **TPE** (Tree-structured Parzen Estimator)：默认，适合大多数情况 ⭐\n- **CMA-ES**：适合连续参数空间\n- **Random**：随机搜索\n- **Grid**：网格搜索\n\n### TPE (Tree-structured Parzen Estimator)\n\nTPE是Optuna的默认采样器，效果很好。\n\n**核心思想**：\n\n```\n将参数分为两组：\n- l(x)：表现好的参数分布（loss < 某阈值）\n- g(x)：表现不好的参数分布\n\n选择下一个参数：\n- 最大化 l(x) / g(x) 的值\n- 即：在\"好参数\"区域采样的概率高，在\"坏参数\"区域采样的概率低\n```\n\n**简单理解**：\n- 维护两个模型：\"好参数在哪里\" 和 \"坏参数在哪里\"\n- 下次优先测试\"可能是好参数\"的地方\n\n接下来我们会用Optuna优化LightGBM！",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "ubp9rbfdsek",
   "source": "# ========================================\n# Cell 9: Optuna 超参数优化实现\n# ========================================\n\nprint(\"🔧 使用 Optuna 进行超参数优化\")\nprint(\"=\"*60)\n\n# 检查optuna是否安装\ntry:\n    import optuna\n    from optuna.visualization import plot_optimization_history, plot_param_importances\n    print(\"✅ Optuna 已安装\")\nexcept ImportError:\n    print(\"❌ Optuna 未安装\")\n    print(\"请运行: pip install optuna\")\n    print(\"暂时跳过此部分\")\n\n# 加载完整特征集数据\nprint(\"\\n📂 加载数据...\")\ntrain_data = pd.read_csv('train_all_features.csv')\n\n# 准备数据\nexclude_cols = ['id', 'charges']\ncat_cols = train_data.select_dtypes(include=['object', 'category']).columns.tolist()\nif cat_cols:\n    train_data = pd.get_dummies(train_data, columns=cat_cols, drop_first=True)\n\nX = train_data.drop(exclude_cols, axis=1, errors='ignore')\ny = train_data['charges']\n\nprint(f\"特征数量: {X.shape[1]}\")\nprint(f\"样本数量: {len(X)}\")\n\n# ========================================\n# 定义Optuna目标函数\n# ========================================\n\ndef objective(trial):\n    \"\"\"\n    Optuna目标函数\n    \n    参数:\n        trial: Optuna trial对象\n    \n    返回:\n        平均RMSE（越小越好）\n    \"\"\"\n    \n    # ========================================\n    # 定义搜索空间\n    # ========================================\n    \n    params = {\n        # 基本参数\n        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n        \n        # 树结构参数\n        'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n        'max_depth': trial.suggest_int('max_depth', 3, 12),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'min_child_weight': trial.suggest_float('min_child_weight', 1e-3, 10.0, log=True),\n        \n        # 采样参数（防止过拟合）\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'subsample_freq': trial.suggest_int('subsample_freq', 0, 7),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        \n        # 正则化参数\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n        \n        # 固定参数\n        'random_state': SEED,\n        'verbose': -1\n    }\n    \n    # ========================================\n    # 交叉验证评估\n    # ========================================\n    \n    kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    fold_scores = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # 训练模型\n        model = lgb.LGBMRegressor(**params)\n        \n        model.fit(\n            X_tr, np.log1p(y_tr),\n            eval_set=[(X_val, np.log1p(y_val))],\n            callbacks=[\n                lgb.early_stopping(stopping_rounds=50, verbose=False),\n                lgb.log_evaluation(period=0)  # 禁止输出\n            ]\n        )\n        \n        # 预测\n        pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n        \n        # 计算RMSE\n        fold_rmse = np.sqrt(mean_squared_error(y_val, pred))\n        fold_scores.append(fold_rmse)\n    \n    # 返回平均RMSE\n    mean_rmse = np.mean(fold_scores)\n    \n    return mean_rmse\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"开始优化（这可能需要几分钟）\")\nprint(\"=\"*60)\n\n# ========================================\n# 运行优化\n# ========================================\n\n# 创建study\nstudy = optuna.create_study(\n    direction='minimize',  # 最小化RMSE\n    sampler=optuna.samplers.TPESampler(seed=SEED),  # 使用TPE采样器\n    study_name='lgb_optimization'\n)\n\n# 运行优化（调整n_trials可以控制优化时间）\n# 50 trials约需要5-10分钟，100 trials约需要10-20分钟\nN_TRIALS = 50  # 可以调整这个值\n\nprint(f\"\\n🚀 开始 {N_TRIALS} 次优化...\")\nprint(\"进度条会显示当前进度\\n\")\n\nstudy.optimize(\n    objective,\n    n_trials=N_TRIALS,\n    show_progress_bar=True,\n    n_jobs=1  # 串行执行（避免内存问题）\n)\n\nprint(\"\\n✅ 优化完成！\")\nprint(\"=\"*60)\n\n# ========================================\n# 输出结果\n# ========================================\n\nprint(\"\\n📊 优化结果:\")\nprint(\"-\"*60)\nprint(f\"最佳RMSE: {study.best_value:.2f}\")\nprint(f\"最佳参数:\")\nfor param, value in study.best_params.items():\n    print(f\"  {param:20s}: {value}\")\n\nprint(f\"\\n总共尝试: {len(study.trials)} 组参数\")\nprint(f\"完成的trials: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}\")\n\n# 保存最佳参数\nimport json\nwith open('best_params_lgb.json', 'w') as f:\n    json.dump(study.best_params, f, indent=2)\nprint(\"\\n💾 最佳参数已保存到: best_params_lgb.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aege4obo9cr",
   "source": "# ========================================\n# Cell 10: 优化结果分析与可视化\n# ========================================\n\nprint(\"📊 优化结果分析\")\nprint(\"=\"*60)\n\n# ========================================\n# 1. 优化历史可视化\n# ========================================\n\nprint(\"\\n1️⃣ 优化历史\")\nprint(\"-\"*60)\n\n# 获取所有trial的值\ntrial_values = [t.value for t in study.trials if t.value is not None]\ntrial_numbers = list(range(1, len(trial_values) + 1))\n\n# 计算累积最佳值\ncumulative_best = []\ncurrent_best = float('inf')\nfor val in trial_values:\n    if val < current_best:\n        current_best = val\n    cumulative_best.append(current_best)\n\n# 可视化\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 左图：每次trial的RMSE\nax1 = axes[0]\nax1.plot(trial_numbers, trial_values, 'o-', alpha=0.6, markersize=4, label='Trial RMSE')\nax1.plot(trial_numbers, cumulative_best, 'r-', linewidth=2, label='Best RMSE')\nax1.set_xlabel('Trial Number')\nax1.set_ylabel('RMSE')\nax1.set_title('优化历史 - RMSE随Trial变化')\nax1.legend()\nax1.grid(alpha=0.3)\n\n# 右图：前20个trials详细对比\nax2 = axes[1]\nn_show = min(20, len(trial_values))\nax2.bar(range(1, n_show+1), trial_values[:n_show], alpha=0.7)\nbest_idx = trial_values.index(min(trial_values[:n_show])) + 1\nax2.axhline(study.best_value, color='red', linestyle='--', label=f'Best: {study.best_value:.2f}')\nax2.set_xlabel('Trial Number')\nax2.set_ylabel('RMSE')\nax2.set_title(f'前{n_show}个Trials的RMSE')\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\n💡 观察要点:\")\nprint(f\"  - 最佳RMSE在第 {trial_values.index(study.best_value)+1} 次trial出现\")\nprint(f\"  - 前10次trials的平均RMSE: {np.mean(trial_values[:10]):.2f}\")\nprint(f\"  - 后10次trials的平均RMSE: {np.mean(trial_values[-10:]):.2f}\")\nprint(f\"  - RMSE改善: {np.mean(trial_values[:10]) - study.best_value:.2f}\")\n\n# ========================================\n# 2. 参数重要性分析\n# ========================================\n\nprint(\"\\n\\n2️⃣ 参数重要性分析\")\nprint(\"-\"*60)\n\n# 计算参数重要性\ntry:\n    importance = optuna.importance.get_param_importances(study)\n    \n    print(\"\\n参数重要性排序（越大越重要）:\")\n    for i, (param, imp) in enumerate(sorted(importance.items(), key=lambda x: x[1], reverse=True), 1):\n        bar = '█' * int(imp * 50)\n        print(f\"  {i:2d}. {param:20s}: {imp:6.4f} {bar}\")\n    \n    # 可视化\n    fig, ax = plt.subplots(figsize=(10, 6))\n    params = list(importance.keys())\n    values = list(importance.values())\n    \n    # 按重要性排序\n    sorted_idx = np.argsort(values)[::-1]\n    params_sorted = [params[i] for i in sorted_idx]\n    values_sorted = [values[i] for i in sorted_idx]\n    \n    bars = ax.barh(params_sorted, values_sorted, color='skyblue', edgecolor='black')\n    \n    # 高亮前3个最重要的参数\n    for i in range(min(3, len(bars))):\n        bars[i].set_color('coral')\n    \n    ax.set_xlabel('Importance')\n    ax.set_title('参数重要性分析')\n    ax.grid(axis='x', alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    \n    print(\"\\n💡 解读:\")\n    top3 = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:3]\n    print(f\"  - 最重要的3个参数: {', '.join([p[0] for p in top3])}\")\n    print(f\"  - 这些参数对模型性能影响最大，需要重点调整\")\n    \nexcept Exception as e:\n    print(f\"参数重要性计算失败: {e}\")\n\n# ========================================\n# 3. 对比默认参数 vs 优化参数\n# ========================================\n\nprint(\"\\n\\n3️⃣ 默认参数 vs 优化参数对比\")\nprint(\"-\"*60)\n\n# 使用默认参数评估\nprint(\"\\n测试默认参数...\")\ndefault_params = {\n    'n_estimators': 100,\n    'learning_rate': 0.1,\n    'num_leaves': 31,\n    'random_state': SEED,\n    'verbose': -1\n}\n\nkf = KFold(n_splits=5, shuffle=True, random_state=SEED)\ndefault_scores = []\n\nfor train_idx, val_idx in kf.split(X):\n    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgb.LGBMRegressor(**default_params)\n    model.fit(X_tr, np.log1p(y_tr), \n              eval_set=[(X_val, np.log1p(y_val))],\n              callbacks=[lgb.early_stopping(50, verbose=False)])\n    \n    pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n    rmse = np.sqrt(mean_squared_error(y_val, pred))\n    default_scores.append(rmse)\n\ndefault_rmse = np.mean(default_scores)\n\nprint(\"\\n📊 对比结果:\")\nprint(\"-\"*60)\nprint(f\"{'参数设置':<20s} {'RMSE':>10s} {'提升':>15s}\")\nprint(\"-\"*60)\nprint(f\"{'默认参数':<20s} {default_rmse:>10.2f} {'-':>15s}\")\nprint(f\"{'Optuna优化':<20s} {study.best_value:>10.2f} {f'-{default_rmse - study.best_value:.2f} ({(default_rmse - study.best_value)/default_rmse*100:.1f}%)':>15s}\")\nprint(\"-\"*60)\n\n# 可视化对比\nfig, ax = plt.subplots(figsize=(8, 5))\nmethods = ['默认参数', 'Optuna优化']\nrmse_values = [default_rmse, study.best_value]\ncolors = ['lightcoral', 'lightgreen']\n\nbars = ax.bar(methods, rmse_values, color=colors, edgecolor='black', width=0.6)\nax.set_ylabel('RMSE')\nax.set_title('参数优化效果对比')\n\n# 添加数值标签\nfor bar, val in zip(bars, rmse_values):\n    height = bar.get_height()\n    ax.text(bar.get_x() + bar.get_width()/2., height,\n            f'{val:.2f}',\n            ha='center', va='bottom', fontsize=12, fontweight='bold')\n\n# 添加提升标注\nimprovement = default_rmse - study.best_value\nax.annotate(f'提升: {improvement:.2f}\\n({improvement/default_rmse*100:.1f}%)',\n            xy=(0.5, (default_rmse + study.best_value)/2),\n            xytext=(0.5, (default_rmse + study.best_value)/2 + 200),\n            ha='center',\n            fontsize=11,\n            bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n            arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n\nax.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n✅ 第三部分完成！\")\nprint(\"=\"*60)\nprint(\"\\n🎓 第三部分知识总结:\")\nprint(\"  1. ✅ 理解了超参数vs模型参数的区别\")\nprint(\"  2. ✅ 掌握了三种调参策略的优缺点\")\nprint(\"  3. ✅ 学会了使用Optuna进行贝叶斯优化\")\nprint(\"  4. ✅ 学会了分析参数重要性\")\nprint(f\"  5. ✅ RMSE提升: {improvement:.0f} ({improvement/default_rmse*100:.1f}%)\")\nprint(\"\\n💡 超参数优化能带来稳定的性能提升！\")\nprint(\"\\n下一步: 模型融合 → 预期再降低200-300 RMSE！\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "25fzd25q7dc",
   "source": "---\n\n# 第四部分：模型融合（Ensemble）⭐⭐⭐\n\n## 🎯 学习目标\n1. 理解\"三个臭皮匠赛过诸葛亮\"的数学原理\n2. 掌握多种模型融合策略\n3. 实现完整的Stacking流程\n4. 学会评估融合效果\n\n## 📚 理论知识：Ensemble思想\n\n### 为什么模型融合有效？\n\n**核心原理**：不同模型会犯不同的错误，融合可以互补\n\n```\n假设有3个模型，每个准确率80%：\n- 单个模型：准确率 = 80%\n- 如果错误独立，3个模型投票：准确率 ≈ 90%+\n\n关键：模型要有多样性（diversity）\n```\n\n### 模型多样性的来源\n\n1. **不同算法**：LightGBM, XGBoost, CatBoost, Ridge\n2. **不同特征**：使用不同的特征子集\n3. **不同参数**：同一算法，不同超参数\n4. **不同随机种子**：训练过程的随机性\n5. **不同数据采样**：Bagging思想\n\n### 融合策略对比\n\n| 策略 | 复杂度 | 效果 | 适用场景 |\n|------|--------|------|----------|\n| **Simple Average** | ⭐ | ⭐⭐ | 快速尝试 |\n| **Weighted Average** | ⭐⭐ | ⭐⭐⭐ | 模型性能差异大 |\n| **Stacking** | ⭐⭐⭐ | ⭐⭐⭐⭐ | 追求极致性能 |\n| **Blending** | ⭐⭐ | ⭐⭐⭐ | 数据量大时 |\n\n### Stacking原理\n\n**两层模型架构**：\n\n```\nLevel 0（基模型）:\n  ├── LightGBM  → 预测1\n  ├── XGBoost   → 预测2\n  ├── CatBoost  → 预测3\n  └── Ridge     → 预测4\n        ↓\nLevel 1（Meta模型）:\n  将Level 0的4个预测作为特征\n  训练一个新模型（通常是线性模型）\n        ↓\n     最终预测\n```\n\n**关键技巧**：Out-of-Fold预测\n\n```python\n# Level 0模型在训练集上做OOF预测\n# 确保Meta模型的训练数据是\"未见过\"的预测\nfor fold in KFold:\n    train_idx, val_idx = fold\n    model.fit(X[train_idx], y[train_idx])\n    oof_pred[val_idx] = model.predict(X[val_idx])  # OOF预测\n    test_pred += model.predict(X_test) / n_folds   # 测试集预测\n\n# Meta模型用OOF预测作为特征\nmeta_model.fit(oof_pred, y)\n```\n\n接下来我们会实现完整的Stacking流程！",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f76wd6zt71o",
   "source": "# ========================================\n# Cell 11: 完整模型融合实现 + 最终提交\n# ========================================\n\nprint(\"🎭 模型融合与最终提交\")\nprint(\"=\"*60)\n\n# 加载最佳参数\nimport json\ntry:\n    with open('best_params_lgb.json', 'r') as f:\n        best_lgb_params = json.load(f)\n    print(\"✅ 已加载Optuna优化的最佳参数\")\nexcept:\n    # 如果没有运行Optuna，使用默认优化参数\n    best_lgb_params = {\n        'n_estimators': 500,\n        'learning_rate': 0.05,\n        'num_leaves': 50,\n        'max_depth': 7,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'reg_alpha': 0.1,\n        'reg_lambda': 0.1,\n        'random_state': SEED,\n        'verbose': -1\n    }\n    print(\"⚠️  使用默认优化参数\")\n\n# ========================================\n# 准备数据\n# ========================================\n\nprint(\"\\n📂 准备数据...\")\ntrain_full = pd.read_csv('train_all_features.csv')\ntest_full = pd.read_csv('test_all_features.csv')\n\n# 保存测试集ID\ntest_ids = test_full['id'].values\n\n# 准备特征\nexclude_cols = ['id', 'charges']\ncat_cols = train_full.select_dtypes(include=['object', 'category']).columns.tolist()\n\nif cat_cols:\n    train_full = pd.get_dummies(train_full, columns=cat_cols, drop_first=True)\n    test_full = pd.get_dummies(test_full, columns=cat_cols, drop_first=True)\n\n# 对齐特征\ntrain_full, test_full = train_full.align(test_full, join='left', axis=1, fill_value=0)\n\nX_full = train_full.drop(exclude_cols, axis=1, errors='ignore')\ny_full = train_full['charges']\nX_test_full = test_full.drop(exclude_cols, axis=1, errors='ignore')\n\nprint(f\"训练集: {X_full.shape}\")\nprint(f\"测试集: {X_test_full.shape}\")\n\n# ========================================\n# 模型融合：Stacking\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"🎯 训练融合模型（Stacking）\")\nprint(\"=\"*60)\n\nN_FOLDS = 5\nkf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\n# 存储OOF预测和测试集预测\noof_predictions = {\n    'lgb': np.zeros(len(X_full)),\n    'ridge': np.zeros(len(X_full))\n}\n\ntest_predictions = {\n    'lgb': np.zeros(len(X_test_full)),\n    'ridge': np.zeros(len(X_test_full))\n}\n\n# ========================================\n# Level 0 模型1: LightGBM\n# ========================================\n\nprint(\"\\n1️⃣ 训练 LightGBM (Level 0)...\")\nprint(\"-\"*60)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n    print(f\"  Fold {fold}/{N_FOLDS}...\", end=' ')\n    \n    X_tr, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n    y_tr, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n    \n    # 训练LightGBM\n    lgb_model = lgb.LGBMRegressor(**best_lgb_params)\n    lgb_model.fit(\n        X_tr, np.log1p(y_tr),\n        eval_set=[(X_val, np.log1p(y_val))],\n        callbacks=[lgb.early_stopping(50, verbose=False)]\n    )\n    \n    # OOF预测\n    oof_predictions['lgb'][val_idx] = np.expm1(\n        lgb_model.predict(X_val, num_iteration=lgb_model.best_iteration_)\n    )\n    \n    # 测试集预测（累加）\n    test_predictions['lgb'] += np.expm1(\n        lgb_model.predict(X_test_full, num_iteration=lgb_model.best_iteration_)\n    ) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, oof_predictions['lgb'][val_idx]))\n    print(f\"RMSE: {fold_rmse:.2f}\")\n\nlgb_oof_rmse = np.sqrt(mean_squared_error(y_full, oof_predictions['lgb']))\nprint(f\"\\nLightGBM OOF RMSE: {lgb_oof_rmse:.2f}\")\n\n# ========================================\n# Level 0 模型2: Ridge（线性模型作为补充）\n# ========================================\n\nprint(\"\\n2️⃣ 训练 Ridge (Level 0)...\")\nprint(\"-\"*60)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_full), 1):\n    print(f\"  Fold {fold}/{N_FOLDS}...\", end=' ')\n    \n    X_tr, X_val = X_full.iloc[train_idx], X_full.iloc[val_idx]\n    y_tr, y_val = y_full.iloc[train_idx], y_full.iloc[val_idx]\n    \n    # 标准化\n    scaler = StandardScaler()\n    X_tr_scaled = scaler.fit_transform(X_tr)\n    X_val_scaled = scaler.transform(X_val)\n    X_test_scaled = scaler.transform(X_test_full)\n    \n    # 训练Ridge\n    ridge_model = Ridge(alpha=10.0, random_state=SEED)\n    ridge_model.fit(X_tr_scaled, np.log1p(y_tr))\n    \n    # OOF预测\n    oof_predictions['ridge'][val_idx] = np.expm1(ridge_model.predict(X_val_scaled))\n    \n    # 测试集预测\n    test_predictions['ridge'] += np.expm1(ridge_model.predict(X_test_scaled)) / N_FOLDS\n    \n    fold_rmse = np.sqrt(mean_squared_error(y_val, oof_predictions['ridge'][val_idx]))\n    print(f\"RMSE: {fold_rmse:.2f}\")\n\nridge_oof_rmse = np.sqrt(mean_squared_error(y_full, oof_predictions['ridge']))\nprint(f\"\\nRidge OOF RMSE: {ridge_oof_rmse:.2f}\")\n\n# ========================================\n# 融合策略对比\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"📊 融合策略效果对比\")\nprint(\"=\"*60)\n\nresults = []\n\n# 策略1: 单模型（LightGBM）\nresults.append({\n    '策略': 'LightGBM单模型',\n    'OOF RMSE': lgb_oof_rmse,\n    '权重': 'LGB:100%'\n})\n\n# 策略2: 单模型（Ridge）\nresults.append({\n    '策略': 'Ridge单模型',\n    'OOF RMSE': ridge_oof_rmse,\n    '权重': 'Ridge:100%'\n})\n\n# 策略3: 简单平均\navg_pred = (oof_predictions['lgb'] + oof_predictions['ridge']) / 2\navg_rmse = np.sqrt(mean_squared_error(y_full, avg_pred))\nresults.append({\n    '策略': '简单平均',\n    'OOF RMSE': avg_rmse,\n    '权重': 'LGB:50%, Ridge:50%'\n})\n\n# 策略4: 加权平均（网格搜索最优权重）\nbest_weight = 0.5\nbest_weighted_rmse = float('inf')\n\nfor w in np.arange(0.0, 1.01, 0.05):\n    weighted_pred = oof_predictions['lgb'] * w + oof_predictions['ridge'] * (1 - w)\n    rmse = np.sqrt(mean_squared_error(y_full, weighted_pred))\n    if rmse < best_weighted_rmse:\n        best_weighted_rmse = rmse\n        best_weight = w\n\nresults.append({\n    '策略': '加权平均（优化）',\n    'OOF RMSE': best_weighted_rmse,\n    '权重': f'LGB:{best_weight:.0%}, Ridge:{1-best_weight:.0%}'\n})\n\n# 策略5: Stacking (Meta-learner)\nprint(\"\\n3️⃣ 训练 Meta-learner (Level 1)...\")\nprint(\"-\"*60)\n\n# 准备Level 1的特征（Level 0的预测）\nmeta_features = np.column_stack([oof_predictions['lgb'], oof_predictions['ridge']])\nmeta_test_features = np.column_stack([test_predictions['lgb'], test_predictions['ridge']])\n\n# 训练Meta模型（使用简单的Ridge）\nmeta_model = Ridge(alpha=1.0, random_state=SEED)\nmeta_model.fit(meta_features, np.log1p(y_full))\n\n# Meta模型预测\nmeta_pred = np.expm1(meta_model.predict(meta_features))\nmeta_rmse = np.sqrt(mean_squared_error(y_full, meta_pred))\n\nprint(f\"Meta-learner系数: LGB={meta_model.coef_[0]:.4f}, Ridge={meta_model.coef_[1]:.4f}\")\nprint(f\"Meta-learner OOF RMSE: {meta_rmse:.2f}\")\n\nresults.append({\n    '策略': 'Stacking（Meta-learner）',\n    'OOF RMSE': meta_rmse,\n    '权重': 'Meta模型学习'\n})\n\n# ========================================\n# 结果汇总\n# ========================================\n\nresults_df = pd.DataFrame(results)\nprint(\"\\n\" + \"=\"*60)\nprint(\"📊 融合效果对比\")\nprint(\"=\"*60)\nprint(results_df.to_string(index=False))\n\n# 选择最佳策略\nbest_idx = results_df['OOF RMSE'].idxmin()\nbest_strategy = results_df.loc[best_idx]\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"🏆 最佳融合策略\")\nprint(\"=\"*60)\nprint(f\"策略: {best_strategy['策略']}\")\nprint(f\"OOF RMSE: {best_strategy['OOF RMSE']:.2f}\")\nprint(f\"权重: {best_strategy['权重']}\")\n\n# ========================================\n# 生成最终提交\n# ========================================\n\nprint(\"\\n\\n\" + \"=\"*60)\nprint(\"📝 生成最终提交文件\")\nprint(\"=\"*60)\n\n# 使用最佳策略的预测\nif best_idx == 5:  # Stacking\n    final_test_pred = np.expm1(meta_model.predict(meta_test_features))\nelif best_idx == 3:  # 加权平均\n    final_test_pred = test_predictions['lgb'] * best_weight + test_predictions['ridge'] * (1 - best_weight)\nelse:\n    final_test_pred = test_predictions['lgb']  # 默认LightGBM\n\n# Clip负值\nfinal_test_pred = np.maximum(final_test_pred, 0)\n\n# 创建提交文件\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'charges': final_test_pred\n})\n\nsubmission.to_csv('final_submission.csv', index=False)\n\nprint(f\"\\n✅ 最终提交文件已生成: final_submission.csv\")\nprint(f\"\\n预测统计:\")\nprint(submission['charges'].describe())\n\n# 可视化\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# 左图：策略对比\nax1 = axes[0]\nstrategies = [s.replace('（', '\\n(').replace('）', ')') for s in results_df['策略']]\ncolors = ['gold' if i == best_idx else 'skyblue' for i in range(len(results_df))]\nbars = ax1.bar(range(len(results_df)), results_df['OOF RMSE'], color=colors, edgecolor='black')\nax1.set_xticks(range(len(results_df)))\nax1.set_xticklabels(strategies, rotation=20, ha='right', fontsize=9)\nax1.set_ylabel('OOF RMSE')\nax1.set_title('不同融合策略效果对比')\nax1.grid(axis='y', alpha=0.3)\n\nfor bar, rmse in zip(bars, results_df['OOF RMSE']):\n    height = bar.get_height()\n    ax1.text(bar.get_x() + bar.get_width()/2., height,\n            f'{rmse:.0f}',\n            ha='center', va='bottom', fontsize=9, fontweight='bold')\n\n# 右图：预测分布\nax2 = axes[1]\nax2.hist(final_test_pred, bins=50, edgecolor='black', alpha=0.7)\nax2.set_xlabel('Predicted Charges')\nax2.set_ylabel('Frequency')\nax2.set_title('测试集预测分布')\nax2.axvline(final_test_pred.mean(), color='red', linestyle='--', label=f'Mean: {final_test_pred.mean():.0f}')\nax2.legend()\nax2.grid(axis='y', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"🎉 全部优化流程完成！\")\nprint(\"=\"*60)\nprint(\"\\n📊 完整优化路径总结:\")\nprint(\"  1. ✅ 数据清洗（异常值处理）\")\nprint(\"  2. ✅ 特征工程（50+个新特征）\")\nprint(\"  3. ✅ 超参数优化（Optuna贝叶斯优化）\")\nprint(\"  4. ✅ 模型融合（Stacking）\")\nprint(f\"\\n🏆 最终OOF RMSE: {best_strategy['OOF RMSE']:.2f}\")\nprint(f\"📈 预期排名: Top 10-20%（如果是Kaggle比赛）\")\nprint(\"\\n💡 你已经掌握了完整的机器学习竞赛流程！\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dg02slyrb57",
   "source": "---\n\n# 🎓 完整教程总结\n\n## 🏆 恭喜你完成了机器学习模型优化的完整学习！\n\n### 📊 你学到的核心知识\n\n#### 1️⃣ 数据清洗与预处理\n- **异常值检测方法**：IQR统计方法 + 领域知识\n- **处理策略对比**：删除、截断、替换、保留\n- **实验驱动**：通过对比实验选择最佳策略\n- **收获**：数据质量是模型性能的基础\n\n#### 2️⃣ 特征工程（最重要！）⭐⭐⭐\n- **领域知识特征**：28个基于保险业务的特征\n  - 风险评分（年龄、BMI、吸烟）\n  - 交互特征（smoker × age, smoker × bmi等）\n  - 多项式特征（age², bmi²等）\n  \n- **Target Encoding**：高级编码技术\n  - K-Fold方式防止数据泄漏\n  - 贝叶斯平滑处理小样本\n  \n- **分组统计特征**：20+个聚合特征\n  - 按类别分组的统计量\n  - 相对特征（与群体均值的偏差）\n  \n- **收获**：\"数据和特征决定机器学习的上限\"\n\n#### 3️⃣ 超参数优化\n- **调参策略对比**：Grid Search vs Random Search vs Bayesian Optimization\n- **Optuna实战**：TPE采样器的使用\n- **参数重要性分析**：了解哪些参数最关键\n- **收获**：系统化的调参方法比手工调参高效得多\n\n#### 4️⃣ 模型融合\n- **Ensemble原理**：\"三个臭皮匠赛过诸葛亮\"\n- **融合策略**：\n  - 简单平均\n  - 加权平均\n  - Stacking（两层模型）\n- **模型多样性**：不同模型犯不同的错误\n- **收获**：融合通常能带来稳定的性能提升\n\n---\n\n### 📈 性能提升路线图\n\n```\n起点（基线）: RMSE ≈ 6700\n    ↓\n数据清洗: -100 RMSE\n    ↓\n特征工程: -300~400 RMSE  ← 最大提升！\n    ↓\n超参数优化: -100~200 RMSE\n    ↓\n模型融合: -200~300 RMSE\n    ↓\n终点: RMSE ≈ 5500~5800\n\n总提升: 900~1200 RMSE (13-18%)\n```\n\n---\n\n### 💻 可复用的代码模板\n\n你现在拥有了完整的代码模板，可以应用到其他项目：\n\n1. **异常值检测与处理**\n   ```python\n   # IQR方法 + 领域知识\n   # 4种策略对比实验\n   ```\n\n2. **K-Fold Target Encoding类**\n   ```python\n   class TargetEncoder:\n       # 防止数据泄漏的完整实现\n   ```\n\n3. **分组统计特征函数**\n   ```python\n   def create_aggregation_features():\n       # 按类别分组统计\n   ```\n\n4. **Optuna优化框架**\n   ```python\n   def objective(trial):\n       # 贝叶斯优化目标函数\n   ```\n\n5. **Stacking实现**\n   ```python\n   # Level 0: 多个基模型\n   # Level 1: Meta-learner\n   ```\n\n---\n\n### 🎯 机器学习项目完整流程\n\n你已经掌握了端到端的流程：\n\n```\n1. 数据探索（EDA）\n   └── 理解数据分布、发现异常\n\n2. 数据清洗\n   └── 处理缺失值、异常值\n\n3. 特征工程 ⭐⭐⭐\n   ├── 领域知识特征\n   ├── Target Encoding\n   └── 统计聚合特征\n\n4. 模型训练\n   ├── 基础模型评估\n   └── 选择合适的模型\n\n5. 模型优化\n   ├── 超参数调优（Optuna）\n   └── 交叉验证策略\n\n6. 模型融合\n   ├── 训练多个模型\n   └── Stacking/Blending\n\n7. 后处理\n   ├── 残差分析\n   └── 预测校准\n\n8. 模型诊断\n   ├── 特征重要性\n   └── 错误分析\n\n9. 部署\n   └── 生成最终预测\n```\n\n---\n\n### 🚀 进一步学习建议\n\n#### 想提升到更高水平？\n\n1. **高级特征工程**\n   - 自动特征工程（AutoFE）\n   - 特征选择算法（RFE, LASSO等）\n   - 深度特征交互\n\n2. **更多模型**\n   - Neural Networks（神经网络）\n   - XGBoost深入优化\n   - CatBoost的类别特征处理\n\n3. **高级融合技术**\n   - Multi-level Stacking\n   - Blending策略\n   - 模型蒸馏\n\n4. **生产部署**\n   - 模型序列化（pickle, joblib）\n   - API部署（Flask, FastAPI）\n   - 模型监控与更新\n\n5. **AutoML工具**\n   - H2O AutoML\n   - TPOT\n   - AutoGluon\n\n#### 推荐学习资源\n\n- **Kaggle平台**：参加真实比赛\n- **书籍**：《Feature Engineering for Machine Learning》\n- **课程**：Andrew Ng的机器学习课程\n- **论文**：XGBoost, LightGBM, CatBoost原论文\n\n---\n\n### 📝 关键要点回顾\n\n#### 成功的关键因素（优先级排序）\n\n1. **特征工程** (40%) ⭐⭐⭐⭐⭐\n   - \"垃圾进，垃圾出\"\n   - 好的特征比复杂的模型更重要\n\n2. **数据质量** (30%) ⭐⭐⭐⭐\n   - 异常值处理\n   - 缺失值填充\n   - 数据平衡\n\n3. **模型选择与优化** (20%) ⭐⭐⭐\n   - 选择合适的算法\n   - 超参数调优\n   - 模型融合\n\n4. **验证策略** (10%) ⭐⭐\n   - 交叉验证\n   - 避免过拟合\n   - 可靠的评估\n\n---\n\n### 💡 最后的建议\n\n1. **实践是最好的老师**\n   - 多参加Kaggle比赛\n   - 尝试不同的数据集\n   - 从失败中学习\n\n2. **保持学习的习惯**\n   - 机器学习发展迅速\n   - 关注最新的论文和技术\n   - 参与社区讨论\n\n3. **注重基础**\n   - 理解算法原理\n   - 不要只依赖黑盒工具\n   - 数学和统计基础很重要\n\n4. **工程化思维**\n   - 代码可复用性\n   - 实验可重现性\n   - 文档和注释\n\n5. **业务导向**\n   - 模型要解决实际问题\n   - 性能指标要符合业务目标\n   - 可解释性很重要\n\n---\n\n## 🎉 恭喜你！\n\n你已经完成了从数据清洗到模型部署的完整机器学习流程学习。\n\n这不仅是一个教程的结束，更是你机器学习之旅的开始！\n\n### 下一步行动\n\n1. ✅ **运行全部cells**，看到完整的优化过程\n2. ✅ **修改参数**，尝试不同的配置\n3. ✅ **应用到新项目**，巩固所学知识\n4. ✅ **分享你的经验**，教学相长\n\n### 保持联系\n\n- 遇到问题？查看代码注释和理论说明\n- 想深入？阅读相关论文和文档\n- 有收获？分享给其他学习者\n\n**祝你在机器学习的道路上越走越远！** 🚀\n\n---\n\n*\"The only way to learn a new programming language is by writing programs in it.\"*\n*- Dennis Ritchie*\n\n*机器学习也是如此 - 只有通过实践才能真正掌握！*",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "theory-2",
   "metadata": {},
   "source": [
    "## 🔬 实验：对比不同的异常值处理策略\n",
    "\n",
    "我们将对比以下4种策略的效果：\n",
    "\n",
    "### 策略1: 不处理（Baseline）\n",
    "- 保留所有异常值\n",
    "- 作为对比基准\n",
    "\n",
    "### 策略2: 删除异常值\n",
    "- 删除 BMI > 60 的样本\n",
    "- 优点：彻底移除异常数据\n",
    "- 缺点：减少训练样本量\n",
    "\n",
    "### 策略3: 截断（Clipping）\n",
    "- 将 BMI > 60 的值设为 60\n",
    "- 优点：保留样本数量\n",
    "- 缺点：可能引入偏差\n",
    "\n",
    "### 策略4: 替换为中位数\n",
    "- 将异常值替换为BMI的中位数\n",
    "- 优点：保守处理\n",
    "- 缺点：改变数据分布\n",
    "\n",
    "我们会用简单的LightGBM模型（3折交叉验证）快速测试每种策略的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 3: 异常值处理策略对比实验\n",
    "# ========================================\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def quick_evaluate(train_df, strategy_name):\n",
    "    \"\"\"\n",
    "    快速评估函数：用3折CV评估处理后的数据\n",
    "    \n",
    "    参数:\n",
    "        train_df: 处理后的训练数据\n",
    "        strategy_name: 策略名称\n",
    "    \n",
    "    返回:\n",
    "        oof_rmse: Out-of-Fold RMSE\n",
    "    \"\"\"\n",
    "    # 简单特征工程\n",
    "    df = train_df.copy()\n",
    "    \n",
    "    # 类别编码\n",
    "    df['smoker'] = df['smoker'].map({'yes': 1, 'no': 0})\n",
    "    df = pd.get_dummies(df, columns=['sex', 'region'], drop_first=True)\n",
    "    \n",
    "    # 基础交互特征\n",
    "    df['age_bmi'] = df['age'] * df['bmi']\n",
    "    df['smoker_bmi'] = df['smoker'] * df['bmi']\n",
    "    \n",
    "    # 准备数据\n",
    "    X = df.drop(['charges', 'id'], axis=1, errors='ignore')\n",
    "    y = df['charges']\n",
    "    \n",
    "    # 3折交叉验证\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "    oof_predictions = np.zeros(len(X))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X), 1):\n",
    "        X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # 训练LightGBM（简化参数，快速评估）\n",
    "        model = lgb.LGBMRegressor(\n",
    "            n_estimators=200,\n",
    "            learning_rate=0.05,\n",
    "            num_leaves=31,\n",
    "            random_state=SEED,\n",
    "            verbose=-1\n",
    "        )\n",
    "        \n",
    "        model.fit(\n",
    "            X_tr, np.log1p(y_tr),\n",
    "            eval_set=[(X_val, np.log1p(y_val))],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]\n",
    "        )\n",
    "        \n",
    "        # 预测\n",
    "        pred = np.expm1(model.predict(X_val, num_iteration=model.best_iteration_))\n",
    "        oof_predictions[val_idx] = pred\n",
    "    \n",
    "    # 计算RMSE\n",
    "    oof_rmse = np.sqrt(mean_squared_error(y, oof_predictions))\n",
    "    \n",
    "    return oof_rmse, len(df)\n",
    "\n",
    "\n",
    "print(\"🔬 开始异常值处理策略对比实验\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n实验设置:\")\n",
    "print(\"  - 模型: LightGBM\")\n",
    "print(\"  - 验证: 3-Fold CV\")\n",
    "print(\"  - 评估指标: OOF RMSE\")\n",
    "print(\"  - 异常值定义: BMI > 60\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# 存储结果\n",
    "results = []\n",
    "\n",
    "# ========================================\n",
    "# 策略1: 不处理（Baseline）\n",
    "# ========================================\n",
    "print(\"\\n📊 策略1: 不处理异常值（Baseline）\")\n",
    "print(\"-\"*60)\n",
    "train_1 = train.copy()\n",
    "rmse_1, samples_1 = quick_evaluate(train_1, \"不处理\")\n",
    "results.append({\n",
    "    '策略': '1. 不处理（Baseline）',\n",
    "    '样本数': samples_1,\n",
    "    'OOF RMSE': rmse_1,\n",
    "    'vs Baseline': 0,\n",
    "    '说明': '保留所有异常值'\n",
    "})\n",
    "print(f\"样本数: {samples_1}\")\n",
    "print(f\"OOF RMSE: {rmse_1:.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 策略2: 删除异常值\n",
    "# ========================================\n",
    "print(\"\\n📊 策略2: 删除异常值 (BMI > 60)\")\n",
    "print(\"-\"*60)\n",
    "train_2 = train[train['bmi'] <= 60].copy().reset_index(drop=True)\n",
    "rmse_2, samples_2 = quick_evaluate(train_2, \"删除\")\n",
    "results.append({\n",
    "    '策略': '2. 删除异常值',\n",
    "    '样本数': samples_2,\n",
    "    'OOF RMSE': rmse_2,\n",
    "    'vs Baseline': rmse_2 - rmse_1,\n",
    "    '说明': f'删除了 {samples_1 - samples_2} 个样本'\n",
    "})\n",
    "print(f\"删除样本数: {samples_1 - samples_2}\")\n",
    "print(f\"剩余样本数: {samples_2}\")\n",
    "print(f\"OOF RMSE: {rmse_2:.2f}\")\n",
    "print(f\"相比Baseline: {rmse_2 - rmse_1:+.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 策略3: 截断 (Clipping)\n",
    "# ========================================\n",
    "print(\"\\n📊 策略3: 截断 (Clipping to 60)\")\n",
    "print(\"-\"*60)\n",
    "train_3 = train.copy()\n",
    "train_3['bmi'] = train_3['bmi'].clip(upper=60)\n",
    "rmse_3, samples_3 = quick_evaluate(train_3, \"截断\")\n",
    "results.append({\n",
    "    '策略': '3. 截断 (Clip to 60)',\n",
    "    '样本数': samples_3,\n",
    "    'OOF RMSE': rmse_3,\n",
    "    'vs Baseline': rmse_3 - rmse_1,\n",
    "    '说明': '将BMI>60的值设为60'\n",
    "})\n",
    "print(f\"样本数: {samples_3} (无变化)\")\n",
    "print(f\"修改值数量: {(train['bmi'] > 60).sum()}\")\n",
    "print(f\"OOF RMSE: {rmse_3:.2f}\")\n",
    "print(f\"相比Baseline: {rmse_3 - rmse_1:+.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 策略4: 替换为中位数\n",
    "# ========================================\n",
    "print(\"\\n📊 策略4: 替换为中位数\")\n",
    "print(\"-\"*60)\n",
    "train_4 = train.copy()\n",
    "bmi_median = train_4[train_4['bmi'] <= 60]['bmi'].median()\n",
    "train_4.loc[train_4['bmi'] > 60, 'bmi'] = bmi_median\n",
    "rmse_4, samples_4 = quick_evaluate(train_4, \"替换中位数\")\n",
    "results.append({\n",
    "    '策略': '4. 替换为中位数',\n",
    "    '样本数': samples_4,\n",
    "    'OOF RMSE': rmse_4,\n",
    "    'vs Baseline': rmse_4 - rmse_1,\n",
    "    '说明': f'替换为 {bmi_median:.2f}'\n",
    "})\n",
    "print(f\"样本数: {samples_4} (无变化)\")\n",
    "print(f\"中位数: {bmi_median:.2f}\")\n",
    "print(f\"修改值数量: {(train['bmi'] > 60).sum()}\")\n",
    "print(f\"OOF RMSE: {rmse_4:.2f}\")\n",
    "print(f\"相比Baseline: {rmse_4 - rmse_1:+.2f}\")\n",
    "\n",
    "# ========================================\n",
    "# 结果汇总\n",
    "# ========================================\n",
    "print(\"\\n\\n\" + \"=\"*60)\n",
    "print(\"📊 实验结果汇总\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# 找出最佳策略\n",
    "best_idx = results_df['OOF RMSE'].idxmin()\n",
    "best_strategy = results_df.loc[best_idx]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🏆 最佳策略\")\n",
    "print(\"=\"*60)\n",
    "print(f\"策略: {best_strategy['策略']}\")\n",
    "print(f\"OOF RMSE: {best_strategy['OOF RMSE']:.2f}\")\n",
    "print(f\"性能提升: {-best_strategy['vs Baseline']:.2f}\")\n",
    "print(f\"说明: {best_strategy['说明']}\")\n",
    "\n",
    "# 可视化对比\n",
    "print(\"\\n📊 可视化对比...\")\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['red' if x == best_idx else 'skyblue' for x in range(len(results_df))]\n",
    "bars = ax.bar(range(len(results_df)), results_df['OOF RMSE'], color=colors, edgecolor='black')\n",
    "ax.set_xticks(range(len(results_df)))\n",
    "ax.set_xticklabels([s.split('.')[1].strip() for s in results_df['策略']], rotation=15)\n",
    "ax.set_ylabel('OOF RMSE')\n",
    "ax.set_title('不同异常值处理策略的效果对比')\n",
    "ax.axhline(rmse_1, color='red', linestyle='--', alpha=0.5, label='Baseline')\n",
    "\n",
    "# 添加数值标签\n",
    "for i, (bar, rmse) in enumerate(zip(bars, results_df['OOF RMSE'])):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{rmse:.0f}',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 关键发现:\")\n",
    "print(\"  1. 异常值确实影响模型性能\")\n",
    "print(\"  2. 不同处理策略效果差异明显\")\n",
    "print(f\"  3. 最佳策略是: {best_strategy['策略']}\")\n",
    "print(f\"  4. 相比不处理，RMSE降低了约 {-best_strategy['vs Baseline']:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cell 4: 应用最佳策略 + 保存清洗后的数据\n",
    "# ========================================\n",
    "\n",
    "print(\"🎯 应用最佳异常值处理策略\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 根据实验结果，我们采用表现最好的策略\n",
    "# 这里我们先默认使用截断策略（通常效果较好）\n",
    "# 你运行实验后可以根据结果调整\n",
    "\n",
    "BEST_STRATEGY = \"clip\"  # 可选: \"clip\", \"remove\", \"median\"\n",
    "BMI_THRESHOLD = 60\n",
    "\n",
    "# 处理训练集\n",
    "print(\"\\n处理训练集...\")\n",
    "train_cleaned = train.copy()\n",
    "\n",
    "if BEST_STRATEGY == \"clip\":\n",
    "    print(f\"策略: 截断 (Clipping) - BMI > {BMI_THRESHOLD} 设为 {BMI_THRESHOLD}\")\n",
    "    n_modified = (train_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    train_cleaned['bmi'] = train_cleaned['bmi'].clip(upper=BMI_THRESHOLD)\n",
    "    print(f\"修改了 {n_modified} 个样本\")\n",
    "    \n",
    "elif BEST_STRATEGY == \"remove\":\n",
    "    print(f\"策略: 删除 - 移除 BMI > {BMI_THRESHOLD} 的样本\")\n",
    "    n_before = len(train_cleaned)\n",
    "    train_cleaned = train_cleaned[train_cleaned['bmi'] <= BMI_THRESHOLD].reset_index(drop=True)\n",
    "    n_after = len(train_cleaned)\n",
    "    print(f\"删除了 {n_before - n_after} 个样本\")\n",
    "    \n",
    "elif BEST_STRATEGY == \"median\":\n",
    "    print(f\"策略: 替换为中位数\")\n",
    "    bmi_median = train_cleaned[train_cleaned['bmi'] <= BMI_THRESHOLD]['bmi'].median()\n",
    "    n_modified = (train_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    train_cleaned.loc[train_cleaned['bmi'] > BMI_THRESHOLD, 'bmi'] = bmi_median\n",
    "    print(f\"修改了 {n_modified} 个样本，替换为 {bmi_median:.2f}\")\n",
    "\n",
    "# 处理测试集（同样的策略）\n",
    "print(\"\\n处理测试集...\")\n",
    "test_cleaned = test.copy()\n",
    "\n",
    "if BEST_STRATEGY == \"clip\":\n",
    "    n_modified_test = (test_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    test_cleaned['bmi'] = test_cleaned['bmi'].clip(upper=BMI_THRESHOLD)\n",
    "    print(f\"修改了 {n_modified_test} 个样本\")\n",
    "    \n",
    "elif BEST_STRATEGY == \"median\":\n",
    "    n_modified_test = (test_cleaned['bmi'] > BMI_THRESHOLD).sum()\n",
    "    test_cleaned.loc[test_cleaned['bmi'] > BMI_THRESHOLD, 'bmi'] = bmi_median\n",
    "    print(f\"修改了 {n_modified_test} 个样本\")\n",
    "\n",
    "# 检查其他字段是否有异常\n",
    "print(\"\\n\\n🔍 检查其他字段...\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "# Age检查\n",
    "print(f\"\\nAge 范围: [{train_cleaned['age'].min():.0f}, {train_cleaned['age'].max():.0f}]\")\n",
    "if train_cleaned['age'].min() < 0 or train_cleaned['age'].max() > 120:\n",
    "    print(\"  ⚠️  发现异常年龄值\")\n",
    "else:\n",
    "    print(\"  ✅ 年龄范围正常\")\n",
    "\n",
    "# Children检查\n",
    "print(f\"\\nChildren 范围: [{train_cleaned['children'].min():.0f}, {train_cleaned['children'].max():.0f}]\")\n",
    "if train_cleaned['children'].max() > 10:\n",
    "    print(f\"  ⚠️  发现异常孩子数量: max = {train_cleaned['children'].max():.0f}\")\n",
    "    # 可以选择处理\n",
    "    train_cleaned['children'] = train_cleaned['children'].clip(upper=10)\n",
    "    test_cleaned['children'] = test_cleaned['children'].clip(upper=10)\n",
    "    print(\"  已截断为 10\")\n",
    "else:\n",
    "    print(\"  ✅ 孩子数量范围正常\")\n",
    "\n",
    "# Charges检查\n",
    "print(f\"\\nCharges 范围: [{train_cleaned['charges'].min():.2f}, {train_cleaned['charges'].max():.2f}]\")\n",
    "if train_cleaned['charges'].min() < 0:\n",
    "    print(\"  ⚠️  发现负数费用\")\n",
    "else:\n",
    "    print(\"  ✅ 费用范围正常\")\n",
    "\n",
    "# 保存清洗后的数据\n",
    "print(\"\\n\\n💾 保存清洗后的数据...\")\n",
    "print(\"-\"*60)\n",
    "train_cleaned.to_csv('train_cleaned.csv', index=False)\n",
    "test_cleaned.to_csv('test_cleaned.csv', index=False)\n",
    "print(\"✅ 已保存:\")\n",
    "print(\"  - train_cleaned.csv\")\n",
    "print(\"  - test_cleaned.csv\")\n",
    "\n",
    "# 数据质量报告\n",
    "print(\"\\n\\n📋 数据清洗报告\")\n",
    "print(\"=\"*60)\n",
    "print(f\"训练集: {len(train)} → {len(train_cleaned)} 样本\")\n",
    "print(f\"测试集: {len(test)} → {len(test_cleaned)} 样本\")\n",
    "print(f\"\\nBMI 统计 (清洗后):\")\n",
    "print(f\"  - 最小值: {train_cleaned['bmi'].min():.2f}\")\n",
    "print(f\"  - 中位数: {train_cleaned['bmi'].median():.2f}\")\n",
    "print(f\"  - 最大值: {train_cleaned['bmi'].max():.2f}\")\n",
    "print(f\"  - 标准差: {train_cleaned['bmi'].std():.2f}\")\n",
    "\n",
    "print(\"\\n✅ 第一部分完成！\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n🎓 知识总结:\")\n",
    "print(\"  1. 学会了如何检测异常值（统计方法 + 领域知识）\")\n",
    "print(\"  2. 理解了异常值对模型的影响\")\n",
    "print(\"  3. 掌握了4种异常值处理策略\")\n",
    "print(\"  4. 学会了通过实验选择最佳策略\")\n",
    "print(\"  5. 预期RMSE提升: ~100左右\")\n",
    "print(\"\\n下一步: 高级特征工程 → 预期再降低400+ RMSE！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}