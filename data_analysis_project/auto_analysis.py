"""
è‡ªåŠ¨åŒ–æ•°æ®åˆ†æè„šæœ¬
åªéœ€è¯»å…¥æ•°æ®ï¼Œä¸€é”®ç”Ÿæˆå®Œæ•´çš„è®ºæ–‡çº§åˆ«åˆ†ææŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
python auto_analysis.py --train data/raw/train.csv --target charges

æˆ–è€…åœ¨Jupyterä¸­ï¼š
from auto_analysis import AutoAnalyzer
analyzer = AutoAnalyzer('data/raw/train.csv', target='charges')
analyzer.run_full_analysis()
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import sys
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ srcåˆ°è·¯å¾„
sys.path.append('src')

from utils.data_loader import DataLoader, reduce_mem_usage
from visualization.plot_templates import EDAPlotter
from utils.statistical_tests import StatisticalTester


class AutoAnalyzer:
    """è‡ªåŠ¨åŒ–åˆ†æå™¨"""

    def __init__(self, data_path, target=None, output_dir='reports/auto_analysis'):
        """
        åˆå§‹åŒ–è‡ªåŠ¨åˆ†æå™¨

        Parameters:
        -----------
        data_path : str
            æ•°æ®æ–‡ä»¶è·¯å¾„
        target : str, optional
            ç›®æ ‡å˜é‡å
        output_dir : str
            è¾“å‡ºç›®å½•
        """
        print("="*80)
        print("ğŸš€ è®ºæ–‡çº§åˆ«è‡ªåŠ¨åŒ–æ•°æ®åˆ†æç³»ç»Ÿ")
        print("="*80)

        self.data_path = data_path
        self.target = target
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–å·¥å…·
        self.plotter = EDAPlotter()
        self.tester = StatisticalTester()

        # åŠ è½½æ•°æ®
        print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {data_path}")
        self.df = pd.read_csv(data_path)
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ! å½¢çŠ¶: {self.df.shape}")

        # å†…å­˜ä¼˜åŒ–
        self.df = reduce_mem_usage(self.df, verbose=True)

        # è¯†åˆ«ç‰¹å¾ç±»å‹
        self._identify_features()

        # å¦‚æœæ²¡æœ‰æŒ‡å®štargetï¼Œå°è¯•è‡ªåŠ¨è¯†åˆ«
        if self.target is None:
            self.target = self._auto_detect_target()

        print(f"\nğŸ¯ ç›®æ ‡å˜é‡: {self.target}")
        print(f"ğŸ“Š æ•°å€¼å‹ç‰¹å¾: {len(self.numeric_features)} ä¸ª")
        print(f"ğŸ“Š ç±»åˆ«å‹ç‰¹å¾: {len(self.categorical_features)} ä¸ª")

    def _identify_features(self):
        """è¯†åˆ«ç‰¹å¾ç±»å‹"""
        self.numeric_features = self.df.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_features = self.df.select_dtypes(include=['object']).columns.tolist()

        # IDåˆ—æ£€æµ‹ï¼ˆé€šå¸¸å‘½åä¸ºid, ID, indexç­‰ï¼‰
        id_patterns = ['id', 'index', 'key']
        self.id_columns = [col for col in self.df.columns
                          if any(pattern in col.lower() for pattern in id_patterns)]

        if self.id_columns:
            print(f"\nğŸ”‘ æ£€æµ‹åˆ°IDåˆ—: {self.id_columns}")
            # ä»ç‰¹å¾åˆ—è¡¨ä¸­ç§»é™¤IDåˆ—
            self.numeric_features = [f for f in self.numeric_features if f not in self.id_columns]

    def _auto_detect_target(self):
        """è‡ªåŠ¨æ£€æµ‹ç›®æ ‡å˜é‡"""
        # å¸¸è§çš„ç›®æ ‡å˜é‡åç§°
        target_patterns = ['target', 'label', 'y', 'class', 'outcome', 'result']

        for col in self.df.columns:
            if any(pattern in col.lower() for pattern in target_patterns):
                print(f"\nğŸ’¡ è‡ªåŠ¨æ£€æµ‹åˆ°ç›®æ ‡å˜é‡: {col}")
                return col

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€åˆ—
        print(f"\nâš ï¸ æœªæ‰¾åˆ°æ˜ç¡®çš„ç›®æ ‡å˜é‡ï¼Œä½¿ç”¨æœ€åä¸€åˆ—: {self.df.columns[-1]}")
        return self.df.columns[-1]

    def run_full_analysis(self, save_report=True):
        """è¿è¡Œå®Œæ•´åˆ†æ"""
        print("\n" + "="*80)
        print("ğŸ“Š å¼€å§‹å®Œæ•´åˆ†æ...")
        print("="*80)

        # 1. æ•°æ®æ¦‚è§ˆ
        print("\n[1/6] æ•°æ®æ¦‚è§ˆåˆ†æ...")
        self.data_overview()

        # 2. æ•°æ®è´¨é‡æ£€æŸ¥
        print("\n[2/6] æ•°æ®è´¨é‡æ£€æŸ¥...")
        self.quality_check()

        # 3. å•å˜é‡åˆ†æ
        print("\n[3/6] å•å˜é‡åˆ†æ...")
        self.univariate_analysis()

        # 4. åŒå˜é‡åˆ†æ
        print("\n[4/6] åŒå˜é‡åˆ†æï¼ˆç‰¹å¾vsç›®æ ‡ï¼‰...")
        self.bivariate_analysis()

        # 5. å¤šå˜é‡åˆ†æ
        print("\n[5/6] å¤šå˜é‡åˆ†æï¼ˆç›¸å…³æ€§ï¼‰...")
        self.multivariate_analysis()

        # 6. ç»Ÿè®¡æ£€éªŒ
        print("\n[6/6] ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ...")
        self.statistical_tests()

        # ç”ŸæˆæŠ¥å‘Š
        if save_report:
            print("\nğŸ“ ç”Ÿæˆåˆ†ææŠ¥å‘Š...")
            self.generate_report()

        print("\n" + "="*80)
        print("âœ… åˆ†æå®Œæˆ!")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.output_dir}")
        print("="*80)

    def data_overview(self):
        """æ•°æ®æ¦‚è§ˆ"""
        print("\n" + "-"*80)
        print("ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯")
        print("-"*80)

        print(f"\næ•°æ®å½¢çŠ¶: {self.df.shape[0]:,} è¡Œ Ã— {self.df.shape[1]} åˆ—")
        print(f"å†…å­˜å ç”¨: {self.df.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

        print(f"\nç‰¹å¾ç±»å‹åˆ†å¸ƒ:")
        print(f"  - æ•°å€¼å‹: {len(self.numeric_features)} ä¸ª")
        print(f"  - ç±»åˆ«å‹: {len(self.categorical_features)} ä¸ª")

        print(f"\nå‰5è¡Œæ•°æ®:")
        print(self.df.head())

        print(f"\næ•°æ®ç±»å‹:")
        print(self.df.dtypes)

        print(f"\nåŸºæœ¬ç»Ÿè®¡:")
        print(self.df.describe())

    def quality_check(self):
        """æ•°æ®è´¨é‡æ£€æŸ¥"""
        print("\n" + "-"*80)
        print("ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
        print("-"*80)

        # ç¼ºå¤±å€¼
        missing = self.df.isnull().sum()
        missing_pct = 100 * missing / len(self.df)

        if missing.sum() > 0:
            print("\nâŒ ç¼ºå¤±å€¼æ£€æµ‹:")
            missing_df = pd.DataFrame({
                'ç¼ºå¤±æ•°é‡': missing[missing > 0],
                'ç¼ºå¤±ç‡(%)': missing_pct[missing > 0]
            }).sort_values('ç¼ºå¤±ç‡(%)', ascending=False)
            print(missing_df)

            # å¯è§†åŒ–ç¼ºå¤±å€¼
            if len(missing_df) > 0:
                fig, ax = plt.subplots(figsize=(12, max(6, len(missing_df) * 0.3)))
                missing_df['ç¼ºå¤±ç‡(%)'].plot(kind='barh', ax=ax,
                                           color=self.plotter.config.colors['warning'])
                ax.set_xlabel('ç¼ºå¤±ç‡ (%)', fontweight='bold')
                ax.set_title('ç‰¹å¾ç¼ºå¤±ç‡åˆ†å¸ƒ', fontsize=16, fontweight='bold')
                ax.grid(axis='x', alpha=0.3)
                plt.tight_layout()
                plt.savefig(self.output_dir / '01_missing_values.png', dpi=300, bbox_inches='tight')
                plt.close()
                print(f"  âœ… ç¼ºå¤±å€¼å›¾è¡¨å·²ä¿å­˜")
        else:
            print("\nâœ… æ— ç¼ºå¤±å€¼")

        # é‡å¤è¡Œ
        duplicates = self.df.duplicated().sum()
        if duplicates > 0:
            print(f"\nâš ï¸ æ£€æµ‹åˆ° {duplicates} è¡Œé‡å¤æ•°æ® ({duplicates/len(self.df)*100:.2f}%)")
        else:
            print("\nâœ… æ— é‡å¤è¡Œ")

        # å¸¸æ•°ç‰¹å¾
        constant_features = [col for col in self.df.columns if self.df[col].nunique() == 1]
        if constant_features:
            print(f"\nâš ï¸ å¸¸æ•°ç‰¹å¾ï¼ˆæ— ä¿¡æ¯é‡ï¼‰: {constant_features}")
        else:
            print("\nâœ… æ— å¸¸æ•°ç‰¹å¾")

        # å”¯ä¸€å€¼ç»Ÿè®¡
        print(f"\nğŸ“Š å”¯ä¸€å€¼ç»Ÿè®¡:")
        unique_counts = self.df.nunique().sort_values()
        print(unique_counts.head(10))

    def univariate_analysis(self):
        """å•å˜é‡åˆ†æ"""
        print("\næ­£åœ¨ç”Ÿæˆå•å˜é‡åˆ†æå›¾è¡¨...")

        # åˆ†ææ•°å€¼å‹ç‰¹å¾
        for feature in self.numeric_features[:10]:  # åªåˆ†æå‰10ä¸ªç‰¹å¾ï¼Œé¿å…å¤ªå¤š
            if feature != self.target:
                try:
                    save_path = self.output_dir / f'univariate_numeric_{feature}.png'
                    self.plotter.plot_numeric_distribution(
                        self.df, feature,
                        target=self.target if self.target in self.df.columns else None,
                        save_path=save_path
                    )
                    print(f"  âœ… {feature} åˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"  âŒ {feature} åˆ†æå¤±è´¥: {e}")

        # åˆ†æç±»åˆ«å‹ç‰¹å¾
        for feature in self.categorical_features[:10]:
            if feature != self.target:
                try:
                    save_path = self.output_dir / f'univariate_categorical_{feature}.png'
                    self.plotter.plot_categorical_distribution(
                        self.df, feature, save_path=save_path
                    )
                    print(f"  âœ… {feature} åˆ†æå®Œæˆ")
                except Exception as e:
                    print(f"  âŒ {feature} åˆ†æå¤±è´¥: {e}")

        # ç›®æ ‡å˜é‡åˆ†æ
        if self.target in self.df.columns:
            try:
                save_path = self.output_dir / f'target_analysis_{self.target}.png'
                self.plotter.plot_target_analysis(self.df, self.target, save_path=save_path)
                print(f"  âœ… ç›®æ ‡å˜é‡ {self.target} åˆ†æå®Œæˆ")
            except Exception as e:
                print(f"  âŒ ç›®æ ‡å˜é‡åˆ†æå¤±è´¥: {e}")

    def bivariate_analysis(self):
        """åŒå˜é‡åˆ†æ"""
        if self.target not in self.df.columns:
            print("âš ï¸ æœªæŒ‡å®šç›®æ ‡å˜é‡ï¼Œè·³è¿‡åŒå˜é‡åˆ†æ")
            return

        print("\næ­£åœ¨è¿›è¡ŒåŒå˜é‡åˆ†æ...")

        # åˆ›å»ºå¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()

        # åˆ†æå‰4ä¸ªæœ€ç›¸å…³çš„ç‰¹å¾
        if pd.api.types.is_numeric_dtype(self.df[self.target]):
            # å›å½’é—®é¢˜ï¼šè®¡ç®—ç›¸å…³æ€§
            correlations = []
            for feature in self.numeric_features:
                if feature != self.target:
                    corr = self.df[feature].corr(self.df[self.target])
                    correlations.append((feature, abs(corr)))

            top_features = sorted(correlations, key=lambda x: x[1], reverse=True)[:4]

            for idx, (feature, corr) in enumerate(top_features):
                ax = axes[idx]
                ax.scatter(self.df[feature], self.df[self.target], alpha=0.5,
                          color=self.plotter.config.colors['primary'])
                ax.set_xlabel(feature, fontweight='bold')
                ax.set_ylabel(self.target, fontweight='bold')
                ax.set_title(f'{feature} vs {self.target}\nç›¸å…³ç³»æ•°: {corr:.3f}',
                           fontweight='bold')
                ax.grid(True, alpha=0.3)

        else:
            # åˆ†ç±»é—®é¢˜ï¼šç®±çº¿å›¾
            top_features = self.numeric_features[:4]
            for idx, feature in enumerate(top_features):
                ax = axes[idx]
                self.df.boxplot(column=feature, by=self.target, ax=ax)
                ax.set_title(f'{feature} æŒ‰ {self.target} åˆ†ç»„', fontweight='bold')
                ax.set_ylabel(feature, fontweight='bold')

        plt.suptitle('ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³ç³»åˆ†æ', fontsize=18, fontweight='bold', y=1.00)
        plt.tight_layout()
        plt.savefig(self.output_dir / '02_bivariate_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ… åŒå˜é‡åˆ†æå›¾è¡¨å·²ä¿å­˜")

    def multivariate_analysis(self):
        """å¤šå˜é‡åˆ†æ"""
        print("\næ­£åœ¨è¿›è¡Œå¤šå˜é‡åˆ†æ...")

        # ç›¸å…³æ€§çƒ­åŠ›å›¾
        numeric_df = self.df[self.numeric_features].select_dtypes(include=[np.number])

        if len(numeric_df.columns) > 1:
            save_path = self.output_dir / '03_correlation_heatmap.png'
            self.plotter.plot_correlation_heatmap(numeric_df, save_path=save_path)
            print("  âœ… ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜")

        # é…å¯¹å›¾ï¼ˆå¦‚æœç‰¹å¾ä¸å¤ªå¤šï¼‰
        if len(numeric_df.columns) <= 5 and len(self.df) <= 5000:
            print("  æ­£åœ¨ç”Ÿæˆé…å¯¹å›¾ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
            try:
                important_features = numeric_df.columns[:5].tolist()
                if self.target in self.df.columns and self.target not in important_features:
                    important_features.append(self.target)

                pairplot_data = self.df[important_features]
                sns.pairplot(pairplot_data, diag_kind='kde', corner=True)
                plt.savefig(self.output_dir / '04_pairplot.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("  âœ… é…å¯¹å›¾å·²ä¿å­˜")
            except Exception as e:
                print(f"  âš ï¸ é…å¯¹å›¾ç”Ÿæˆå¤±è´¥: {e}")

    def statistical_tests(self):
        """ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
        if self.target not in self.df.columns:
            print("âš ï¸ æœªæŒ‡å®šç›®æ ‡å˜é‡ï¼Œè·³è¿‡ç»Ÿè®¡æ£€éªŒ")
            return

        print("\næ­£åœ¨è¿›è¡Œç»Ÿè®¡æ£€éªŒ...")

        results = []

        # å¯¹æ¯ä¸ªç‰¹å¾è¿›è¡Œæ£€éªŒ
        for feature in (self.numeric_features + self.categorical_features):
            if feature != self.target and feature not in self.id_columns:
                try:
                    test_result = self.tester.comprehensive_analysis(
                        self.df, feature, self.target
                    )
                    results.append({
                        'feature': feature,
                        'test_type': test_result['test_type'],
                        'result': test_result
                    })
                except Exception as e:
                    print(f"  âš ï¸ {feature} æ£€éªŒå¤±è´¥: {e}")

        # ä¿å­˜ç»“æœ
        self.test_results = results
        print(f"  âœ… å®Œæˆ {len(results)} ä¸ªç‰¹å¾çš„ç»Ÿè®¡æ£€éªŒ")

    def generate_report(self):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        report_path = self.output_dir / 'analysis_report.txt'

        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("è®ºæ–‡çº§åˆ«æ•°æ®åˆ†ææŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")

            # 1. æ•°æ®æ¦‚è§ˆ
            f.write("1. æ•°æ®æ¦‚è§ˆ\n")
            f.write("-"*80 + "\n")
            f.write(f"æ•°æ®å½¢çŠ¶: {self.df.shape}\n")
            f.write(f"æ•°å€¼å‹ç‰¹å¾: {len(self.numeric_features)} ä¸ª\n")
            f.write(f"ç±»åˆ«å‹ç‰¹å¾: {len(self.categorical_features)} ä¸ª\n")
            f.write(f"ç›®æ ‡å˜é‡: {self.target}\n\n")

            # 2. æ•°æ®è´¨é‡
            f.write("2. æ•°æ®è´¨é‡\n")
            f.write("-"*80 + "\n")
            missing = self.df.isnull().sum()
            if missing.sum() > 0:
                f.write("ç¼ºå¤±å€¼:\n")
                for col in missing[missing > 0].index:
                    f.write(f"  - {col}: {missing[col]} ({missing[col]/len(self.df)*100:.2f}%)\n")
            else:
                f.write("âœ… æ— ç¼ºå¤±å€¼\n")

            f.write(f"\né‡å¤è¡Œ: {self.df.duplicated().sum()}\n\n")

            # 3. ç»Ÿè®¡æ£€éªŒç»“æœ
            if hasattr(self, 'test_results'):
                f.write("3. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ\n")
                f.write("-"*80 + "\n")
                for result in self.test_results:
                    f.write(f"\nç‰¹å¾: {result['feature']}\n")
                    f.write(f"æ£€éªŒç±»å‹: {result['test_type']}\n")

                    # æ ¹æ®æ£€éªŒç±»å‹æå–å…³é”®ä¿¡æ¯
                    if 'correlation' in result['result']:
                        corr_result = result['result'].get('pearson', result['result'].get('spearman'))
                        if corr_result:
                            f.write(f"ç›¸å…³ç³»æ•°: {corr_result['correlation']:.3f}\n")
                            f.write(f"på€¼: {corr_result['p_value']:.4f}\n")
                            f.write(f"è§£é‡Š: {corr_result['interpretation']}\n")

            f.write("\n" + "="*80 + "\n")
            f.write("åˆ†æå®Œæˆ\n")
            f.write("="*80 + "\n")

        print(f"  âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse

    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–æ•°æ®åˆ†æ')
    parser.add_argument('--train', type=str, required=True, help='è®­ç»ƒæ•°æ®è·¯å¾„')
    parser.add_argument('--target', type=str, help='ç›®æ ‡å˜é‡å')
    parser.add_argument('--output', type=str, default='reports/auto_analysis', help='è¾“å‡ºç›®å½•')

    args = parser.parse_args()

    # è¿è¡Œåˆ†æ
    analyzer = AutoAnalyzer(args.train, target=args.target, output_dir=args.output)
    analyzer.run_full_analysis()


if __name__ == "__main__":
    main()
