# 数据分析与建模知识点检查清单

> 通过这个竞赛项目，系统掌握数据分析的核心知识

---

## 📊 Phase 1: 探索性数据分析 (EDA)

### 1.1 数据加载与理解
- [ ] **Pandas基础操作**
  - [ ] `pd.read_csv()` 各种参数的使用
  - [ ] `DataFrame`基本属性：shape, dtypes, columns
  - [ ] `info()`, `describe()`, `head()`, `tail()`
  - [ ] 数据类型转换：`astype()`
  - [ ] 内存优化技巧

- [ ] **数据质量检查**
  - [ ] 缺失值检测：`isnull()`, `isna()`
  - [ ] 重复值检测：`duplicated()`
  - [ ] 数据一致性检查

### 1.2 单变量分析
- [ ] **数值型变量**
  - [ ] 集中趋势：均值、中位数、众数
  - [ ] 离散程度：方差、标准差、四分位数
  - [ ] 分布形态：偏度（Skewness）、峰度（Kurtosis）
  - [ ] 可视化：直方图、箱线图、密度图、QQ图

- [ ] **类别型变量**
  - [ ] 频数统计：`value_counts()`
  - [ ] 类别占比
  - [ ] 可视化：柱状图、饼图
  - [ ] 识别高基数特征、稀有类别

### 1.3 双变量分析
- [ ] **数值 vs 数值**
  - [ ] 相关系数：Pearson, Spearman, Kendall
  - [ ] 散点图、散点矩阵
  - [ ] 相关性热力图

- [ ] **类别 vs 数值**
  - [ ] 分组统计：`groupby()`
  - [ ] 小提琴图、箱线图分组
  - [ ] 方差分析（ANOVA）

- [ ] **类别 vs 类别**
  - [ ] 交叉表：`pd.crosstab()`
  - [ ] 卡方检验
  - [ ] 堆叠柱状图

- [ ] **特征 vs 目标变量**
  - [ ] 分类问题：按类别分析特征分布差异
  - [ ] 回归问题：特征与目标的相关性
  - [ ] 特征重要性初步判断

### 1.4 多变量分析
- [ ] **相关矩阵分析**
  - [ ] 强相关特征识别（多重共线性）
  - [ ] 相关性可视化

- [ ] **降维可视化**
  - [ ] PCA降维
  - [ ] t-SNE降维
  - [ ] UMAP降维

---

## 🧹 Phase 2: 数据预处理

### 2.1 缺失值处理
- [ ] **缺失机制理解**
  - [ ] MCAR（完全随机缺失）
  - [ ] MAR（随机缺失）
  - [ ] MNAR（非随机缺失）

- [ ] **处理方法**
  - [ ] 删除法：行删除、列删除
  - [ ] 填充法：
    - [ ] 统计量填充（均值、中位数、众数）
    - [ ] 前向/后向填充（时间序列）
    - [ ] 常数填充
  - [ ] 高级方法：
    - [ ] KNN填充
    - [ ] 回归填充
    - [ ] 多重插补（MICE）

### 2.2 异常值处理
- [ ] **检测方法**
  - [ ] IQR方法（箱线图法）
  - [ ] Z-score方法
  - [ ] Isolation Forest
  - [ ] LOF（局部异常因子）

- [ ] **处理策略**
  - [ ] 删除
  - [ ] 截断（Winsorization）
  - [ ] 替换（用中位数等）
  - [ ] 保留（可能是有价值的信号）

### 2.3 数据转换
- [ ] **标准化/归一化**
  - [ ] StandardScaler（Z-score标准化）
  - [ ] MinMaxScaler（最小-最大归一化）
  - [ ] RobustScaler（鲁棒标准化）
  - [ ] Normalizer（L2归一化）

- [ ] **分布转换**
  - [ ] 对数变换：`np.log1p()`
  - [ ] 平方根变换
  - [ ] Box-Cox变换
  - [ ] Yeo-Johnson变换

### 2.4 数据分割
- [ ] **训练集/验证集/测试集划分**
  - [ ] `train_test_split()`
  - [ ] 分层采样（stratify）
  - [ ] 时间序列分割

---

## 🔧 Phase 3: 特征工程

### 3.1 类别特征编码
- [ ] **基础编码**
  - [ ] Label Encoding（标签编码）
  - [ ] One-Hot Encoding（独热编码）
  - [ ] `pd.get_dummies()`

- [ ] **高级编码**
  - [ ] Target Encoding（目标编码）
  - [ ] Frequency Encoding（频数编码）
  - [ ] Binary Encoding（二进制编码）
  - [ ] Hash Encoding（哈希编码）
  - [ ] WOE Encoding（证据权重编码）

### 3.2 数值特征构造
- [ ] **基础变换**
  - [ ] 四则运算（加减乘除）
  - [ ] 特征比例
  - [ ] 多项式特征

- [ ] **分箱（Binning）**
  - [ ] 等宽分箱
  - [ ] 等频分箱
  - [ ] 自定义分箱
  - [ ] 决策树分箱

- [ ] **时间特征**
  - [ ] 年、月、日、星期、小时
  - [ ] 节假日特征
  - [ ] 时间差特征
  - [ ] 周期性特征（sin/cos变换）

### 3.3 特征交互
- [ ] **特征组合**
  - [ ] 特征相乘
  - [ ] 特征相除
  - [ ] 特征聚合（groupby统计特征）

- [ ] **自动特征交互**
  - [ ] PolynomialFeatures
  - [ ] FeatureTools

### 3.4 特征选择
- [ ] **过滤法（Filter）**
  - [ ] 相关系数筛选
  - [ ] 卡方检验
  - [ ] 互信息
  - [ ] 方差阈值

- [ ] **包装法（Wrapper）**
  - [ ] 递归特征消除（RFE）
  - [ ] Sequential Feature Selection

- [ ] **嵌入法（Embedded）**
  - [ ] Lasso回归（L1正则）
  - [ ] 树模型特征重要性
  - [ ] SHAP值

### 3.5 降维
- [ ] **线性降维**
  - [ ] PCA（主成分分析）
  - [ ] LDA（线性判别分析）
  - [ ] SVD（奇异值分解）

- [ ] **非线性降维**
  - [ ] t-SNE
  - [ ] UMAP
  - [ ] Autoencoder

---

## 🤖 Phase 4: 模型训练

### 4.1 机器学习基础
- [ ] **监督学习 vs 无监督学习**
- [ ] **分类 vs 回归**
- [ ] **训练集、验证集、测试集概念**
- [ ] **过拟合 vs 欠拟合**
- [ ] **偏差-方差权衡（Bias-Variance Tradeoff）**

### 4.2 经典算法
- [ ] **线性模型**
  - [ ] 线性回归（Linear Regression）
  - [ ] 岭回归（Ridge）
  - [ ] Lasso回归
  - [ ] 弹性网络（ElasticNet）
  - [ ] 逻辑回归（Logistic Regression）

- [ ] **树模型**
  - [ ] 决策树（Decision Tree）
  - [ ] 随机森林（Random Forest）
  - [ ] 理解Bagging原理

- [ ] **Boosting模型**
  - [ ] GBDT原理
  - [ ] XGBoost
  - [ ] LightGBM
  - [ ] CatBoost
  - [ ] 理解Boosting原理

- [ ] **其他模型**
  - [ ] KNN（K近邻）
  - [ ] SVM（支持向量机）
  - [ ] 朴素贝叶斯

### 4.3 交叉验证
- [ ] **验证策略**
  - [ ] K-Fold交叉验证
  - [ ] StratifiedKFold（分层K折）
  - [ ] GroupKFold（分组K折）
  - [ ] TimeSeriesSplit（时间序列分割）
  - [ ] Leave-One-Out

- [ ] **实现方法**
  - [ ] `cross_val_score()`
  - [ ] `cross_validate()`
  - [ ] 手动循环实现

### 4.4 超参数优化
- [ ] **搜索策略**
  - [ ] 网格搜索（GridSearchCV）
  - [ ] 随机搜索（RandomizedSearchCV）
  - [ ] 贝叶斯优化（Bayesian Optimization）
  - [ ] Optuna框架使用

- [ ] **关键超参数理解**
  - [ ] LightGBM/XGBoost关键参数
  - [ ] 学习率、树深度、叶子节点数
  - [ ] 正则化参数

### 4.5 模型集成
- [ ] **Bagging**
  - [ ] 理解原理
  - [ ] BaggingClassifier/Regressor

- [ ] **Boosting**
  - [ ] 理解原理
  - [ ] AdaBoost

- [ ] **Stacking**
  - [ ] 多层模型堆叠
  - [ ] StackingClassifier/Regressor

- [ ] **Blending**
  - [ ] 加权平均
  - [ ] 简单平均

---

## 📈 Phase 5: 模型评估与优化

### 5.1 评估指标
- [ ] **分类问题**
  - [ ] 准确率（Accuracy）
  - [ ] 精确率（Precision）
  - [ ] 召回率（Recall）
  - [ ] F1-Score
  - [ ] AUC-ROC
  - [ ] AUC-PR
  - [ ] 混淆矩阵
  - [ ] 对数损失（Log Loss）

- [ ] **回归问题**
  - [ ] MAE（平均绝对误差）
  - [ ] MSE（均方误差）
  - [ ] RMSE（均方根误差）
  - [ ] R²（决定系数）
  - [ ] MAPE（平均绝对百分比误差）

- [ ] **指标选择原则**
  - [ ] 根据业务目标选择
  - [ ] 不平衡数据集的指标选择

### 5.2 模型诊断
- [ ] **学习曲线（Learning Curve）**
  - [ ] 诊断过拟合/欠拟合
  - [ ] 判断是否需要更多数据

- [ ] **验证曲线（Validation Curve）**
  - [ ] 分析单个超参数的影响

- [ ] **残差分析**
  - [ ] 残差图
  - [ ] 残差的正态性检验

- [ ] **特征重要性**
  - [ ] 内置特征重要性
  - [ ] Permutation Importance
  - [ ] 特征消融实验

### 5.3 模型解释
- [ ] **全局解释**
  - [ ] SHAP值（TreeExplainer）
  - [ ] LIME
  - [ ] Partial Dependence Plot（PDP）
  - [ ] Individual Conditional Expectation（ICE）

- [ ] **局部解释**
  - [ ] 单样本SHAP解释
  - [ ] Force Plot
  - [ ] Waterfall Plot

### 5.4 模型优化策略
- [ ] **处理过拟合**
  - [ ] 增加数据
  - [ ] 正则化
  - [ ] Early Stopping
  - [ ] Dropout
  - [ ] 特征选择

- [ ] **处理欠拟合**
  - [ ] 增加模型复杂度
  - [ ] 增加特征
  - [ ] 减少正则化

- [ ] **类别不平衡处理**
  - [ ] 过采样（SMOTE）
  - [ ] 欠采样
  - [ ] 类别权重调整
  - [ ] 集成采样方法

---

## 🛠️ 工具与技能

### 编程基础
- [ ] **Python基础**
  - [ ] 列表推导式
  - [ ] 函数定义与调用
  - [ ] 面向对象编程基础
  - [ ] 异常处理

- [ ] **Numpy**
  - [ ] 数组操作
  - [ ] 广播机制
  - [ ] 数学运算

- [ ] **Pandas进阶**
  - [ ] `apply()`, `map()`, `applymap()`
  - [ ] `groupby()`高级用法
  - [ ] `merge()`, `join()`, `concat()`
  - [ ] 时间序列处理

### 可视化
- [ ] **Matplotlib**
  - [ ] 图表类型选择
  - [ ] 子图布局
  - [ ] 样式定制

- [ ] **Seaborn**
  - [ ] 统计图表
  - [ ] 配色方案
  - [ ] FacetGrid

- [ ] **Plotly**
  - [ ] 交互式图表
  - [ ] Dashboard

### 版本控制
- [ ] **Git基础**
  - [ ] git add, commit, push
  - [ ] 分支管理
  - [ ] .gitignore配置

### 实验管理
- [ ] **记录实验**
  - [ ] Jupyter Notebook最佳实践
  - [ ] 实验日志
  - [ ] 参数记录

- [ ] **模型保存与加载**
  - [ ] pickle/joblib
  - [ ] 模型版本管理

---

## 📚 理论知识

### 统计学
- [ ] **描述性统计**
  - [ ] 集中趋势、离散程度
  - [ ] 分布类型（正态、泊松等）

- [ ] **推断性统计**
  - [ ] 假设检验
  - [ ] p值理解
  - [ ] 置信区间

- [ ] **概率论**
  - [ ] 条件概率
  - [ ] 贝叶斯定理
  - [ ] 概率分布

### 机器学习理论
- [ ] **损失函数**
  - [ ] 交叉熵损失
  - [ ] MSE/MAE
  - [ ] Hinge Loss

- [ ] **优化算法**
  - [ ] 梯度下降
  - [ ] 学习率概念
  - [ ] 动量、Adam等优化器

- [ ] **正则化**
  - [ ] L1/L2正则化
  - [ ] Dropout
  - [ ] Early Stopping

---

## ✅ 完成标准

每个知识点达到以下标准才算真正掌握：

1. **理解**：能用自己的话解释概念
2. **实践**：能独立编写代码实现
3. **应用**：能在新问题中正确应用
4. **调试**：遇到问题能自行解决
5. **优化**：能提出改进方案

---

## 📊 进度追踪

- Phase 1 完成度: ____%
- Phase 2 完成度: ____%
- Phase 3 完成度: ____%
- Phase 4 完成度: ____%
- Phase 5 完成度: ____%

**总体完成度: ____%**

---

## 💡 学习建议

1. **循序渐进**：按照Phase顺序学习
2. **边学边做**：每个知识点都在项目中实践
3. **记录笔记**：在Notebook中记录理解和心得
4. **定期复习**：每周回顾已学内容
5. **主动提问**：不懂的地方及时查资料或提问
6. **参考优秀案例**：学习Kaggle高分Notebook

祝你学习顺利！🎉
