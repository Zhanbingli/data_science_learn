# æ•°æ®åˆ†æžå¿«é€Ÿå‚è€ƒæ‰‹å†Œ

> å¸¸ç”¨ä»£ç ç‰‡æ®µå’Œå‘½ä»¤é€ŸæŸ¥è¡¨

---

## ðŸ”§ çŽ¯å¢ƒç®¡ç†

```bash
# åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
python -m venv venv

# æ¿€æ´»çŽ¯å¢ƒ
source venv/bin/activate  # Mac/Linux
venv\Scripts\activate     # Windows

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# å¯åŠ¨Jupyter
jupyter lab

# æŸ¥çœ‹å·²å®‰è£…çš„åŒ…
pip list
```

---

## ðŸ“Š Pandaså¸¸ç”¨æ“ä½œ

### æ•°æ®åŠ è½½
```python
import pandas as pd

# CSV
df = pd.read_csv('data.csv')

# æŒ‡å®šç¼–ç 
df = pd.read_csv('data.csv', encoding='utf-8')

# è§£æžæ—¥æœŸ
df = pd.read_csv('data.csv', parse_dates=['date_col'])

# åªè¯»å–éƒ¨åˆ†åˆ—
df = pd.read_csv('data.csv', usecols=['col1', 'col2'])
```

### æ•°æ®æŸ¥çœ‹
```python
# åŸºæœ¬ä¿¡æ¯
df.shape              # (è¡Œæ•°, åˆ—æ•°)
df.info()             # æ•°æ®ç±»åž‹å’Œç¼ºå¤±å€¼
df.describe()         # ç»Ÿè®¡æ‘˜è¦
df.head(10)           # å‰10è¡Œ
df.tail(10)           # åŽ10è¡Œ
df.columns            # åˆ—å
df.dtypes             # æ•°æ®ç±»åž‹

# æŸ¥çœ‹å”¯ä¸€å€¼
df['col'].nunique()
df['col'].value_counts()
```

### ç¼ºå¤±å€¼å¤„ç†
```python
# æ£€æŸ¥ç¼ºå¤±
df.isnull().sum()
df.isnull().sum() / len(df) * 100  # ç¼ºå¤±çŽ‡

# åˆ é™¤ç¼ºå¤±
df.dropna()           # åˆ é™¤åŒ…å«ç¼ºå¤±çš„è¡Œ
df.dropna(axis=1)     # åˆ é™¤åŒ…å«ç¼ºå¤±çš„åˆ—
df.dropna(thresh=5)   # è‡³å°‘æœ‰5ä¸ªéžç©ºå€¼æ‰ä¿ç•™

# å¡«å……ç¼ºå¤±
df.fillna(0)                        # å¡«å……ä¸º0
df.fillna(df.mean())                # å¡«å……ä¸ºå‡å€¼
df.fillna(df.median())              # å¡«å……ä¸ºä¸­ä½æ•°
df.fillna(method='ffill')           # å‰å‘å¡«å……
df.fillna(method='bfill')           # åŽå‘å¡«å……
df['col'].fillna(df['col'].mode()[0])  # å¡«å……ä¸ºä¼—æ•°
```

### æ•°æ®ç­›é€‰
```python
# æ¡ä»¶ç­›é€‰
df[df['age'] > 30]
df[(df['age'] > 30) & (df['gender'] == 'M')]
df[df['name'].isin(['Alice', 'Bob'])]

# é€‰æ‹©åˆ—
df['col']                    # å•åˆ—
df[['col1', 'col2']]        # å¤šåˆ—

# ä½ç½®é€‰æ‹©
df.iloc[0]                   # ç¬¬ä¸€è¡Œ
df.iloc[0:5]                 # å‰5è¡Œ
df.iloc[:, 0:3]              # å‰3åˆ—

# æ ‡ç­¾é€‰æ‹©
df.loc[0]                    # ç´¢å¼•ä¸º0çš„è¡Œ
df.loc[:, 'col1':'col3']     # åˆ—åèŒƒå›´
```

### æ•°æ®è½¬æ¢
```python
# ç±»åž‹è½¬æ¢
df['col'] = df['col'].astype(int)
df['col'] = df['col'].astype('category')
df['date'] = pd.to_datetime(df['date'])

# åº”ç”¨å‡½æ•°
df['col'].apply(lambda x: x * 2)
df.apply(lambda row: row['A'] + row['B'], axis=1)

# æ˜ å°„
df['col'].map({'A': 1, 'B': 2})

# æ›¿æ¢
df['col'].replace({'old': 'new'})
```

### åˆ†ç»„èšåˆ
```python
# åŸºç¡€åˆ†ç»„
df.groupby('category')['value'].mean()
df.groupby('category')['value'].agg(['mean', 'sum', 'count'])

# å¤šåˆ—åˆ†ç»„
df.groupby(['cat1', 'cat2'])['value'].mean()

# è‡ªå®šä¹‰èšåˆ
df.groupby('category').agg({
    'col1': 'mean',
    'col2': 'sum',
    'col3': ['min', 'max']
})
```

### åˆå¹¶æ•°æ®
```python
# æ‹¼æŽ¥
pd.concat([df1, df2], axis=0)      # çºµå‘æ‹¼æŽ¥
pd.concat([df1, df2], axis=1)      # æ¨ªå‘æ‹¼æŽ¥

# åˆå¹¶
pd.merge(df1, df2, on='key')                    # å†…è¿žæŽ¥
pd.merge(df1, df2, on='key', how='left')        # å·¦è¿žæŽ¥
pd.merge(df1, df2, on='key', how='outer')       # å¤–è¿žæŽ¥
pd.merge(df1, df2, left_on='a', right_on='b')   # ä¸åŒåˆ—å
```

---

## ðŸ“ˆ æ•°æ®å¯è§†åŒ–

### Matplotlib
```python
import matplotlib.pyplot as plt

# æŠ˜çº¿å›¾
plt.plot(x, y)
plt.xlabel('X Label')
plt.ylabel('Y Label')
plt.title('Title')
plt.show()

# æ•£ç‚¹å›¾
plt.scatter(x, y)

# æŸ±çŠ¶å›¾
plt.bar(categories, values)

# ç›´æ–¹å›¾
plt.hist(data, bins=30)

# å­å›¾
fig, axes = plt.subplots(2, 2, figsize=(12, 8))
axes[0, 0].plot(x, y)
```

### Seaborn
```python
import seaborn as sns

# åˆ†å¸ƒå›¾
sns.histplot(data=df, x='col')
sns.kdeplot(data=df, x='col')

# ç®±çº¿å›¾
sns.boxplot(data=df, x='category', y='value')

# å°æç´å›¾
sns.violinplot(data=df, x='category', y='value')

# æ•£ç‚¹å›¾
sns.scatterplot(data=df, x='col1', y='col2', hue='category')

# ç›¸å…³æ€§çƒ­åŠ›å›¾
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')

# æˆå¯¹å…³ç³»å›¾
sns.pairplot(df, hue='target')
```

---

## ðŸ§¹ æ•°æ®é¢„å¤„ç†

### å¼‚å¸¸å€¼å¤„ç†
```python
from scipy import stats

# IQRæ–¹æ³•
Q1 = df['col'].quantile(0.25)
Q3 = df['col'].quantile(0.75)
IQR = Q3 - Q1
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
df_clean = df[(df['col'] >= lower) & (df['col'] <= upper)]

# Z-scoreæ–¹æ³•
z_scores = np.abs(stats.zscore(df['col']))
df_clean = df[z_scores < 3]

# æˆªæ–­ï¼ˆWinsorizationï¼‰
df['col'] = df['col'].clip(lower, upper)
```

### æ•°æ®æ ‡å‡†åŒ–
```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler

# æ ‡å‡†åŒ–ï¼ˆZ-scoreï¼‰
scaler = StandardScaler()
df['col_scaled'] = scaler.fit_transform(df[['col']])

# å½’ä¸€åŒ–ï¼ˆ0-1ï¼‰
scaler = MinMaxScaler()
df['col_normalized'] = scaler.fit_transform(df[['col']])

# é²æ£’æ ‡å‡†åŒ–
scaler = RobustScaler()
df['col_robust'] = scaler.fit_transform(df[['col']])
```

### ç±»åˆ«ç¼–ç 
```python
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from category_encoders import TargetEncoder

# Label Encoding
le = LabelEncoder()
df['col_encoded'] = le.fit_transform(df['col'])

# One-Hot Encoding
df_onehot = pd.get_dummies(df, columns=['col'])

# Target Encoding
te = TargetEncoder()
df['col_target'] = te.fit_transform(df['col'], df['target'])

# Frequency Encoding
freq = df['col'].value_counts().to_dict()
df['col_freq'] = df['col'].map(freq)
```

---

## ðŸ¤– æœºå™¨å­¦ä¹ 

### æ•°æ®åˆ†å‰²
```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### äº¤å‰éªŒè¯
```python
from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold

# K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')

# Stratified K-Foldï¼ˆåˆ†å±‚ï¼‰
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')
```

### LightGBM
```python
import lightgbm as lgb

# å‚æ•°è®¾ç½®
params = {
    'objective': 'binary',
    'metric': 'auc',
    'boosting_type': 'gbdt',
    'num_leaves': 31,
    'learning_rate': 0.05,
    'feature_fraction': 0.8,
    'bagging_fraction': 0.8,
    'bagging_freq': 5,
    'verbose': -1
}

# è®­ç»ƒ
train_data = lgb.Dataset(X_train, label=y_train)
valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)

model = lgb.train(
    params,
    train_data,
    num_boost_round=1000,
    valid_sets=[train_data, valid_data],
    callbacks=[lgb.early_stopping(stopping_rounds=50)]
)

# é¢„æµ‹
y_pred = model.predict(X_test)

# ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': X_train.columns,
    'importance': model.feature_importance()
}).sort_values('importance', ascending=False)
```

### XGBoost
```python
import xgboost as xgb

# å‚æ•°è®¾ç½®
params = {
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'max_depth': 6,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8
}

# è®­ç»ƒ
dtrain = xgb.DMatrix(X_train, label=y_train)
dvalid = xgb.DMatrix(X_valid, label=y_valid)

model = xgb.train(
    params,
    dtrain,
    num_boost_round=1000,
    evals=[(dtrain, 'train'), (dvalid, 'valid')],
    early_stopping_rounds=50,
    verbose_eval=100
)

# é¢„æµ‹
dtest = xgb.DMatrix(X_test)
y_pred = model.predict(dtest)
```

### è¶…å‚æ•°ä¼˜åŒ–ï¼ˆOptunaï¼‰
```python
import optuna

def objective(trial):
    params = {
        'objective': 'binary',
        'metric': 'auc',
        'num_leaves': trial.suggest_int('num_leaves', 20, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'max_depth': trial.suggest_int('max_depth', 3, 15)
    }

    model = lgb.LGBMClassifier(**params)
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)

print('Best params:', study.best_params)
print('Best score:', study.best_value)
```

---

## ðŸ“Š æ¨¡åž‹è¯„ä¼°

### åˆ†ç±»æŒ‡æ ‡
```python
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report
)

# åŸºç¡€æŒ‡æ ‡
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)

# AUC
auc = roc_auc_score(y_true, y_pred_proba)

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(y_true, y_pred)

# åˆ†ç±»æŠ¥å‘Š
report = classification_report(y_true, y_pred)
```

### å›žå½’æŒ‡æ ‡
```python
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score
)

# MAE
mae = mean_absolute_error(y_true, y_pred)

# MSE
mse = mean_squared_error(y_true, y_pred)

# RMSE
rmse = np.sqrt(mse)

# RÂ²
r2 = r2_score(y_true, y_pred)
```

---

## ðŸ” æ¨¡åž‹è§£é‡Š

### SHAP
```python
import shap

# åˆ›å»ºexplainer
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)

# Summary plot
shap.summary_plot(shap_values, X_test)

# Force plotï¼ˆå•æ ·æœ¬ï¼‰
shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])

# Dependence plot
shap.dependence_plot('feature_name', shap_values, X_test)
```

---

## ðŸ’¾ æ¨¡åž‹ä¿å­˜ä¸ŽåŠ è½½

```python
import joblib
import pickle

# Joblibï¼ˆæŽ¨èï¼‰
joblib.dump(model, 'model.pkl')
model = joblib.load('model.pkl')

# Pickle
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

with open('model.pkl', 'rb') as f:
    model = pickle.load(f)

# LightGBMåŽŸç”Ÿæ ¼å¼
model.save_model('model.txt')
model = lgb.Booster(model_file='model.txt')
```

---

## ðŸŽ¯ å¸¸ç”¨æŠ€å·§

### å†…å­˜ä¼˜åŒ–
```python
# é™ä½Žæ•°å€¼ç±»åž‹ç²¾åº¦
df['int_col'] = df['int_col'].astype('int8')
df['float_col'] = df['float_col'].astype('float32')

# ç±»åˆ«ç±»åž‹
df['cat_col'] = df['cat_col'].astype('category')

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨
df.memory_usage(deep=True)
```

### æ—¶é—´ç‰¹å¾æå–
```python
df['date'] = pd.to_datetime(df['date'])
df['year'] = df['date'].dt.year
df['month'] = df['date'].dt.month
df['day'] = df['date'].dt.day
df['dayofweek'] = df['date'].dt.dayofweek
df['hour'] = df['date'].dt.hour
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)
```

### è¿›åº¦æ¡
```python
from tqdm import tqdm

# å¾ªçŽ¯
for i in tqdm(range(100)):
    # do something
    pass

# Pandas apply
tqdm.pandas()
df['new_col'] = df['col'].progress_apply(lambda x: x * 2)
```

---

## ðŸ”¨ è°ƒè¯•æŠ€å·§

```python
# æ–­ç‚¹è°ƒè¯•
import pdb; pdb.set_trace()

# æ‰“å°å˜é‡ç±»åž‹å’Œå€¼
print(f"Type: {type(var)}, Value: {var}")

# æŸ¥çœ‹DataFrameä¿¡æ¯
print(df.info())
print(df.describe())
print(df.head())

# æ£€æŸ¥ç©ºå€¼
assert df.isnull().sum().sum() == 0, "å­˜åœ¨ç©ºå€¼"

# æ£€æŸ¥å½¢çŠ¶
print(f"Shape: {df.shape}")
```

---

## ðŸ“ JupyteræŠ€å·§

```python
# æ˜¾ç¤ºæ‰€æœ‰åˆ—
pd.set_option('display.max_columns', None)

# æ˜¾ç¤ºæ›´å¤šè¡Œ
pd.set_option('display.max_rows', 100)

# æ˜¾ç¤ºå°æ•°ä½æ•°
pd.set_option('display.float_format', '{:.3f}'.format)

# æŸ¥çœ‹å˜é‡
%whos

# è®¡æ—¶
%%time
# code here

# å†…å­˜åˆ†æž
%load_ext memory_profiler
%memit df.groupby('col').mean()

# è‡ªåŠ¨é‡è½½æ¨¡å—
%load_ext autoreload
%autoreload 2
```

---

## ðŸŽ¯ å¿«é€Ÿæµ‹è¯•æµç¨‹

```python
# 1. åŠ è½½æ•°æ®
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# 2. ç®€å•ç‰¹å¾å·¥ç¨‹
X = train.drop('target', axis=1)
y = train['target']

# 3. å¿«é€Ÿè®­ç»ƒ
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X, y)

# 4. é¢„æµ‹
pred = model.predict(test)

# 5. æäº¤
submission = pd.DataFrame({'id': test_id, 'target': pred})
submission.to_csv('submission.csv', index=False)
```

---

ä¿å­˜è¿™ä¸ªæ–‡ä»¶ï¼Œåœ¨éœ€è¦æ—¶å¿«é€ŸæŸ¥é˜…ï¼ðŸš€
